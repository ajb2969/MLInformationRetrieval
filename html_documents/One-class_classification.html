<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1678">One-class classification</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>One-class classification</h1>
<hr>In [[machine learning]], '''one-class classification''', also known as '''unary classification''', tries to ''identify'' objects of a specific class amongst all objects, by learning from a [[training set]] containing only the objects of that class. This is different from and more difficult than the traditional [[classification (mac
<p>hine learning)|classification]] problem, which tries to <em>distinguish between</em> two or more classes with the training set containing objects from all the classes. An example is the classification of the operational status of a nuclear plant as 'normal':<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> In this scenario, there are (fortunately) few or no examples of catastrophic system states, only the statistics of normal operation are known. The term <strong>One-class classification</strong> was coined by Moya &amp; Hush (1996)<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> and many applications can be found in scientific literature, for example <a href="outlier_detection" title="wikilink">outlier detection</a>, <a href="anomaly_detection" title="wikilink">anomaly detection</a>, <a href="novelty_detection" title="wikilink">novelty detection</a>.</p>

<p>A similar problem is <strong>PU learning</strong>, in which a <a href="binary_classification" title="wikilink">binary classifier</a> is learned in a <a href="semi-supervised_learning" title="wikilink">semi-supervised</a> way from only <em>positive</em> and <em>unlabeled</em> samples.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="pu-learning">PU learning</h2>

<p>In PU learning, two sets of samples are assumed to be available for training: the positive set 

<math display="inline" id="One-class_classification:0">
 <semantics>
  <mi>P</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>P</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P
  </annotation>
 </semantics>
</math>

 and a <em>mixed set</em> 

<math display="inline" id="One-class_classification:1">
 <semantics>
  <mi>U</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>U</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   U
  </annotation>
 </semantics>
</math>

, which is assumed to contain both positive and negative samples, but without these being labeled as such. This contrasts with other forms of semisupervised learning, where it is assumed that a labeled set containing examples of both classes is available in addition to unlabeled samples. A variety of techniques exist to adapt <a href="supervised_learning" title="wikilink">supervised</a> classifiers to the PU learning setting, including variants of the <a href="Expectation-maximization" title="wikilink">EM algorithm</a>. PU learning has been successfully applied to <a href="text_classification" title="wikilink">text</a>,<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> time series,<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> and <a class="uri" href="bioinformatics" title="wikilink">bioinformatics</a> tasks.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Multiclass_classification" title="wikilink">Multiclass classification</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Statistical_classification" title="wikilink">Category:Statistical classification</a> <a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="http://homepage.tudelft.nl/n9d04/thesis.pdf">Tax, D. (2001) One-class classiﬁcation: Concept-learning in the absence of counter-examples. Doctoral Dissertation, University of Delft, The Netherlands.</a><a href="#fnref1">↩</a></li>
<li id="fn2">Moya, M. and Hush, D. (1996). "Network constraints and multi- objective optimization for one-class classification". <em>Neural Networks</em>, 9(3):463–474. <a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
</ol>
</section>
</hr></body>
</html>
