<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="302">Generalized Hebbian Algorithm</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Generalized Hebbian Algorithm</h1>
<hr>The '''Generalized Hebbian Algorithm''' ('''GHA'''), also known in the literature as '''Sanger's rule''', is a linear [[Feedforward neural network|feedforward]] [[neural network model]] for [[unsupervised learning]] with applications primarily in [[principal components analysis]]. First defined in 1989,<ref name="Sanger89">{{cite journal |last=Sanger |first=Terence D. |authorlink=Terence Sanger |year=1989 |title= Optimal unsupervised learning in a single-layer linear feedforward neural network |journal=Neural Networks |volume=2 |issue=6 |pages=459–473 |id= |url=http://courses.cs.washington.edu/courses/cse528/09sp/sanger_pca_nn.pdf |accessdate= 2007-11-24 |quot
<p>e=|doi= 10.1016/0893-6080(89)90044-0 }} it is similar to <a href="Oja's_rule" title="wikilink">Oja's rule</a> in its formulation and stability, except it can be applied to networks with multiple outputs. The name originates because of the similarity between the algorithm and a hypothesis made by <a href="Donald_Hebb" title="wikilink">Donald Hebb</a><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> about the way in which synaptic strengths in the brain are modified in response to experience, i.e., that changes are proportional to the correlation between the firing of pre- and post-synaptic <a class="uri" href="neurons" title="wikilink">neurons</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="theory">Theory</h2>

<p>GHA combines Oja's rule with the <a href="Gram-Schmidt_process" title="wikilink">Gram-Schmidt process</a> to produce a learning rule of the form</p>

<p>

<math display="block" id="Generalized_Hebbian_Algorithm:0">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="1.7pt" width="+1.7pt">
     <mi mathvariant="normal">Δ</mi>
    </mpadded>
    <mpadded width="+3.3pt">
     <msub>
      <mi>w</mi>
      <mrow>
       <mi>i</mi>
       <mi>j</mi>
      </mrow>
     </msub>
    </mpadded>
   </mrow>
   <mo rspace="5.8pt">=</mo>
   <mrow>
    <mi>η</mi>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mrow>
       <msub>
        <mi>y</mi>
        <mi>j</mi>
       </msub>
       <msub>
        <mi>x</mi>
        <mi>i</mi>
       </msub>
      </mrow>
      <mo>-</mo>
      <mrow>
       <msub>
        <mi>y</mi>
        <mi>j</mi>
       </msub>
       <mrow>
        <munderover>
         <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
         <mrow>
          <mi>k</mi>
          <mo>=</mo>
          <mn>1</mn>
         </mrow>
         <mi>j</mi>
        </munderover>
        <mrow>
         <msub>
          <mi>w</mi>
          <mrow>
           <mi>i</mi>
           <mi>k</mi>
          </mrow>
         </msub>
         <msub>
          <mi>y</mi>
          <mi>k</mi>
         </msub>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <times></times>
       <ci>i</ci>
       <ci>j</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>η</ci>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>j</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>j</ci>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <sum></sum>
          <apply>
           <eq></eq>
           <ci>k</ci>
           <cn type="integer">1</cn>
          </apply>
         </apply>
         <ci>j</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>w</ci>
          <apply>
           <times></times>
           <ci>i</ci>
           <ci>k</ci>
          </apply>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>y</ci>
          <ci>k</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \,\Delta w_{ij}~{}=~{}\eta\left(y_{j}x_{i}-y_{j}\sum_{k=1}^{j}w_{ik}y_{k}\right)
  </annotation>
 </semantics>
</math>

,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>where <mtpl></mtpl> defines the <a href="synaptic_weight" title="wikilink">synaptic weight</a> or connection strength between the 

<math display="inline" id="Generalized_Hebbian_Algorithm:1">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

th input and 

<math display="inline" id="Generalized_Hebbian_Algorithm:2">
 <semantics>
  <mi>j</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>j</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   j
  </annotation>
 </semantics>
</math>

th output neurons, 

<math display="inline" id="Generalized_Hebbian_Algorithm:3">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Generalized_Hebbian_Algorithm:4">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 are the input and output vectors, respectively, and 

<math display="inline" id="Generalized_Hebbian_Algorithm:5">
 <semantics>
  <mi>η</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>η</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   η
  </annotation>
 </semantics>
</math>

 is the <em>learning rate</em> parameter.</p>
<h3 id="derivation">Derivation</h3>

<p>In matrix form, Oja's rule can be written</p>

<p>

<math display="block" id="Generalized_Hebbian_Algorithm:6">
 <semantics>
  <mrow>
   <mpadded lspace="1.7pt" width="+5pt">
    <mfrac>
     <mrow>
      <mi>d</mi>
      <mi>w</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>t</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mrow>
      <mi>d</mi>
      <mi>t</mi>
     </mrow>
    </mfrac>
   </mpadded>
   <mo rspace="5.8pt">=</mo>
   <mrow>
    <mrow>
     <mi>w</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>t</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>Q</mi>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mi>diag</mi>
     <mrow>
      <mo stretchy="false">[</mo>
      <mrow>
       <mi>w</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <mi>Q</mi>
       <mi>w</mi>
       <msup>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>t</mi>
         <mo stretchy="false">)</mo>
        </mrow>
        <mi mathvariant="normal">T</mi>
       </msup>
      </mrow>
      <mo stretchy="false">]</mo>
     </mrow>
     <mi>w</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>t</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>d</ci>
      <ci>w</ci>
      <ci>t</ci>
     </apply>
     <apply>
      <times></times>
      <ci>d</ci>
      <ci>t</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>w</ci>
      <ci>t</ci>
      <ci>Q</ci>
     </apply>
     <apply>
      <times></times>
      <ci>diag</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <times></times>
        <ci>w</ci>
        <ci>t</ci>
        <ci>Q</ci>
        <ci>w</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>t</ci>
         <ci>normal-T</ci>
        </apply>
       </apply>
      </apply>
      <ci>w</ci>
      <ci>t</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \,\frac{dw(t)}{dt}~{}=~{}w(t)Q-\mathrm{diag}[w(t)Qw(t)^{\mathrm{T}}]w(t)
  </annotation>
 </semantics>
</math>

,</p>

<p>and the Gram-Schmidt algorithm is</p>

<p>

<math display="block" id="Generalized_Hebbian_Algorithm:7">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="1.7pt" width="+1.7pt">
     <mi mathvariant="normal">Δ</mi>
    </mpadded>
    <mi>w</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo rspace="5.8pt" stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo rspace="5.8pt">=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mi>lower</mi>
     <mrow>
      <mo stretchy="false">[</mo>
      <mrow>
       <mi>w</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <mi>w</mi>
       <msup>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>t</mi>
         <mo stretchy="false">)</mo>
        </mrow>
        <mi mathvariant="normal">T</mi>
       </msup>
      </mrow>
      <mo stretchy="false">]</mo>
     </mrow>
     <mi>w</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>t</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <ci>w</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>lower</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <times></times>
        <ci>w</ci>
        <ci>t</ci>
        <ci>w</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>t</ci>
         <ci>normal-T</ci>
        </apply>
       </apply>
      </apply>
      <ci>w</ci>
      <ci>t</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \,\Delta w(t)~{}=~{}-\mathrm{lower}[w(t)w(t)^{\mathrm{T}}]w(t)
  </annotation>
 </semantics>
</math>

,</p>

<p>where 

<math display="inline" id="Generalized_Hebbian_Algorithm:8">
 <semantics>
  <mrow>
   <mi>w</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>t</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>w</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w(t)
  </annotation>
 </semantics>
</math>

 is any matrix, in this case representing synaptic weights, <mtpl> <em>η</em> <strong>x</strong> <strong>x</strong><sup>T</sup>}}</mtpl> is the autocorrelation matrix, simply the outer product of inputs, 

<math display="inline" id="Generalized_Hebbian_Algorithm:9">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mi>i</mi>
   <mi>a</mi>
   <mi>g</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>d</ci>
    <ci>i</ci>
    <ci>a</ci>
    <ci>g</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   diag
  </annotation>
 </semantics>
</math>

 is the function that <a href="diagonalization" title="wikilink">diagonalizes</a> a matrix, and 

<math display="inline" id="Generalized_Hebbian_Algorithm:10">
 <semantics>
  <mrow>
   <mi>l</mi>
   <mi>o</mi>
   <mi>w</mi>
   <mi>e</mi>
   <mi>r</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>l</ci>
    <ci>o</ci>
    <ci>w</ci>
    <ci>e</ci>
    <ci>r</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   lower
  </annotation>
 </semantics>
</math>

 is the function that sets all matrix elements on or above the diagonal equal to 0. We can combine these equations to get our original rule in matrix form,</p>

<p>

<math display="block" id="Generalized_Hebbian_Algorithm:11">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="1.7pt" width="+1.7pt">
     <mi mathvariant="normal">Δ</mi>
    </mpadded>
    <mi>w</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo rspace="5.8pt" stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo rspace="5.8pt">=</mo>
   <mrow>
    <mi>η</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mrow>
       <mi>𝐲</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <mi>𝐱</mi>
       <msup>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>t</mi>
         <mo stretchy="false">)</mo>
        </mrow>
        <mi mathvariant="normal">T</mi>
       </msup>
      </mrow>
      <mo>-</mo>
      <mrow>
       <mi>LT</mi>
       <mrow>
        <mo stretchy="false">[</mo>
        <mrow>
         <mi>𝐲</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>t</mi>
          <mo stretchy="false">)</mo>
         </mrow>
         <mi>𝐲</mi>
         <msup>
          <mrow>
           <mo stretchy="false">(</mo>
           <mi>t</mi>
           <mo stretchy="false">)</mo>
          </mrow>
          <mi mathvariant="normal">T</mi>
         </msup>
        </mrow>
        <mo stretchy="false">]</mo>
       </mrow>
       <mi>w</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <ci>w</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <times></times>
     <ci>η</ci>
     <ci>t</ci>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>𝐲</ci>
       <ci>t</ci>
       <ci>𝐱</ci>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>t</ci>
        <ci>normal-T</ci>
       </apply>
      </apply>
      <apply>
       <times></times>
       <ci>LT</ci>
       <apply>
        <csymbol cd="latexml">delimited-[]</csymbol>
        <apply>
         <times></times>
         <ci>𝐲</ci>
         <ci>t</ci>
         <ci>𝐲</ci>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>t</ci>
          <ci>normal-T</ci>
         </apply>
        </apply>
       </apply>
       <ci>w</ci>
       <ci>t</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \,\Delta w(t)~{}=~{}\eta(t)\left(\mathbf{y}(t)\mathbf{x}(t)^{\mathrm{T}}-%
\mathrm{LT}[\mathbf{y}(t)\mathbf{y}(t)^{\mathrm{T}}]w(t)\right)
  </annotation>
 </semantics>
</math>

,</p>

<p>where the function 

<math display="inline" id="Generalized_Hebbian_Algorithm:12">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mi>T</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <ci>T</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   LT
  </annotation>
 </semantics>
</math>

 sets all matrix elements above the diagonal equal to 0, and note that our output 

<math display="inline" id="Generalized_Hebbian_Algorithm:13">
 <semantics>
  <mrow>
   <mrow>
    <mi>𝐲</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>w</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>𝐱</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>𝐲</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <times></times>
     <ci>w</ci>
     <ci>t</ci>
     <ci>𝐱</ci>
     <ci>t</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{y}(t)=w(t)\mathbf{x}(t)
  </annotation>
 </semantics>
</math>

 is a linear neuron.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h3 id="stability-and-pca">Stability and PCA</h3>

<p><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> <a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="applications">Applications</h2>

<p>GHA is used in applications where a <a href="self-organizing_map" title="wikilink">self-organizing map</a> is necessary, or where a feature or <a href="principal_components_analysis" title="wikilink">principal components analysis</a> can be used. Examples of such cases include <a href="artificial_intelligence" title="wikilink">artificial intelligence</a> and speech and image processing.</p>

<p>Its importance comes from the fact that learning is a single-layer process—that is, a synaptic weight changes only depending on the response of the inputs and outputs of that layer, thus avoiding the multi-layer dependence associated with the <a class="uri" href="backpropagation" title="wikilink">backpropagation</a> algorithm. It also has a simple and predictable trade-off between learning speed and accuracy of convergence as set by the learning rate parameter 

<math display="inline" id="Generalized_Hebbian_Algorithm:14">
 <semantics>
  <mi>η</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>η</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   η
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Hebbian_learning" title="wikilink">Hebbian learning</a></li>
<li><a href="Oja's_rule" title="wikilink">Oja's rule</a></li>
<li><a href="Factor_analysis" title="wikilink">Factor analysis</a></li>
<li><a href="PCA_network" title="wikilink">PCA network</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"></li>
</ol>
</section>
</ref></hr></body>
</html>
