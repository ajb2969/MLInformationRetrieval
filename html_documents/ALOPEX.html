<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1224">ALOPEX</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>ALOPEX</h1>
<hr/>
<p><strong>ALOPEX</strong> (an acronym from "<em><strong>AL</strong>gorithms <strong>O</strong>f <strong>P</strong>attern <strong>EX</strong>traction</em>") is a correlation based machine learning algorithm first proposed by <a href="Evangelia_Micheli-Tzanakou" title="wikilink">Tzanakou</a> and Harth in 1974.</p>
<h2 id="principle">Principle</h2>
<p>In <a href="machine_learning" title="wikilink">machine learning</a>, the goal is to train a system to minimize a cost function or (referring to ALOPEX) a response function. Many training algorithms, such as <a class="uri" href="backpropagation" title="wikilink">backpropagation</a>, have an inherent susceptibility to getting "stuck" in local minima or maxima of the response function. ALOPEX uses a cross-correlation of differences and a stochastic process to overcome this in an attempt to reach the absolute minimum (or maximum) of the response function.</p>
<h2 id="method">Method</h2>
<p>ALOPEX, in its simplest form is defined by an updating equation:</p>
<p><span class="LaTeX">$\Delta\ W_{ij}(n) = \gamma\ \Delta\ W_{ij}(n-1) \Delta\ R(n) + r_i(n)$</span></p>
<p>Where:</p>
<ul>
<li><span class="LaTeX">$n \geq 0$</span> is the iteration or time-step.</li>
<li><span class="LaTeX">$\Delta\ W_{ij}(n)$</span> is the difference between the current and previous value of system variable <span class="LaTeX">$\ W_{ij}$</span> at iteration <span class="LaTeX">$n \$</span>.</li>
<li><span class="LaTeX">$\Delta\ R(n)$</span> is the difference between the current and previous value of the response function <span class="LaTeX">$\ R,$</span> at iteration <span class="LaTeX">$n \$</span>.</li>
<li><span class="LaTeX">$\gamma\$</span> is the learning rate parameter <span class="LaTeX">$(\gamma\ < 0$</span> minimizes <span class="LaTeX">$R, \$</span> and <span class="LaTeX">$\gamma\ > 0$</span> maximizes <span class="LaTeX">$R \ )$</span></li>
<li><span class="LaTeX">$r_i(n) \sim\ N(0,\sigma\ ^2)$</span></li>
</ul>
<h2 id="discussion">Discussion</h2>
<p>Essentially, ALOPEX changes each system variable <span class="LaTeX">$W_{ij}(n)$</span> based on a product of: the previous change in the variable <span class="LaTeX">$\Delta$</span><span class="LaTeX">$W_{ij}(n-1)$</span>, the resulting change in the cost function <span class="LaTeX">$\Delta$</span><span class="LaTeX">$R(n)$</span>, and the learning rate parameter <span class="LaTeX">$\gamma$</span>. Further, to find the absolute minimum (or maximum), the stochastic process <span class="LaTeX">$r_{ij}(n)$</span> (Gaussian or other) is added to stochastically "push" the algorithm out of any local minima.</p>
<h2 id="references">References</h2>
<ul>
<li>Harth, E., & Tzanakou, E. (1974) Alopex: A stochastic method for determining visual receptive fields. Vision Research, <strong>14</strong>:1475-1482. <a href="http://dx.doi.org/10.1016/0042-6989(74)90024-8">Abstract from ScienceDirect</a></li>
</ul>
<p>"</p>
<p><a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a> <a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a></p>
</body>
</html>
