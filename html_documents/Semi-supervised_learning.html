<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1339">Semi-supervised learning</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Semi-supervised learning</h1>
<hr/>
<figure><b>(Figure)</b>
<figcaption>An example of the influence of unlabeled data in semi-supervised learning. The top panel shows a decision boundary we might adopt after seeing only one positive (white circle) and one negative (black circle) example. The bottom panel shows a decision boundary we might adopt if, in addition to the two labeled examples, we were given a collection of unlabeled data (gray circles). This could be viewed as performing <a href="Cluster_analysis" title="wikilink">clustering</a> and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.</figcaption>
</figure>

<p><strong>Semi-supervised learning</strong> is a class of <a href="supervised_learning" title="wikilink">supervised learning</a> tasks and techniques that also make use of unlabeled <a class="uri" href="data" title="wikilink">data</a> for training - typically a small amount of <a href="labeled_data" title="wikilink">labeled data</a> with a large amount of unlabeled data. Semi-supervised learning falls between <a href="unsupervised_learning" title="wikilink">unsupervised learning</a> (without any labeled training data) and <a href="supervised_learning" title="wikilink">supervised learning</a> (with completely labeled training data). Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.</p>

<p>As in the supervised learning framework, we are given a set of 

<math display="inline" id="Semi-supervised_learning:0">
 <semantics>
  <mi>l</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>l</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l
  </annotation>
 </semantics>
</math>

 <a href="Independent_identically_distributed" title="wikilink">independently identically distributed</a> examples 

<math display="inline" id="Semi-supervised_learning:1">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>x</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>x</mi>
     <mi>l</mi>
    </msub>
   </mrow>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>l</ci>
     </apply>
    </list>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1},\dots,x_{l}\in X
  </annotation>
 </semantics>
</math>

 with corresponding labels 

<math display="inline" id="Semi-supervised_learning:2">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>y</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mi>l</mi>
    </msub>
   </mrow>
   <mo>∈</mo>
   <mi>Y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>l</ci>
     </apply>
    </list>
    <ci>Y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{1},\dots,y_{l}\in Y
  </annotation>
 </semantics>
</math>

. Additionally, we are given 

<math display="inline" id="Semi-supervised_learning:3">
 <semantics>
  <mi>u</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>u</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   u
  </annotation>
 </semantics>
</math>

 unlabeled examples 

<math display="inline" id="Semi-supervised_learning:4">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>x</mi>
     <mrow>
      <mi>l</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>x</mi>
     <mrow>
      <mi>l</mi>
      <mo>+</mo>
      <mi>u</mi>
     </mrow>
    </msub>
   </mrow>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <apply>
       <plus></plus>
       <ci>l</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <apply>
       <plus></plus>
       <ci>l</ci>
       <ci>u</ci>
      </apply>
     </apply>
    </list>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{l+1},\dots,x_{l+u}\in X
  </annotation>
 </semantics>
</math>

. Semi-supervised learning attempts to make use of this combined information to surpass the <a href="Statistical_classification" title="wikilink">classification</a> performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.</p>

<p>Semi-supervised learning may refer to either <a href="Transduction_(machine_learning)" title="wikilink">transductive learning</a> or <a href="Inductive_reasoning" title="wikilink">inductive learning</a>. The goal of transductive learning is to infer the correct labels for the given unlabeled data 

<math display="inline" id="Semi-supervised_learning:5">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>l</mi>
     <mo>+</mo>
     <mn>1</mn>
    </mrow>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>l</mi>
     <mo>+</mo>
     <mi>u</mi>
    </mrow>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <apply>
      <plus></plus>
      <ci>l</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <apply>
      <plus></plus>
      <ci>l</ci>
      <ci>u</ci>
     </apply>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{l+1},\dots,x_{l+u}
  </annotation>
 </semantics>
</math>

 only. The goal of inductive learning is to infer the correct mapping from 

<math display="inline" id="Semi-supervised_learning:6">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 to 

<math display="inline" id="Semi-supervised_learning:7">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

.</p>

<p>Intuitively, we can think of the learning problem as an exam and labeled data as the few example problems that the teacher solved in class. The teacher also provides a set of unsolved problems. In the transductive setting, these unsolved problems are a take-home exam and you want to do well on them in particular. In the inductive setting, these are practice problems of the sort you will encounter on the in-class exam.</p>

<p>It is unnecessary (and, according to <a href="Vapnik's_principle" title="wikilink">Vapnik's principle</a>, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.</p>
<h2 id="assumptions-used-in-semi-supervised-learning">Assumptions used in semi-supervised learning</h2>

<p>In order to make any use of unlabeled data, we must assume some structure to the underlying distribution of data. Semi-supervised learning algorithms make use of at least one of the following assumptions. <a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h3 id="smoothness-assumption">Smoothness assumption</h3>

<p><em>Points which are close to each other are more likely to share a label.</em> This is also generally assumed in supervised learning and yields a preference for geometrically simple <a href="decision_boundary" title="wikilink">decision boundaries</a>. In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so that there are fewer points close to each other but in different classes.</p>
<h3 id="cluster-assumption">Cluster assumption</h3>

<p><em>The data tend to form discrete clusters, and points in the same cluster are more likely to share a label</em> (although data sharing a label may be spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to <a href="feature_learning" title="wikilink">feature learning</a> with clustering algorithms.</p>
<h3 id="manifold-assumption">Manifold assumption</h3>

<p><em>The data lie approximately on a <a class="uri" href="manifold" title="wikilink">manifold</a> of much lower dimension than the input space.</em> In this case we can attempt to learn the manifold using both the labeled and unlabeled data to avoid the <a href="curse_of_dimensionality" title="wikilink">curse of dimensionality</a>. Then learning can proceed using distances and densities defined on the manifold.</p>

<p>The manifold assumption is practical when high-dimensional data are being generated by some process that may be hard to model directly, but which only has a few degrees of freedom. For instance, human voice is controlled by a few vocal folds,<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> and images of various facial expressions are controlled by a few muscles. We would like in these cases to use distances and smoothness in the natural space of the generating problem, rather than in the space of all possible acoustic waves or images respectively.</p>
<h2 id="history">History</h2>

<p>The heuristic approach of <em>self-training</em> (also known as <em>self-learning</em> or <em>self-labeling</em>) is historically the oldest approach to semi-supervised learning,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> with examples of applications starting in the 1960s (see for instance Scudder (1965)<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a>).</p>

<p>The transductive learning framework was formally introduced by <a href="Vladimir_Vapnik" title="wikilink">Vladimir Vapnik</a> in the 1970s.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Interest in inductive learning using generative models also began in the 1970s. A <a href="Probably_approximately_correct_learning" title="wikilink"><em>probably approximately correct</em> learning</a> bound for semi-supervised learning of a Gaussian mixture was demonstrated by Ratsaby and Venkatesh in 1995 <a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>Semi-supervised learning has recently become more popular and practically relevant due to the variety of problems for which vast quantities of unlabeled data are available—e.g. text on websites, protein sequences, or images. For a review of recent work see a survey article by Zhu (2008).<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="methods-for-semi-supervised-learning">Methods for semi-supervised learning</h2>
<h3 id="generative-models">Generative models</h3>

<p>Generative approaches to statistical learning first seek to estimate 

<math display="inline" id="Semi-supervised_learning:8">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">|</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x|y)
  </annotation>
 </semantics>
</math>

, the distribution of data points belonging to each class. The probability 

<math display="inline" id="Semi-supervised_learning:9">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo stretchy="false">|</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(y|x)
  </annotation>
 </semantics>
</math>

 that a given point 

<math display="inline" id="Semi-supervised_learning:10">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 has label 

<math display="inline" id="Semi-supervised_learning:11">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 is then proportional to 

<math display="inline" id="Semi-supervised_learning:12">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">|</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x|y)p(y)
  </annotation>
 </semantics>
</math>

 by <a href="Bayes'_theorem" title="wikilink">Bayes' rule</a>. Semi-supervised learning with generative models can be viewed either as an extension of supervised learning (classification plus information about 

<math display="inline" id="Semi-supervised_learning:13">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x)
  </annotation>
 </semantics>
</math>

) or as an extension of unsupervised learning (clustering plus some labels).</p>

<p>Generative models assume that the distributions take some particular form 

<math display="inline" id="Semi-supervised_learning:14">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">|</mo>
    <mi>y</mi>
    <mo>,</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x|y,\theta)
  </annotation>
 </semantics>
</math>

 parameterized by the vector 

<math display="inline" id="Semi-supervised_learning:15">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta
  </annotation>
 </semantics>
</math>

. If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone. <a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> However, if the assumptions are correct, then the unlabeled data necessarily improves performance.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>

<p>The unlabeled data are distributed according to a mixture of individual-class distributions. In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions. Gaussian mixture distributions are identifiable and commonly used for generative models.</p>

<p>The parameterized <a href="joint_distribution" title="wikilink">joint distribution</a> can be written as 

<math display="inline" id="Semi-supervised_learning:16">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">|</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo stretchy="false">|</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">|</mo>
    <mi>y</mi>
    <mo>,</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x,y|\theta)=p(y|\theta)p(x|y,\theta)
  </annotation>
 </semantics>
</math>

 by using the <a href="Chain_rule_(probability)" title="wikilink">Chain rule</a>. Each parameter vector 

<math display="inline" id="Semi-supervised_learning:17">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta
  </annotation>
 </semantics>
</math>

 is associated with a decision function 

<math display="inline" id="Semi-supervised_learning:18">
 <semantics>
  <mrow>
   <msub>
    <mi>f</mi>
    <mi>θ</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mpadded width="+5pt">
    <munder accentunder="true">
     <mo>argmax</mo>
     <mo>𝑦</mo>
    </munder>
   </mpadded>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo stretchy="false">|</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>θ</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <ci>y</ci>
     <ci>argmax</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{\theta}(x)=\underset{y}{\operatorname{argmax}}\ p(y|x,\theta)
  </annotation>
 </semantics>
</math>

. The parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by 

<math display="inline" id="Semi-supervised_learning:19">
 <semantics>
  <mi>λ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>λ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

:</p>

<p>

<math display="block" id="Semi-supervised_learning:20">
 <semantics>
  <mrow>
   <munder accentunder="true">
    <mo>argmax</mo>
    <mo>Θ</mo>
   </munder>
   <mrow>
    <mo>(</mo>
    <mi>log</mi>
    <mi>p</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msubsup>
      <mrow>
       <mo stretchy="false">{</mo>
       <msub>
        <mi>x</mi>
        <mi>i</mi>
       </msub>
       <mo>,</mo>
       <msub>
        <mi>y</mi>
        <mi>i</mi>
       </msub>
       <mo stretchy="false">}</mo>
      </mrow>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>l</mi>
     </msubsup>
     <mo stretchy="false">|</mo>
     <mi>θ</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>+</mo>
    <mi>λ</mi>
    <mi>log</mi>
    <mi>p</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msubsup>
      <mrow>
       <mo stretchy="false">{</mo>
       <msub>
        <mi>x</mi>
        <mi>i</mi>
       </msub>
       <mo stretchy="false">}</mo>
      </mrow>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mrow>
        <mi>l</mi>
        <mo>+</mo>
        <mn>1</mn>
       </mrow>
      </mrow>
      <mrow>
       <mi>l</mi>
       <mo>+</mo>
       <mi>u</mi>
      </mrow>
     </msubsup>
     <mo stretchy="false">|</mo>
     <mi>θ</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <ci>normal-Θ</ci>
     <ci>argmax</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <log></log>
     <csymbol cd="unknown">p</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <cerror>
         <csymbol cd="ambiguous">fragments</csymbol>
         <ci>normal-{</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <ci>normal-,</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>y</ci>
          <ci>i</ci>
         </apply>
         <ci>normal-}</ci>
        </cerror>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>l</ci>
      </apply>
      <ci>normal-|</ci>
      <csymbol cd="unknown">θ</csymbol>
      <ci>normal-)</ci>
     </cerror>
     <plus></plus>
     <csymbol cd="unknown">λ</csymbol>
     <log></log>
     <csymbol cd="unknown">p</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <cerror>
         <csymbol cd="ambiguous">fragments</csymbol>
         <ci>normal-{</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <ci>normal-}</ci>
        </cerror>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <apply>
          <plus></plus>
          <ci>l</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </apply>
       <apply>
        <plus></plus>
        <ci>l</ci>
        <ci>u</ci>
       </apply>
      </apply>
      <ci>normal-|</ci>
      <csymbol cd="unknown">θ</csymbol>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \underset{\Theta}{\operatorname{argmax}}\left(\log p(\{x_{i},y_{i}\}_{i=1}^{l}%
|\theta)+\lambda\log p(\{x_{i}\}_{i=l+1}^{l+u}|\theta)\right)
  </annotation>
 </semantics>
</math>

 <a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h3 id="low-density-separation">Low-density separation</h3>

<p>Another major class of methods attempts to place boundaries in regions where there are few data points (labeled or unlabeled). One of the most commonly used algorithms is the <a href="Support_vector_machine#Transductive_support_vector_machines" title="wikilink">transductive support vector machine</a>, or TSVM (which, despite its name, may be used for inductive learning as well). Whereas <a href="support_vector_machines" title="wikilink">support vector machines</a> for supervised learning seek a decision boundary with maximal <a href="Margin_(machine_learning)" title="wikilink">margin</a> over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data. In addition to the standard <a href="hinge_loss" title="wikilink">hinge loss</a> 

<math display="inline" id="Semi-supervised_learning:21">
 <semantics>
  <msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mrow>
      <mi>y</mi>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>+</mo>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <times></times>
      <ci>y</ci>
      <ci>f</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <plus></plus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (1-yf(x))_{+}
  </annotation>
 </semantics>
</math>

 for labeled data, a loss function 

<math display="inline" id="Semi-supervised_learning:22">
 <semantics>
  <msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mrow>
      <mo stretchy="false">|</mo>
      <mrow>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo stretchy="false">|</mo>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>+</mo>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <abs></abs>
      <apply>
       <times></times>
       <ci>f</ci>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
    <plus></plus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (1-|f(x)|)_{+}
  </annotation>
 </semantics>
</math>

 is introduced over the unlabeled data by letting 

<math display="inline" id="Semi-supervised_learning:23">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mo>sign</mo>
     <mi>f</mi>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <apply>
     <times></times>
     <apply>
      <ci>sign</ci>
      <ci>f</ci>
     </apply>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=\operatorname{sign}{f(x)}
  </annotation>
 </semantics>
</math>

. TSVM then selects 

<math display="inline" id="Semi-supervised_learning:24">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>f</mi>
     <mo>*</mo>
    </msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msup>
      <mi>h</mi>
      <mo>*</mo>
     </msup>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mi>b</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>f</ci>
      <times></times>
     </apply>
     <ci>x</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>h</ci>
       <times></times>
      </apply>
      <ci>x</ci>
     </apply>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{*}(x)=h^{*}(x)+b
  </annotation>
 </semantics>
</math>

 from a <a href="reproducing_kernel_Hilbert_space" title="wikilink">reproducing kernel Hilbert space</a> 

<math display="inline" id="Semi-supervised_learning:25">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℋ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℋ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

 by minimizing the <a href="Regularization_(mathematics)" title="wikilink">regularized</a> <a href="Empirical_risk_minimization" title="wikilink">empirical risk</a>:</p>

<p>

<math display="block" id="Semi-supervised_learning:26">
 <semantics>
  <mrow>
   <msup>
    <mi>f</mi>
    <mo>*</mo>
   </msup>
   <mo>=</mo>
   <mrow>
    <munder accentunder="true">
     <mo>argmin</mo>
     <mo>𝑓</mo>
    </munder>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mrow>
       <munderover>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>l</mi>
       </munderover>
       <msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mrow>
          <mn>1</mn>
          <mo>-</mo>
          <mrow>
           <msub>
            <mi>y</mi>
            <mi>i</mi>
           </msub>
           <mi>f</mi>
           <mrow>
            <mo stretchy="false">(</mo>
            <msub>
             <mi>x</mi>
             <mi>i</mi>
            </msub>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
         </mrow>
         <mo stretchy="false">)</mo>
        </mrow>
        <mo>+</mo>
       </msub>
      </mrow>
      <mo>+</mo>
      <mrow>
       <msub>
        <mi>λ</mi>
        <mn>1</mn>
       </msub>
       <msubsup>
        <mrow>
         <mo fence="true">||</mo>
         <mi>h</mi>
         <mo fence="true">||</mo>
        </mrow>
        <mi class="ltx_font_mathcaligraphic">ℋ</mi>
        <mn>2</mn>
       </msubsup>
      </mrow>
      <mo>+</mo>
      <mrow>
       <msub>
        <mi>λ</mi>
        <mn>2</mn>
       </msub>
       <mrow>
        <munderover>
         <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
         <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mrow>
           <mi>l</mi>
           <mo>+</mo>
           <mn>1</mn>
          </mrow>
         </mrow>
         <mrow>
          <mi>l</mi>
          <mo>+</mo>
          <mi>u</mi>
         </mrow>
        </munderover>
        <msub>
         <mrow>
          <mo stretchy="false">(</mo>
          <mrow>
           <mn>1</mn>
           <mo>-</mo>
           <mrow>
            <mo stretchy="false">|</mo>
            <mrow>
             <mi>f</mi>
             <mrow>
              <mo stretchy="false">(</mo>
              <msub>
               <mi>x</mi>
               <mi>i</mi>
              </msub>
              <mo stretchy="false">)</mo>
             </mrow>
            </mrow>
            <mo stretchy="false">|</mo>
           </mrow>
          </mrow>
          <mo stretchy="false">)</mo>
         </mrow>
         <mo>+</mo>
        </msub>
       </mrow>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>f</ci>
     <times></times>
    </apply>
    <apply>
     <times></times>
     <apply>
      <ci>f</ci>
      <ci>argmin</ci>
     </apply>
     <apply>
      <plus></plus>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>l</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <minus></minus>
         <cn type="integer">1</cn>
         <apply>
          <times></times>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>y</ci>
           <ci>i</ci>
          </apply>
          <ci>f</ci>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>i</ci>
          </apply>
         </apply>
        </apply>
        <plus></plus>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>λ</ci>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <csymbol cd="latexml">norm</csymbol>
          <ci>h</ci>
         </apply>
         <ci>ℋ</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>λ</ci>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <sum></sum>
          <apply>
           <eq></eq>
           <ci>i</ci>
           <apply>
            <plus></plus>
            <ci>l</ci>
            <cn type="integer">1</cn>
           </apply>
          </apply>
         </apply>
         <apply>
          <plus></plus>
          <ci>l</ci>
          <ci>u</ci>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <minus></minus>
          <cn type="integer">1</cn>
          <apply>
           <abs></abs>
           <apply>
            <times></times>
            <ci>f</ci>
            <apply>
             <csymbol cd="ambiguous">subscript</csymbol>
             <ci>x</ci>
             <ci>i</ci>
            </apply>
           </apply>
          </apply>
         </apply>
         <plus></plus>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{*}=\underset{f}{\operatorname{argmin}}\left(\displaystyle\sum_{i=1}^{l}(1-y%
_{i}f(x_{i}))_{+}+\lambda_{1}||h||_{\mathcal{H}}^{2}+\lambda_{2}\sum_{i=l+1}^{%
l+u}(1-|f(x_{i})|)_{+}\right)
  </annotation>
 </semantics>
</math>

</p>

<p>An exact solution is intractable due to the non-<a href="convex_function" title="wikilink">convex</a> term 

<math display="inline" id="Semi-supervised_learning:27">
 <semantics>
  <msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mrow>
      <mo stretchy="false">|</mo>
      <mrow>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo stretchy="false">|</mo>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>+</mo>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <abs></abs>
      <apply>
       <times></times>
       <ci>f</ci>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
    <plus></plus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (1-|f(x)|)_{+}
  </annotation>
 </semantics>
</math>

, so research has focused on finding useful approximations.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>

<p>Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).</p>
<h3 id="graph-based-methods">Graph-based methods</h3>

<p>Graph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its 

<math display="inline" id="Semi-supervised_learning:28">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

 nearest neighbors or to examples within some distance 

<math display="inline" id="Semi-supervised_learning:29">
 <semantics>
  <mi>ϵ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ϵ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \epsilon
  </annotation>
 </semantics>
</math>

. The weight 

<math display="inline" id="Semi-supervised_learning:30">
 <semantics>
  <msub>
   <mi>W</mi>
   <mrow>
    <mi>i</mi>
    <mi>j</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>W</ci>
    <apply>
     <times></times>
     <ci>i</ci>
     <ci>j</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W_{ij}
  </annotation>
 </semantics>
</math>

 of an edge between 

<math display="inline" id="Semi-supervised_learning:31">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Semi-supervised_learning:32">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{j}
  </annotation>
 </semantics>
</math>

 is then set to 

<math display="inline" id="Semi-supervised_learning:33">
 <semantics>
  <msup>
   <mi>e</mi>
   <mfrac>
    <mrow>
     <mo>-</mo>
     <msup>
      <mrow>
       <mo fence="true">||</mo>
       <mrow>
        <msub>
         <mi>x</mi>
         <mi>i</mi>
        </msub>
        <mo>-</mo>
        <msub>
         <mi>x</mi>
         <mi>j</mi>
        </msub>
       </mrow>
       <mo fence="true">||</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mrow>
    <mi>ϵ</mi>
   </mfrac>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>e</ci>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <apply>
         <minus></minus>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>j</ci>
         </apply>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <ci>ϵ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   e^{\frac{-||x_{i}-x_{j}||^{2}}{\epsilon}}
  </annotation>
 </semantics>
</math>

.</p>

<p>Within the framework of <em>manifold regularization</em>, <a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> <a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> the graph serves as a proxy for the manifold. A term is added to the standard <a href="Tikhonov_regularization" title="wikilink">Tikhonov regularization</a> problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes</p>

<p>

<math display="block" id="Semi-supervised_learning:34">
 <semantics>
  <mrow>
   <munder accentunder="true">
    <mo>argmin</mo>
    <mrow>
     <mi>f</mi>
     <mo>∈</mo>
     <mi class="ltx_font_mathcaligraphic">ℋ</mi>
    </mrow>
   </munder>
   <mrow>
    <mo>(</mo>
    <mrow>
     <mrow>
      <mfrac>
       <mn>1</mn>
       <mi>l</mi>
      </mfrac>
      <mrow>
       <munderover>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>l</mi>
       </munderover>
       <mrow>
        <mi>V</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mrow>
          <mi>f</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <msub>
            <mi>x</mi>
            <mi>i</mi>
           </msub>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
         <mo>,</mo>
         <msub>
          <mi>y</mi>
          <mi>i</mi>
         </msub>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
     <mo>+</mo>
     <mrow>
      <msub>
       <mi>λ</mi>
       <mi>A</mi>
      </msub>
      <msubsup>
       <mrow>
        <mo fence="true">||</mo>
        <mi>f</mi>
        <mo fence="true">||</mo>
       </mrow>
       <mi class="ltx_font_mathcaligraphic">ℋ</mi>
       <mn>2</mn>
      </msubsup>
     </mrow>
     <mo>+</mo>
     <mrow>
      <msub>
       <mi>λ</mi>
       <mi>I</mi>
      </msub>
      <mrow>
       <msub>
        <mo largeop="true" symmetric="true">∫</mo>
        <mi class="ltx_font_mathcaligraphic">ℳ</mi>
       </msub>
       <mrow>
        <msup>
         <mrow>
          <mo fence="true">||</mo>
          <mrow>
           <mrow>
            <msub>
             <mo>∇</mo>
             <mi class="ltx_font_mathcaligraphic">ℳ</mi>
            </msub>
            <mi>f</mi>
           </mrow>
           <mrow>
            <mo stretchy="false">(</mo>
            <mi>x</mi>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
          <mo fence="true">||</mo>
         </mrow>
         <mn>2</mn>
        </msup>
        <mi>d</mi>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <apply>
      <in></in>
      <ci>f</ci>
      <ci>ℋ</ci>
     </apply>
     <ci>argmin</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>l</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>l</ci>
       </apply>
       <apply>
        <times></times>
        <ci>V</ci>
        <interval closure="open">
         <apply>
          <times></times>
          <ci>f</ci>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>i</ci>
          </apply>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>y</ci>
          <ci>i</ci>
         </apply>
        </interval>
       </apply>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>A</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="latexml">norm</csymbol>
         <ci>f</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <ci>ℋ</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>I</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <int></int>
        <ci>ℳ</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="latexml">norm</csymbol>
          <apply>
           <times></times>
           <apply>
            <apply>
             <csymbol cd="ambiguous">subscript</csymbol>
             <ci>normal-∇</ci>
             <ci>ℳ</ci>
            </apply>
            <ci>f</ci>
           </apply>
           <ci>x</ci>
          </apply>
         </apply>
         <cn type="integer">2</cn>
        </apply>
        <ci>d</ci>
        <ci>p</ci>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \underset{f\in\mathcal{H}}{\operatorname{argmin}}\left(\frac{1}{l}%
\displaystyle\sum_{i=1}^{l}V(f(x_{i}),y_{i})+\lambda_{A}||f||^{2}_{\mathcal{H}%
}+\lambda_{I}\int_{\mathcal{M}}||\nabla_{\mathcal{M}}f(x)||^{2}dp(x)\right)
  </annotation>
 </semantics>
</math>

 <a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>

<p>where 

<math display="inline" id="Semi-supervised_learning:35">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℋ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℋ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

 is a reproducing kernel Hilbert space and 

<math display="inline" id="Semi-supervised_learning:36">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℳ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℳ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{M}
  </annotation>
 </semantics>
</math>

 is the manifold on which the data lie. The regularization parameters 

<math display="inline" id="Semi-supervised_learning:37">
 <semantics>
  <msub>
   <mi>λ</mi>
   <mi>A</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>λ</ci>
    <ci>A</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda_{A}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Semi-supervised_learning:38">
 <semantics>
  <msub>
   <mi>λ</mi>
   <mi>I</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>λ</ci>
    <ci>I</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda_{I}
  </annotation>
 </semantics>
</math>

 control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the <a href="Laplacian_matrix" title="wikilink">graph Laplacian</a> 

<math display="inline" id="Semi-supervised_learning:39">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mo>=</mo>
   <mrow>
    <mi>D</mi>
    <mo>-</mo>
    <mi>W</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>L</ci>
    <apply>
     <minus></minus>
     <ci>D</ci>
     <ci>W</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L=D-W
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Semi-supervised_learning:40">
 <semantics>
  <mrow>
   <msub>
    <mi>D</mi>
    <mrow>
     <mi>i</mi>
     <mi>i</mi>
    </mrow>
   </msub>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>j</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mrow>
      <mi>l</mi>
      <mo>+</mo>
      <mi>u</mi>
     </mrow>
    </msubsup>
    <msub>
     <mi>W</mi>
     <mrow>
      <mi>i</mi>
      <mi>j</mi>
     </mrow>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>D</ci>
     <apply>
      <times></times>
      <ci>i</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>j</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <apply>
       <plus></plus>
       <ci>l</ci>
       <ci>u</ci>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>W</ci>
      <apply>
       <times></times>
       <ci>i</ci>
       <ci>j</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{ii}=\sum_{j=1}^{l+u}W_{ij}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Semi-supervised_learning:41">
 <semantics>
  <mi>𝐟</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐟</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{f}
  </annotation>
 </semantics>
</math>

 the vector 

<math display="inline" id="Semi-supervised_learning:42">
 <semantics>
  <mrow>
   <mo stretchy="false">[</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>x</mi>
      <mn>1</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi mathvariant="normal">…</mi>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>x</mi>
      <mrow>
       <mi>l</mi>
       <mo>+</mo>
       <mi>u</mi>
      </mrow>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo stretchy="false">]</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">delimited-[]</csymbol>
    <apply>
     <times></times>
     <ci>f</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-…</ci>
     <ci>f</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <apply>
       <plus></plus>
       <ci>l</ci>
       <ci>u</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   [f(x_{1})\dots f(x_{l+u})]
  </annotation>
 </semantics>
</math>

, we have</p>

<p>

<math display="block" id="Semi-supervised_learning:43">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>𝐟</mi>
     <mi>T</mi>
    </msup>
    <mi>L</mi>
    <mi>𝐟</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <mrow>
       <mi>i</mi>
       <mo>,</mo>
       <mi>j</mi>
      </mrow>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mrow>
      <mi>l</mi>
      <mo>+</mo>
      <mi>u</mi>
     </mrow>
    </munderover>
    <mrow>
     <msub>
      <mi>W</mi>
      <mrow>
       <mi>i</mi>
       <mi>j</mi>
      </mrow>
     </msub>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <msub>
         <mi>f</mi>
         <mi>i</mi>
        </msub>
        <mo>-</mo>
        <msub>
         <mi>f</mi>
         <mi>j</mi>
        </msub>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
   <mo>≈</mo>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">∫</mo>
     <mi class="ltx_font_mathcaligraphic">ℳ</mi>
    </msub>
    <mrow>
     <msup>
      <mrow>
       <mo fence="true">||</mo>
       <mrow>
        <mrow>
         <msub>
          <mo>∇</mo>
          <mi class="ltx_font_mathcaligraphic">ℳ</mi>
         </msub>
         <mi>f</mi>
        </mrow>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo fence="true">||</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mi>d</mi>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>𝐟</ci>
       <ci>T</ci>
      </apply>
      <ci>L</ci>
      <ci>𝐟</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <list>
          <ci>i</ci>
          <ci>j</ci>
         </list>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <plus></plus>
        <ci>l</ci>
        <ci>u</ci>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>W</ci>
        <apply>
         <times></times>
         <ci>i</ci>
         <ci>j</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <minus></minus>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>f</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>f</ci>
          <ci>j</ci>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <approx></approx>
     <share href="#.cmml">
     </share>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <int></int>
       <ci>ℳ</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="latexml">norm</csymbol>
         <apply>
          <times></times>
          <apply>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>normal-∇</ci>
            <ci>ℳ</ci>
           </apply>
           <ci>f</ci>
          </apply>
          <ci>x</ci>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <ci>d</ci>
       <ci>p</ci>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{f}^{T}L\mathbf{f}=\displaystyle\sum_{i,j=1}^{l+u}W_{ij}(f_{i}-f_{j})^{%
2}\approx\int_{\mathcal{M}}||\nabla_{\mathcal{M}}f(x)||^{2}dp(x)
  </annotation>
 </semantics>
</math>

.</p>

<p>The Laplacian can also be used to extend the supervised learning algorithms： regularized least squares and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.</p>
<h3 id="heuristic-approaches">Heuristic approaches</h3>

<p>Some methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework. For instance, the labeled and unlabeled examples 

<math display="inline" id="Semi-supervised_learning:44">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>l</mi>
     <mo>+</mo>
     <mi>u</mi>
    </mrow>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <apply>
      <plus></plus>
      <ci>l</ci>
      <ci>u</ci>
     </apply>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1},\dots,x_{l+u}
  </annotation>
 </semantics>
</math>

 may inform a choice of representation, <a href="distance_metric" title="wikilink">distance metric</a>, or <a href="Kernel(statistics)#In_non-parametric_statistics" title="wikilink">kernel</a> for the data in an unsupervised first step. Then supervised learning proceeds from only the labeled examples.</p>

<p><em>Self-training</em> is a wrapper method for semi-supervised learning. First a supervised learning algorithm is used to select a classifier based on the labeled data only. This classifier is then applied to the unlabeled data to generate more labeled examples as input for another supervised learning problem. Generally only the labels the classifier is most confident of are added at each step.</p>

<p><a class="uri" href="Co-training" title="wikilink">Co-training</a> is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.</p>
<h2 id="semi-supervised-learning-in-human-cognition">Semi-supervised learning in human cognition</h2>

<p>Human responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data (for a summary see <a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a>). More natural learning problems may also be viewed as instances of semi-supervised learning. Much of human <a href="concept_learning" title="wikilink">concept learning</a> involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).</p>

<p>Human infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> More recent work has shown that infants and children take into account not only the unlabeled examples available, but the <a href="sampling_(statistics)" title="wikilink">sampling</a> process from which labeled examples arise.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a><a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="PU_learning" title="wikilink">PU learning</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://manifold.cs.uchicago.edu/manifold_regularization/software.html">1</a> A freely available <a class="uri" href="MATLAB" title="wikilink">MATLAB</a> implementation of the graph-based semi-supervised algorithms Laplacian support vector machines and Laplacian regularized least squares.</li>
</ul>

<p>"</p>

<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">Stevens, K.N.(2000), Acoustic Phonetics, MIT Press, ISBN 0-262-69250-3, 978-0-262-69250-2<a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4">Scudder, H.J. Probability of Error of Some Adaptive Pattern-Recognition Machines. IEEE Transaction on Information Theory, 11:363–371 (1965). Cited in Chapelle et al. 2006, page 3.<a href="#fnref4">↩</a></li>
<li id="fn5">Vapnik, V. and Chervonenkis, A. Theory of Pattern Recognition [in Russian]. Nauka, Moscow (1974). Cited in Chapelle et al. 2006, page 3.<a href="#fnref5">↩</a></li>
<li id="fn6">Ratsaby, J. and Venkatesh, S. Learning from a mixture of labeled and unlabeled examples with parametric side information. In <em>Proceedings of the Eighth Annual Conference on Computational Learning Theory</em>, pages 412-417 (1995). Cited in Chapelle et al. 2006, page 4.<a href="#fnref6">↩</a></li>
<li id="fn7">Zhu, Xiaojin. <a href="http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf">Semi-supervised learning literature survey</a>. Computer Sciences, University of Wisconsin-Madison (2008).<a href="#fnref7">↩</a></li>
<li id="fn8">Cozman, F. and Cohen, I. Risks of semi-supervised learning: how unlabeled data can degrade performance of generative classifiers. In: Chapelle et al. (2006).<a href="#fnref8">↩</a></li>
<li id="fn9"></li>
<li id="fn10">Zhu, Xiaojin. <a href="http://pages.cs.wisc.edu/~jerryzhu/pub/SSL_EoML.pdf">Semi-Supervised Learning</a> University of Wisconsin-Madison.<a href="#fnref10">↩</a></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13">M. Belkin, P. Niyogi, V. Sindhwani. On Manifold Regularization. AISTATS 2005.<a href="#fnref13">↩</a></li>
<li id="fn14"></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
<li id="fn16">Younger, B. A. and Fearing, D. D. (1999), Parsing Items into Separate Categories: Developmental Change in Infant Categorization. Child Development, 70: 291–303.<a href="#fnref16">↩</a></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18"><a href="#fnref18">↩</a></li>
</ol>
</section>
</body>
</html>
