<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1742">History of statistics</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>History of statistics</h1>
<hr/>

<p>The <strong>History of statistics</strong> can be said to start around 1749 although, over time, there have been changes to the interpretation of the word <em><a class="uri" href="statistics" title="wikilink">statistics</a></em>. In early times, the meaning was restricted to information about <a href="Sovereign_state" title="wikilink">states</a>. This was later extended to include all collections of information of all types, and later still it was extended to include the analysis and interpretation of such data. In modern terms, "statistics" means both sets of collected information, as in <a href="national_accounts" title="wikilink">national accounts</a> and <a href="temperature_record" title="wikilink">temperature records</a>, and analytical work which requires <a href="statistical_inference" title="wikilink">statistical inference</a>.</p>

<p>Statistical activities are often associated with models expressed using <a href="probability" title="wikilink">probabilities</a>, and require <a href="probability_theory" title="wikilink">probability theory</a> for them to be put on a firm <a href="Theory_(mathematical_logic)" title="wikilink">theoretical basis</a>: see <a href="History_of_probability" title="wikilink">History of probability</a>.</p>

<p>A number of statistical concepts have had an important impact on a wide range of sciences. These include the <a href="design_of_experiments" title="wikilink">design of experiments</a> and approaches to statistical inference such as <a href="Bayesian_inference" title="wikilink">Bayesian inference</a>, each of which can be considered to have their own sequence in the development of the ideas underlying modern statistics.</p>
<h2 id="introduction">Introduction</h2>

<p>By the 18th century, the term "<a class="uri" href="statistics" title="wikilink">statistics</a>" designated the <a href="official_statistics" title="wikilink">systematic collection</a> of <a class="uri" href="demographic" title="wikilink">demographic</a> and <a href="economics" title="wikilink">economic</a> data by states. For at least two millennia, these data were mainly tabulations of human and material resources that might be taxed or put to military use. In the early 19th century, collection intensified, and the meaning of "statistics" broadened to include the discipline concerned with the collection, summary, and analysis of data. Today, data are collected and statistics are computed and widely distributed in government, business, most of the sciences and sports, and even for many pastimes. Electronic <a href="computer" title="wikilink">computers</a> have expedited more elaborate <a href="computational_statistics" title="wikilink">statistical computation</a> even as they have facilitated the collection and aggregation of data. A single data analyst may have available a set of data-files with millions of records, each with dozens or hundreds of separate measurements. These were collected over time from computer activity (for example, a stock exchange) or from computerized sensors, point-of-sale registers, and so on. Computers then produce simple, accurate summaries, and allow more tedious analyses, such as those that require inverting a large matrix or perform hundreds of steps of iteration, that would never be attempted by hand. Faster computing has allowed statisticians to develop "computer-intensive" methods which may look at all permutations, or use randomization to look at 10,000 permutations of a problem, to estimate answers that are not easy to quantify by theory alone.</p>

<p>The term "<a href="mathematical_statistics" title="wikilink">mathematical statistics</a>" designates the mathematical theories of <a href="probability_theory" title="wikilink">probability</a> and <a href="statistical_inference" title="wikilink">statistical inference</a>, which are used in <a href="applied_statistics" title="wikilink">statistical practice</a>. The relation between statistics and probability theory developed rather late, however. In the 19th century, statistics increasingly used <a href="probability_theory" title="wikilink">probability theory</a>, whose initial results were found in the 17th and 18th centuries, particularly in the analysis of <a href="games_of_chance" title="wikilink">games of chance</a> (gambling). By 1800, astronomy used probability models and statistical theories, particularly the <a href="method_of_least_squares" title="wikilink">method of least squares</a>. Early probability theory and statistics was systematized in the 19th century and statistical reasoning and probability models were used by social scientists to advance the new sciences of <a href="experimental_psychology" title="wikilink">experimental psychology</a> and <a class="uri" href="sociology" title="wikilink">sociology</a>, and by physical scientists in <a class="uri" href="thermodynamics" title="wikilink">thermodynamics</a> and <a href="statistical_mechanics" title="wikilink">statistical mechanics</a>. The development of statistical reasoning was closely associated with the development of <a href="inductive_logic" title="wikilink">inductive logic</a> and the <a href="scientific_method" title="wikilink">scientific method</a>, which are concerns that move statisticians away from the narrower area of mathematical statistics. Much of the theoretical work was readily available by the time computers were available to exploit them. By the 1970s, <a href="Samuel_Kotz" title="wikilink">Johnson and Kotz</a> produced a four-volume Compendium on Statistical Distributions (First Edition 1969-1972), which is still an invaluable resource.</p>

<p>Applied statistics can be regarded as not a field of <a class="uri" href="mathematics" title="wikilink">mathematics</a> but an autonomous <a href="mathematical_science" title="wikilink">mathematical science</a>, like <a href="computer_science" title="wikilink">computer science</a> and <a href="operations_research" title="wikilink">operations research</a>. Unlike mathematics, statistics had its origins in <a href="public_administration" title="wikilink">public administration</a>. Applications arose early in <a class="uri" href="demography" title="wikilink">demography</a> and <a class="uri" href="economics" title="wikilink">economics</a>; large areas of micro- and macro-economics today are "statistics" with an emphasis on time-series analyses. With its emphasis on learning from data and making best predictions, statistics also has been shaped by areas of academic research including psychological testing, medicine and <a class="uri" href="epidemiology" title="wikilink">epidemiology</a>. The ideas of statistical testing have considerable overlap with <a href="decision_science" title="wikilink">decision science</a>. With its concerns with searching and effectively presenting <a class="uri" href="data" title="wikilink">data</a>, statistics has overlap with <a href="information_science" title="wikilink">information science</a> and <a href="computer_science" title="wikilink">computer science</a>.</p>
<h2 id="etymology">Etymology</h2>
<dl>
<dd><em>Look up <strong><a href="wikt:statistics" title="wikilink">statistics</a></strong> in <a class="uri" href="wiktionary" title="wikilink">wiktionary</a>, the free dictionary.</em>
</dd>
</dl>

<p>The term <em>statistics</em> is ultimately derived from the <a href="New_Latin" title="wikilink">New Latin</a> <em>statisticum collegium</em> ("council of state") and the <a href="Italian_language" title="wikilink">Italian</a> word <em>statista</em> ("statesman" or "<a class="uri" href="politician" title="wikilink">politician</a>"). The <a href="German_language" title="wikilink">German</a> <em>Statistik</em>, first introduced by <a href="Gottfried_Achenwall" title="wikilink">Gottfried Achenwall</a> (1749), originally designated the analysis of <a class="uri" href="data" title="wikilink">data</a> about the <a href="State_(polity)" title="wikilink">state</a>, signifying the "science of state" (then called <em>political arithmetic</em> in English). It acquired the meaning of the collection and classification of data generally in the early 19th century. It was introduced into English in 1791 by <a href="Sir_John_Sinclair" title="wikilink">Sir John Sinclair</a> when he published the first of 21 volumes titled <em>Statistical Account of Scotland</em>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>Thus, the original principal purpose of <em>Statistik</em> was data to be used by governmental and (often centralized) administrative bodies. The collection of data about states and localities continues, largely through <a href="List_of_national_and_international_statistical_services" title="wikilink">national and international statistical services</a>. In particular, <a class="uri" href="censuses" title="wikilink">censuses</a> provide frequently updated information about the <a class="uri" href="population" title="wikilink">population</a>.</p>

<p>The first book to have 'statistics' in its title was "Contributions to Vital Statistics" (1845) by Francis GP Neison, actuary to the Medical Invalid and General Life Office.</p>
<h2 id="origins-in-probability-theory">Origins in probability theory</h2>

<p>Basic forms of statistics have been used since the beginning of civilization. Early empires often collated censuses of the population or recorded the trade in various commodities. The <a href="Roman_Empire" title="wikilink">Roman Empire</a> was one of the first states to extensively gather data on the size of the empire's population, geographical area and wealth.</p>

<p>The use of statistical methods dates back to least to the 5th century BCE. The historian <a class="uri" href="Thucydides" title="wikilink">Thucydides</a> in his <em><a href="History_of_the_Peloponnesian_War" title="wikilink">History of the Peloponnesian War</a></em> <a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> describes how the Athenians calculated the height of the wall of <a class="uri" href="Platea" title="wikilink">Platea</a> by counting the number of bricks in an unplastered section of the wall sufficiently near them to be able to count them. The count was repeated several times by a number of soldiers. The most frequent value (in modern terminology - the <a href="Mode_(statistics)" title="wikilink">mode</a> ) so determined was taken to be the most likely value of the number of bricks. Multiplying this value by the height of the bricks used in the wall allowed the Athenians to determine the height of the ladders necessary to scale the walls.</p>

<p>In the Indian epic - the <a class="uri" href="Mahabharata" title="wikilink">Mahabharata</a> (Book 3: The Story of Nala) - King Rtuparna estimated the number of <a class="uri" href="fruit" title="wikilink">fruit</a> and <a href="Leaf" title="wikilink">leaves</a> (2095 fruit and 50,000,000 - five <a href="crore" title="wikilink">crores</a> - leaves) on two great branches of a <a href="Vibhitaka_tree" title="wikilink">Vibhitaka tree</a> by counting them on a single twig. This number was then multiplied by the number of twigs on the branches. This estimate was later checked and found to be very close to the actual number. With knowledge of this method Nala was subsequently able to regain his kingdom.</p>

<p>The earliest writing on statistics was found in a 9th-century book entitled: "Manuscript on Deciphering Cryptographic Messages", written by <a class="uri" href="Al-Kindi" title="wikilink">Al-Kindi</a> (801–873 CE). In his book, Al-Kindi gave a detailed description of how to use <a class="uri" href="statistics" title="wikilink">statistics</a> and <a href="frequency_analysis" title="wikilink">frequency analysis</a> to decipher encrypted messages. This text arguably gave rise to the birth of both statistics and cryptanalysis.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>The <a href="Trial_of_the_Pyx" title="wikilink">Trial of the Pyx</a> is a test of the purity of the coinage of the <a href="Royal_Mint" title="wikilink">Royal Mint</a> which has been held on a regular basis since the 12th century. The Trial itself is based on statistical sampling methods. After minting a series of coins - originally from ten pounds of silver - a single coin was placed in the Pyx - a box in <a href="Westminster_Abbey" title="wikilink">Westminster Abbey</a>. After a given period - now once a year - the coins are removed and weighed. A sample of coins removed from the box are then tested for purity.</p>

<p>The <em><a href="Nuova_Cronica" title="wikilink">Nuova Cronica</a></em>, a 14th-century <a href="history_of_Florence" title="wikilink">history of Florence</a> by the Florentine banker and official <a href="Giovanni_Villani" title="wikilink">Giovanni Villani</a>, includes much statistical information on population, ordinances, commerce and trade, education, and religious facilities and has been described as the first introduction of statistics as a positive element in history,<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> though neither the term nor the concept of statistics as a specific field yet existed. But this was proven to be incorrect after the rediscovery of <a class="uri" href="Al-Kindi" title="wikilink">Al-Kindi</a>'s book on <a href="frequency_analysis" title="wikilink">frequency analysis</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>

<p>The arithmetic <a class="uri" href="mean" title="wikilink">mean</a>, although a concept known to the Greeks, was not generalised to more than two values until the 16th century. The invention of the decimal system by <a href="Simon_Stevin" title="wikilink">Simon Stevin</a> in 1585 seems likely to have facilitated these calculations. This method was first adopted in astronomy by <a href="Tycho_Brahe" title="wikilink">Tycho Brahe</a> who was attempting to reduce the errors in his estimates of the locations of various celestial bodies.</p>

<p>The idea of the <a class="uri" href="median" title="wikilink">median</a> originated in <a href="Edward_Wright_(mathematician)" title="wikilink">Edward Wright</a>'s book on navigation (<em>Certaine Errors in Navigation</em>) in 1599 in a section concerning the determination of location with a compass. Wright felt that this value was the most likely to be the correct value in a series of observations.</p>
<figure><b>(Figure)</b>
<figcaption>Sir <a href="William_Petty" title="wikilink">William Petty</a>, a 17th-century economist who used early statistical methods to analyse demographic data.</figcaption>
</figure>

<p>The birth of statistics is often dated to 1662, when <a href="John_Graunt" title="wikilink">John Graunt</a>, along with <a href="William_Petty" title="wikilink">William Petty</a>, developed early human statistical and <a class="uri" href="census" title="wikilink">census</a> methods that provided a framework for modern <a class="uri" href="demography" title="wikilink">demography</a>. He produced the first <a href="life_table" title="wikilink">life table</a>, giving probabilities of survival to each age. His book <em>Natural and Political Observations Made upon the Bills of Mortality</em> used analysis of the <a href="Mortality_rate" title="wikilink">mortality</a> rolls to make the first statistically based estimation of the population of <a class="uri" href="London" title="wikilink">London</a>. He knew that there were around 13,000 funerals per year in London and that three people died per eleven families per year. He estimated from the parish records that the average family size was 8 and calculated that the population of London was about 384,000. <a class="uri" href="Laplace" title="wikilink">Laplace</a> in 1802 estimated the population of France with a similar method.</p>

<p>Although the original scope of statistics was limited to data useful for governance, the approach was extended to many fields of a scientific or commercial nature during the 19th century. The mathematical foundations for the subject heavily drew on the new <a href="probability_theory" title="wikilink">probability theory</a>, pioneered in the 16th century in the correspondence amongst <a href="Gerolamo_Cardano" title="wikilink">Gerolamo Cardano</a>, <a href="Pierre_de_Fermat" title="wikilink">Pierre de Fermat</a> and <a href="Blaise_Pascal" title="wikilink">Blaise Pascal</a>. <a href="Christiaan_Huygens" title="wikilink">Christiaan Huygens</a> (1657) gave the earliest known scientific treatment of the subject. <a href="Jakob_Bernoulli" title="wikilink">Jakob Bernoulli</a>'s <em><a href="Ars_Conjectandi" title="wikilink">Ars Conjectandi</a></em> (posthumous, 1713) and <a href="Abraham_de_Moivre" title="wikilink">Abraham de Moivre</a>'s <em><a href="The_Doctrine_of_Chances" title="wikilink">The Doctrine of Chances</a></em> (1718) treated the subject as a branch of mathematics. In his book Bernoulli introduced the idea of representing complete certainty as one and probability as a number between zero and one.</p>

<p>The formal study of <a href="theory_of_errors" title="wikilink">theory of errors</a> may be traced back to <a href="Roger_Cotes" title="wikilink">Roger Cotes</a>' <em>Opera Miscellanea</em> (posthumous, 1722), but a memoir prepared by <a href="Thomas_Simpson" title="wikilink">Thomas Simpson</a> in 1755 (printed 1756) first applied the theory to the discussion of errors of observation. The reprint (1757) of this memoir lays down the <a href="axiom" title="wikilink">axioms</a> that positive and negative errors are equally probable, and that there are certain assignable limits within which all errors may be supposed to fall; continuous errors are discussed and a probability curve is given. Simpson discussed several possible distributions of error. He first considered the <a href="Discrete_uniform_distribution" title="wikilink">uniform distribution</a> and then the discrete symmetric <a href="triangular_distribution" title="wikilink">triangular distribution</a> followed by the continuous symmetric triangle distribution. <a href="Tobias_Mayer" title="wikilink">Tobias Mayer</a>, in his study of the <a class="uri" href="libration" title="wikilink">libration</a> of the <a class="uri" href="moon" title="wikilink">moon</a> (<em>Kosmographische Nachrichten</em>, Nuremberg, 1750), invented the first formal method for estimating the unknown quantities by generalized the averaging of observations under identical circumstances to the averaging of groups of similar equations.</p>

<p><a href="Ruder_Boškovic" title="wikilink">Ruder Boškovic</a> in 1755 based in his work on the shape of the earth proposed in his book <em>De Litteraria expeditione per pontificiam ditionem ad dimetiendos duos meridiani gradus a PP. Maire et Boscovicli</em> that the true value of a series of observations would be that which minimises the sum of absolute errors. In modern terminology this value is the median. The first example of what later became known as the normal curve was studied by <a href="Abraham_de_Moivre" title="wikilink">Abraham de Moivre</a> who plotted this curve on November 12, 1733.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> de Moivre was studying the number of heads that occurred when a 'fair' coin was tossed.</p>

<p>In 1761 <a href="Thomas_Bayes" title="wikilink">Thomas Bayes</a> proved <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a> and in 1765 <a href="Joseph_Priestley" title="wikilink">Joseph Priestley</a> invented the first <a class="uri" href="timeline" title="wikilink">timeline</a> charts.</p>

<p><a href="Johann_Heinrich_Lambert" title="wikilink">Johann Heinrich Lambert</a> in his 1765 book <em>Anlage zur Architectonic</em> proposed the <a class="uri" href="semicircle" title="wikilink">semicircle</a> as a distribution of errors:</p>

<p>

<math display="block" id="History_of_statistics:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mn>2</mn>
    </mfrac>
    <msqrt>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <msup>
        <mi>x</mi>
        <mn>2</mn>
       </msup>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </msqrt>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <root></root>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>x</ci>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=\frac{1}{2}\sqrt{(1-x^{2})}
  </annotation>
 </semantics>
</math>

</p>

<p>with -1 Laplace, P-S. (1774). "Mémoire sur la probabilité des causes par les évènements". <em>Mémoires de l'Académie Royale des Sciences Présentés par Divers Savants</em>, 6, 621–656<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> This distribution is now known as the <a href="Laplace_distribution" title="wikilink">Laplace distribution</a>. Lagrange proposed a <a href="parabolic_distribution" title="wikilink">parabolic distribution</a> of errors in 1776.</p>

<p>Laplace in 1778 published his second law of errors wherein he noted that the frequency of an error was proportional to the exponential of the square of its magnitude. This was subsequently rediscovered by <a class="uri" href="Gauss" title="wikilink">Gauss</a> (possibly in 1795) and is now best known as the <a href="normal_distribution" title="wikilink">normal distribution</a> which is of central importance in statistics.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> This distribution was first referred to as the <em>normal</em> distribution by Pierce in 1873 who was studying measurement errors when an object was dropped onto a wooden base.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> He chose the term <em>normal</em> because of its frequent occurrence in naturally occurring variables.</p>

<p>Lagrange also suggested in 1781 two other distributions for errors - a <a href="Raised_cosine_distribution" title="wikilink">Raised cosine distribution</a> and a <a href="logarithmic_distribution" title="wikilink">logarithmic distribution</a>.</p>

<p>Laplace gave (1781) a formula for the law of facility of error (a term due to <a href="Joseph_Louis_Lagrange" title="wikilink">Joseph Louis Lagrange</a>, 1774), but one which led to unmanageable equations. <a href="Daniel_Bernoulli" title="wikilink">Daniel Bernoulli</a> (1778) introduced the principle of the maximum product of the probabilities of a system of concurrent errors.</p>

<p>In 1786 <a href="William_Playfair" title="wikilink">William Playfair</a> (1759-1823) introduced the idea of graphical representation into statistics. He invented the <a href="line_chart" title="wikilink">line chart</a>, <a href="bar_chart" title="wikilink">bar chart</a> and <a class="uri" href="histogram" title="wikilink">histogram</a> and incorporated them into his works on <a class="uri" href="economics" title="wikilink">economics</a>, the <em>Commercial and Political Atlas</em>. This was followed in 1795 by his invention of the <a href="pie_chart" title="wikilink">pie chart</a> and circle chart which he used to display the evolution of England's imports and exports. These latter charts came to general attention when he published examples in his <em>Statistical Breviary</em> in 1801.</p>

<p>Laplace, in an investigation of the motions of <a class="uri" href="Saturn" title="wikilink">Saturn</a> and <a class="uri" href="Jupiter" title="wikilink">Jupiter</a> in 1787, generalized Mayer's method by using different linear combinations of a single group of equations.</p>

<p>In 1802 Laplace estimated the population of France to be 28,328,612.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> He calculated this figure using the number of births in the previous year and census data for three communities. The census data of these communities showed that they had 2,037,615 persons and that the number of births were 71,866. Assuming that these samples were representative of France, Laplace produced his estimate for the entire population.</p>

<p> The <a href="method_of_least_squares" title="wikilink">method of least squares</a>, which was used to minimize errors in data <a class="uri" href="measurement" title="wikilink">measurement</a>, was published independently by <a href="Adrien-Marie_Legendre" title="wikilink">Adrien-Marie Legendre</a> (1805), <a href="Robert_Adrain" title="wikilink">Robert Adrain</a> (1808), and <a href="Carl_Friedrich_Gauss" title="wikilink">Carl Friedrich Gauss</a> (1809). Gauss had used the method in his famous 1801 prediction of the location of the <a href="dwarf_planet" title="wikilink">dwarf planet</a> <a href="Ceres_(dwarf_planet)" title="wikilink">Ceres</a>. The observations that Gauss based his calculations on were made by the Italian monk Piazzi.</p>

<p>The term <em>probable error</em> (<em>der wahrscheinliche Fehler</em>) - the median deviation from the mean - was introduced in 1815 by the German astronomer <a href="Friedrich_Bessel" title="wikilink">Frederik Wilhelm Bessel</a>. <a href="Antoine_Augustin_Cournot" title="wikilink">Antoine Augustin Cournot</a> in 1843 was the first to use the term <em>median</em> (<em>valeur médiane</em>) for the value that divides a probability distribution into two equal halves.</p>

<p>Other contributors to the theory of errors were Ellis (1844), <a href="Augustus_De_Morgan" title="wikilink">De Morgan</a> (1864), <a href="James_Whitbread_Lee_Glaisher" title="wikilink">Glaisher</a> (1872), and <a href="Giovanni_Schiaparelli" title="wikilink">Giovanni Schiaparelli</a> (1875). Peters's (1856) formula for 

<math display="inline" id="History_of_statistics:1">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r
  </annotation>
 </semantics>
</math>

, the "probable error" of a single observation was widely used and inspired early <a href="robust_statistics" title="wikilink">robust statistics</a> (resistant to <a href="outlier" title="wikilink">outliers</a>: see <a href="Peirce's_criterion" title="wikilink">Peirce's criterion</a>).</p>

<p>In the 19th century authors on <a href="statistical_theory" title="wikilink">statistical theory</a> included Laplace, <a href="Sylvestre_Lacroix" title="wikilink">S. Lacroix</a> (1816), Littrow (1833), <a href="Richard_Dedekind" title="wikilink">Dedekind</a> (1860), Helmert (1872), <a href="Hermann_Laurent" title="wikilink">Laurent</a> (1873), Liagre, Didion, <a href="Augustus_De_Morgan" title="wikilink">De Morgan</a> and <a href="George_Boole" title="wikilink">Boole</a>.</p>

<p><a href="Gustav_Theodor_Fechner" title="wikilink">Gustav Theodor Fechner</a> used the median (<em>Centralwerth</em>) in sociological and psychological phenomena.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> It had earlier been used only in astronomy and related fields. <a href="Francis_Galton" title="wikilink">Francis Galton</a> used the English term <em>median</em> for the first time in 1881 having earlier used the terms <em>middle-most value</em> in 1869 and the <em>medium</em> in 1880.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>

<p><a href="Adolphe_Quetelet" title="wikilink">Adolphe Quetelet</a> (1796–1874), another important founder of statistics, introduced the notion of the "average man" (<em>l'homme moyen</em>) as a means of understanding complex social phenomena such as <a href="crime_rates" title="wikilink">crime rates</a>, <a href="marriage_rates" title="wikilink">marriage rates</a>, and <a href="suicide_rates" title="wikilink">suicide rates</a>.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>

<p>The first tests of the normal distribution were invented by the German statistician <a href="Wilhelm_Lexis" title="wikilink">Wilhelm Lexis</a> in the 1870s. The only data sets available to him that he was able to show were normally distributed were birth rates.</p>
<h3 id="development-of-modern-statistics">Development of modern statistics</h3>

<p>Although the origins of statistical theory lie in the 18th century advances in probability, the modern field of statistics only emerged in the late 19th and early 20th century in three stages. The first wave, at the turn of the century, was led by the work of Sir <a href="Francis_Galton" title="wikilink">Francis Galton</a> and <a href="Karl_Pearson" title="wikilink">Karl Pearson</a>, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. The second wave of the 1910s and 20s was initiated by <a href="William_Gosset" title="wikilink">William Gosset</a>, and reached its culmination in the insights of Sir <a href="Ronald_Fisher" title="wikilink">Ronald Fisher</a>. This involved the development of better experimental models, hypothesis testing and techniques for use with small data samples. The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between <a href="Egon_Pearson" title="wikilink">Egon Pearson</a> and <a href="Jerzy_Neyman" title="wikilink">Jerzy Neyman</a> in the 1930s.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology.</p>

<p> The first statistical bodies were established in the early 19th century. The <a href="Royal_Statistical_Society" title="wikilink">Royal Statistical Society</a> was founded in 1834 and <a href="Florence_Nightingale" title="wikilink">Florence Nightingale</a>, its first female member, pioneered the application of statistical analysis to health problems for the furtherance of epidemiological understanding and public health practice. However, the methods then used would not be considered as modern statistics today.</p>

<p>The <a href="Oxford_University" title="wikilink">Oxford</a> scholar <a href="Francis_Ysidro_Edgeworth" title="wikilink">Francis Ysidro Edgeworth</a>'s book, <em>Metretike: or The Method of Measuring Probability and Utility</em> (1887) dealt with probability as the basis of inductive reasoning, and his later works focused on the 'philosophy of chance'.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> His first paper on statistics (1883) explored the law of error (<a href="normal_distribution" title="wikilink">normal distribution</a>), and his <em>Methods of Statistics</em> (1885) introduced an early version of the <a href="Student's_t-distribution" title="wikilink">t distribution</a>, the <a href="Edgeworth_expansion" title="wikilink">Edgeworth expansion</a>, the <a href="Edgeworth_series" title="wikilink">Edgeworth series</a>, the method of variate transformation and the asymptotic theory of maximum likelihood estimates.</p>

<p>The Norwegian <a href="Anders_Nicolai_Kiær" title="wikilink">Anders Nicolai Kiær</a> introduced the concept of <a href="stratified_sampling" title="wikilink">stratified sampling</a> in 1895.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> <a href="Arthur_Lyon_Bowley" title="wikilink">Arthur Lyon Bowley</a> introduced new methods of data sampling in 1906 when working on social statistics. Although statistical surveys of social conditions had started with <a href="Charles_Booth_(philanthropist)" title="wikilink">Charles Booth</a>'s "Life and Labour of the People in London" (1889-1903) and Seebohm Rowntree's "Poverty, A Study of Town Life" (1901), Bowley's, key innovation consisted of the use of <a href="random_sampling" title="wikilink">random sampling</a> techniques. His efforts culminated in his <em>New Survey of London Life and Labour</em>.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a></p>

<p>Sir <a href="Francis_Galton" title="wikilink">Francis Galton</a> is credited as one of the principal founders of statistical theory. His contributions to the field included introducing the concepts of <a href="standard_deviation" title="wikilink">standard deviation</a>, <a class="uri" href="correlation" title="wikilink">correlation</a>, <a href="regression_analysis" title="wikilink">regression</a> and the application of these methods to the study of the variety of human characteristics - height, weight, eyelash length among others. He found that many of these could be fitted to a normal curve distribution.<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a></p>

<p>Galton submitted a paper to <em>Nature</em> in 1907 on the usefulness of the median.<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> He examined the accuracy of 787 guesses of the weight of an ox at a country fair. The actual weight was 1208 pounds: the median guess was 1198. The guesses were markedly non-normally distributed.</p>

<p> Galton's publication of <em>Natural Inheritance</em> in 1889 sparked the interest of a brilliant mathematician, <a href="Karl_Pearson" title="wikilink">Karl Pearson</a>,<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> then working at <a href="University_College_London" title="wikilink">University College London</a>, and he went on to found the discipline of mathematical statistics.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> He emphasised the statistical foundation of scientific laws and promoted its study and his laboratory attracted students from around the world attracted by his new methods of analysis, including <a href="Udny_Yule" title="wikilink">Udny Yule</a>. His work grew to encompass the fields of <a class="uri" href="biology" title="wikilink">biology</a>, <a class="uri" href="epidemiology" title="wikilink">epidemiology</a>, anthropometry, <a class="uri" href="medicine" title="wikilink">medicine</a> and social <a class="uri" href="history" title="wikilink">history</a>. In 1901, with <a href="Walter_Frank_Raphael_Weldon" title="wikilink">Walter Weldon</a>, founder of <a class="uri" href="biometry" title="wikilink">biometry</a>, and Galton, he founded the journal <em><a class="uri" href="Biometrika" title="wikilink">Biometrika</a></em> as the first journal of mathematical statistics and biometry.</p>

<p>His work, and that of Galton's, underpins many of the 'classical' statistical methods which are in common use today, including the <a href="Pearson_product-moment_correlation_coefficient" title="wikilink">Correlation coefficient</a>, defined as a product-moment;<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> the <a href="Method_of_moments_(statistics)" title="wikilink">method of moments</a> for the fitting of distributions to samples; <a href="Pearson_distribution" title="wikilink">Pearson's system of continuous curves</a> that forms the basis of the now conventional continuous probability distributions; <a href="Mahalanobis_distance" title="wikilink">Chi distance</a> a precursor and special case of the <a href="Mahalanobis_distance" title="wikilink">Mahalanobis distance</a><a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a> and <a class="uri" href="P-value" title="wikilink">P-value</a>, defined as the probability measure of the complement of the <a href="Ball_(mathematics)" title="wikilink">ball</a> with the hypothesized value as center point and chi distance as radius.<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> He also introduced the term 'standard deviation'.</p>

<p>He also founded the <a href="Statistical_hypothesis_testing" title="wikilink">statistical hypothesis testing theory</a>,<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> <a href="Pearson's_chi-squared_test" title="wikilink">Pearson's chi-squared test</a> and <a href="principal_component_analysis" title="wikilink">principal component analysis</a>.<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a><a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a> In 1911 he founded the world's first university statistics department at <a href="University_College_London" title="wikilink">University College London</a>.</p>

<p> The second wave of mathematical statistics was pioneered by Sir <a href="Ronald_Fisher" title="wikilink">Ronald Fisher</a> who wrote the textbooks that were to define the academic discipline in universities around the world. He also systematized previous results, putting them on a firm mathematical footing. His most important publications were his 1916 seminal paper <em><a href="The_Correlation_between_Relatives_on_the_Supposition_of_Mendelian_Inheritance" title="wikilink">The Correlation between Relatives on the Supposition of Mendelian Inheritance</a></em> and his classic 1925 work <em><a href="Statistical_Methods_for_Research_Workers" title="wikilink">Statistical Methods for Research Workers</a></em>. His paper was the first use to use the statistical term, <a class="uri" href="variance" title="wikilink">variance</a>. In 1919, at <a href="Rothamsted_Experimental_Station" title="wikilink">Rothamsted Experimental Station</a> he started a major study of the extensive collections of data recorded over many years. This resulted in a series of reports under the general title <em>Studies in Crop Variation.</em></p>

<p> Over the next seven years, he pioneered the principles of the <a href="design_of_experiments" title="wikilink">design of experiments</a> (see below) and elaborated his studies of analysis of variance. He furthered his studies of the statistics of small samples. Perhaps even more important, he began his systematic approach of the analysis of real data as the springboard for the development of new statistical methods. He developed computational algorithms for analyzing data from his balanced experimental designs. In 1925, this work resulted in the publication of his first book, <em><a href="Statistical_Methods_for_Research_Workers" title="wikilink">Statistical Methods for Research Workers</a></em>.<a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a> This book went through many editions and translations in later years, and it became the standard reference work for scientists in many disciplines. In 1935, this book was followed by <em><a href="The_Design_of_Experiments" title="wikilink">The Design of Experiments</a></em>, which was also widely used.</p>

<p>In addition to analysis of variance, Fisher named and promoted the method of <a href="maximum_likelihood" title="wikilink">maximum likelihood</a> estimation. Fisher also originated the concepts of <a href="sufficiency_(statistics)" title="wikilink">sufficiency</a>, <a href="ancillary_statistic" title="wikilink">ancillary statistics</a>, <a href="linear_discriminant_analysis" title="wikilink">Fisher's linear discriminator</a> and <a href="Fisher_information" title="wikilink">Fisher information</a>. His article <em>On a distribution yielding the error functions of several well known statistics</em> (1924) presented <a href="Pearson's_chi-squared_test" title="wikilink">Pearson's chi-squared test</a> and <a href="William_Gosset" title="wikilink">William Gosset</a>'s <a href="Student's_t-distribution" title="wikilink">t</a> in the same framework as the <a href="Gaussian_distribution" title="wikilink">Gaussian distribution</a>, and his own parameter in the analysis of variance <a href="Fisher's_z-distribution" title="wikilink">Fisher's z-distribution</a> (more commonly used decades later in the form of the <a href="F_distribution" title="wikilink">F distribution</a>).<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a> The 5% level of significance appears to have been introduced by Fisher in 1925.<a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a> Fisher stated that deviations exceeding twice the standard deviation are regarded as significant. Before this deviations exceeding three times the probable error were considered significant. For a symmetrical distribution the probable error is half the interquartile range. For a normal distribution the probable error is approximately 2/3 the standard deviation. It appears that Fisher's 5% criterion was rooted in previous practice.</p>

<p>Other important contributions at this time included <a href="Charles_Spearman" title="wikilink">Charles Spearman</a>'s <a href="Spearman's_rank_correlation_coefficient" title="wikilink">rank correlation coefficient</a> that was a useful extension of the Pearson correlation coefficient. <a href="William_Sealy_Gosset" title="wikilink">William Sealy Gosset</a>, the English statistician better known under his pseudonym of <em>Student</em>, introduced <a href="Student's_t-distribution" title="wikilink">Student's t-distribution</a>, a continuous probability distribution useful in situations where the sample size is small and population standard deviation is unknown.</p>

<p><a href="Egon_Pearson" title="wikilink">Egon Pearson</a> (Karl's son) and <a href="Jerzy_Neyman" title="wikilink">Jerzy Neyman</a> introduced the concepts of "<a href="Type_I_and_type_II_errors" title="wikilink">Type II</a>" error, power of a test and <a href="confidence_interval" title="wikilink">confidence intervals</a>. <a href="Jerzy_Neyman" title="wikilink">Jerzy Neyman</a> in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.<a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a></p>
<h2 id="design-of-experiments">Design of experiments</h2>

<p> In 1747, while serving as surgeon on HM Bark <em>Salisbury</em>, <a href="James_Lind_(physician)" title="wikilink">James Lind</a> carried out a controlled experiment to develop a cure for <a class="uri" href="scurvy" title="wikilink">scurvy</a>.<a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a> In this study his subjects' cases "were as similar as I could have them", that is he provided strict entry requirements to reduce extraneous variation. The men were paired, which provided <a href="Blocking_(statistics)" title="wikilink">blocking</a>. From a modern perspective, the main thing that is missing is randomized allocation of subjects to treatments.</p>

<p><a href="James_Lind" title="wikilink">James Lind</a> is today often described as a one-factor-at-a-time experimenter.<a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a> Similar one-factor-at-a-time (OFAT) experimentation was performed at the <a href="Rothamsted_Research_Station" title="wikilink">Rothamsted Research Station</a> in the 1840s by Sir <a href="John_Lawes" title="wikilink">John Lawes</a> to determine the optimal inorganic fertilizer for use on wheat.<a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a></p>

<p>A theory of statistical inference was developed by <a href="Charles_Sanders_Peirce" title="wikilink">Charles S. Peirce</a> in "<a href="Charles_Sanders_Peirce_bibliography#illus" title="wikilink">Illustrations of the Logic of Science</a>" (1877–1878) and "<a href="Charles_Sanders_Peirce_bibliography#SIL" title="wikilink">A Theory of Probable Inference</a>" (1883), two publications that emphasized the importance of randomization-based inference in statistics. In another study, Peirce randomly assigned volunteers to a <a href="blinding_(medicine)" title="wikilink">blinded</a>, <a href="repeated_measures_design" title="wikilink">repeated-measures design</a> to evaluate their ability to discriminate weights.<a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a><a class="footnoteRef" href="#fn38" id="fnref38"><sup>38</sup></a><a class="footnoteRef" href="#fn39" id="fnref39"><sup>39</sup></a><a class="footnoteRef" href="#fn40" id="fnref40"><sup>40</sup></a></p>

<p>Peirce's experiment inspired other researchers in psychology and education, which developed a research tradition of randomized experiments in laboratories and specialized textbooks in the 1800s.<a class="footnoteRef" href="#fn41" id="fnref41"><sup>41</sup></a><a class="footnoteRef" href="#fn42" id="fnref42"><sup>42</sup></a><a class="footnoteRef" href="#fn43" id="fnref43"><sup>43</sup></a><a class="footnoteRef" href="#fn44" id="fnref44"><sup>44</sup></a> Peirce also contributed the first English-language publication on an <a href="optimal_design" title="wikilink">optimal design</a> for <a href="Regression_analysis" title="wikilink">regression</a>-<a href="statistical_model" title="wikilink">models</a> in 1876.<a class="footnoteRef" href="#fn45" id="fnref45"><sup>45</sup></a> A pioneering <a href="optimal_design" title="wikilink">optimal design</a> for <a href="polynomial_regression" title="wikilink">polynomial regression</a> was suggested by <a href="Joseph_Diaz_Gergonne" title="wikilink">Gergonne</a> in 1815. In 1918 <a href="Kirstine_Smith" title="wikilink">Kirstine Smith</a> published optimal designs for polynomials of degree six (and less).<a class="footnoteRef" href="#fn46" id="fnref46"><sup>46</sup></a></p>

<p>The use of a sequence of experiments, where the design of each may depend on the results of previous experiments, including the possible decision to stop experimenting, was pioneered<a class="footnoteRef" href="#fn47" id="fnref47"><sup>47</sup></a> by <a href="Abraham_Wald" title="wikilink">Abraham Wald</a> in the context of sequential tests of statistical hypotheses.<a class="footnoteRef" href="#fn48" id="fnref48"><sup>48</sup></a> Surveys are available of optimal <a href="sequential_analysis" title="wikilink">sequential designs</a>,<a class="footnoteRef" href="#fn49" id="fnref49"><sup>49</sup></a> and of <a href="Minimisation_(clinical_trials)" title="wikilink">adaptive designs</a>.<a class="footnoteRef" href="#fn50" id="fnref50"><sup>50</sup></a> One specific type of sequential design is the "two-armed bandit", generalized to the <a href="multi-armed_bandit" title="wikilink">multi-armed bandit</a>, on which early work was done by <a href="Herbert_Robbins" title="wikilink">Herbert Robbins</a> in 1952.<a class="footnoteRef" href="#fn51" id="fnref51"><sup>51</sup></a></p>

<p>The term "design of experiments" (DOE) derives from early statistical work performed by <a href="Ronald_Fisher" title="wikilink">Sir Ronald Fisher</a>. He was described by <a href="Anders_Hald" title="wikilink">Anders Hald</a> as "a genius who almost single-handedly created the foundations for modern statistical science."<a class="footnoteRef" href="#fn52" id="fnref52"><sup>52</sup></a> Fisher initiated the principles of <a href="design_of_experiments" title="wikilink">design of experiments</a> and elaborated on his studies of "<a href="analysis_of_variance" title="wikilink">analysis of variance</a>". Perhaps even more important, Fisher began his systematic approach to the analysis of real data as the springboard for the development of new statistical methods. He began to pay particular attention to the labour involved in the necessary computations performed by hand, and developed methods that were as practical as they were founded in rigour. In 1925, this work culminated in the publication of his first book, <em><a href="Statistical_Methods_for_Research_Workers" title="wikilink">Statistical Methods for Research Workers</a></em>.<a class="footnoteRef" href="#fn53" id="fnref53"><sup>53</sup></a> This went into many editions and translations in later years, and became a standard reference work for scientists in many disciplines.<a class="footnoteRef" href="#fn54" id="fnref54"><sup>54</sup></a></p>

<p>A methodology for designing experiments was proposed by <a href="Ronald_Fisher" title="wikilink">Ronald A. Fisher</a>, in his innovative book <em><a href="The_Design_of_Experiments" title="wikilink">The Design of Experiments</a></em> (1935) which also became a standard.<a class="footnoteRef" href="#fn55" id="fnref55"><sup>55</sup></a><a class="footnoteRef" href="#fn56" id="fnref56"><sup>56</sup></a><a class="footnoteRef" href="#fn57" id="fnref57"><sup>57</sup></a><a class="footnoteRef" href="#fn58" id="fnref58"><sup>58</sup></a> As an example, he described how to test the <a class="uri" href="hypothesis" title="wikilink">hypothesis</a> that a certain lady could distinguish by flavour alone whether the milk or the tea was first placed in the cup. While this sounds like a frivolous application, it allowed him to illustrate the most important ideas of experimental design: see <a href="Lady_tasting_tea" title="wikilink">Lady tasting tea</a>.</p>

<p><a href="Agricultural_science" title="wikilink">Agricultural science</a> advances served to meet the combination of larger city populations and fewer farms. But for crop scientists to take due account of widely differing geographical growing climates and needs, it was important to differentiate local growing conditions. To extrapolate experiments on local crops to a national scale, they had to extend crop sample testing economically to overall populations. As statistical methods advanced (primarily the efficacy of designed experiments instead of one-factor-at-a-time experimentation), representative factorial design of experiments began to enable the meaningful extension, by inference, of experimental sampling results to the population as a whole. But it was hard to decide how representative was the crop sample chosen. Factorial design methodology showed how to estimate and correct for any random variation within the sample and also in the data collection procedures.</p>
<h2 id="bayesian-statistics">Bayesian statistics</h2>

<p> The term <em>Bayesian</em> refers to <a href="Thomas_Bayes" title="wikilink">Thomas Bayes</a> (1702–1761), who proved a special case of what is now called <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>. However it was <a href="Pierre-Simon_Laplace" title="wikilink">Pierre-Simon Laplace</a> (1749–1827) who introduced a general version of the theorem and applied it to <a href="celestial_mechanics" title="wikilink">celestial mechanics</a>, medical statistics, <a href="Reliability_(statistics)" title="wikilink">reliability</a>, and <a class="uri" href="jurisprudence" title="wikilink">jurisprudence</a>.<a class="footnoteRef" href="#fn59" id="fnref59"><sup>59</sup></a> When insufficient knowledge was available to specify an informed prior, Laplace used <a href="uniform_distribution_(continuous)" title="wikilink">uniform</a> priors, according to his "<a href="principle_of_insufficient_reason" title="wikilink">principle of insufficient reason</a>".<a class="footnoteRef" href="#fn60" id="fnref60"><sup>60</sup></a><a class="footnoteRef" href="#fn61" id="fnref61"><sup>61</sup></a> Laplace assumed uniform priors for mathematical simplicity rather than for philosophical reasons.<a class="footnoteRef" href="#fn62" id="fnref62"><sup>62</sup></a> Laplace also introduced primitive versions of <a href="conjugate_prior" title="wikilink">conjugate priors</a> and the <a href="Bernstein–von_Mises_theorem" title="wikilink">theorem</a> of <a href="Richard_von_Mises" title="wikilink">von Mises</a> and <a href="S._N._Bernstein" title="wikilink">Bernstein</a>, according to which the posteriors corresponding to initially differing priors ultimately agree, as the number of observations increases.<a class="footnoteRef" href="#fn63" id="fnref63"><sup>63</sup></a> This early Bayesian inference, which used uniform priors following Laplace's <a href="principle_of_insufficient_reason" title="wikilink">principle of insufficient reason</a>, was called "<a href="inverse_probability" title="wikilink">inverse probability</a>" (because it <a href="Inductive_reasoning" title="wikilink">infers</a> backwards from observations to parameters, or from effects to causes<a class="footnoteRef" href="#fn64" id="fnref64"><sup>64</sup></a>).</p>

<p>After the 1920s, <a href="inverse_probability" title="wikilink">inverse probability</a> was largely supplanted by a collection of methods that were developed by <a href="Ronald_A._Fisher" title="wikilink">Ronald A. Fisher</a>, <a href="Jerzy_Neyman" title="wikilink">Jerzy Neyman</a> and <a href="Egon_Pearson" title="wikilink">Egon Pearson</a>. Their methods came to be called <a href="frequentist_statistics" title="wikilink">frequentist statistics</a>.<a class="footnoteRef" href="#fn65" id="fnref65"><sup>65</sup></a> Fisher rejected the Bayesian view, writing that "the theory of inverse probability is founded upon an error, and must be wholly rejected".<a class="footnoteRef" href="#fn66" id="fnref66"><sup>66</sup></a> At the end of his life, however, Fisher expressed greater respect for the essay of Bayes, which Fisher believed to have anticipated his own, <a href="fiducial_inference" title="wikilink">fiducial</a> approach to probability; Fisher still maintained that Laplace's views on probability were "fallacious rubbish".<a class="footnoteRef" href="#fn67" id="fnref67"><sup>67</sup></a> Neyman started out as a "quasi-Bayesian", but subsequently developed <a href="confidence_interval" title="wikilink">confidence intervals</a> (a key method in frequentist statistics) because "the whole theory would look nicer if it were built from the start without reference to Bayesianism and priors".<a class="footnoteRef" href="#fn68" id="fnref68"><sup>68</sup></a> The word <em>Bayesian</em> appeared around 1950, and by the 1960s it became the term preferred by those dissatisfied with the limitations of frequentist statistics.<a class="footnoteRef" href="#fn69" id="fnref69"><sup>69</sup></a><a class="footnoteRef" href="#fn70" id="fnref70"><sup>70</sup></a></p>

<p>In the 20th century, the ideas of Laplace were further developed in two different directions, giving rise to <em>objective</em> and <em>subjective</em> currents in Bayesian practice. In the objectivist stream, the statistical analysis depends on only the model assumed and the data analysed.<a class="footnoteRef" href="#fn71" id="fnref71"><sup>71</sup></a> No subjective decisions need to be involved. In contrast, "subjectivist" statisticians deny the possibility of fully objective analysis for the general case.</p>

<p>In the further development of Laplace's ideas, subjective ideas predate objectivist positions. The idea that 'probability' should be interpreted as 'subjective degree of belief in a proposition' was proposed, for example, by <a href="John_Maynard_Keynes" title="wikilink">John Maynard Keynes</a> in the early 1920s. This idea was taken further by <a href="Bruno_de_Finetti" title="wikilink">Bruno de Finetti</a> in Italy (<em>Fondamenti Logici del Ragionamento Probabilistico</em>, 1930) and <a href="Frank_P._Ramsey" title="wikilink">Frank Ramsey</a> in Cambridge (<em>The Foundations of Mathematics</em>, 1931).<a class="footnoteRef" href="#fn72" id="fnref72"><sup>72</sup></a> The approach was devised to solve problems with the <a href="frequentist" title="wikilink"> frequentist definition of probability</a> but also with the earlier, objectivist approach of Laplace.<a class="footnoteRef" href="#fn73" id="fnref73"><sup>73</sup></a> The subjective Bayesian methods were further developed and popularized in the 1950s by <a href="Leonard_Jimmie_Savage" title="wikilink">L.J. Savage</a>.</p>

<p>Objective Bayesian inference was further developed by <a href="Harold_Jeffreys" title="wikilink">Harold Jeffreys</a> at the <a href="University_of_Cambridge" title="wikilink">University of Cambridge</a>. His seminal book "Theory of probability" first appeared in 1939 and played an important role in the revival of the <a href="Bayesian_probability" title="wikilink">Bayesian view of probability</a>.<a class="footnoteRef" href="#fn74" id="fnref74"><sup>74</sup></a><a class="footnoteRef" href="#fn75" id="fnref75"><sup>75</sup></a> In 1957, <a href="Edwin_Thompson_Jaynes" title="wikilink">Edwin Jaynes</a> promoted the concept of <a href="Principle_of_maximum_entropy" title="wikilink">maximum entropy</a> for constructing priors, which is an important principle in the formulation of objective methods, mainly for discrete problems. In 1965, <a href="Dennis_Lindley" title="wikilink">Dennis Lindley</a>'s 2-volume work "Introduction to Probability and Statistics from a Bayesian Viewpoint" brought Bayesian methods to a wide audience. In 1979, <a href="José-Miguel_Bernardo" title="wikilink">José-Miguel Bernardo</a> introduced <a href="Prior_probability#Uninformative_priors" title="wikilink">reference analysis</a>,<a class="footnoteRef" href="#fn76" id="fnref76"><sup>76</sup></a> which offers a general applicable framework for objective analysis.<a class="footnoteRef" href="#fn77" id="fnref77"><sup>77</sup></a> Other well-known proponents of Bayesian probability theory include <a href="I.J._Good" title="wikilink">I.J. Good</a>, <a href="B.O._Koopman" title="wikilink">B.O. Koopman</a>, <a href="Howard_Raiffa" title="wikilink">Howard Raiffa</a>, <a href="Robert_Schlaifer" title="wikilink">Robert Schlaifer</a> and <a href="Alan_Turing" title="wikilink">Alan Turing</a>.</p>

<p>In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of <a href="Markov_chain_Monte_Carlo" title="wikilink">Markov chain Monte Carlo</a> methods, which removed many of the <a href="computational_problem" title="wikilink">computational problems</a>, and an increasing interest in nonstandard, complex applications.<a class="footnoteRef" href="#fn78" id="fnref78"><sup>78</sup></a> Despite growth of Bayesian research, most undergraduate teaching is still based on frequentist statistics.<a class="footnoteRef" href="#fn79" id="fnref79"><sup>79</sup></a> Nonetheless, Bayesian methods are widely accepted and used, such as for example in the field of <a href="machine_learning" title="wikilink">machine learning</a>.<a class="footnoteRef" href="#fn80" id="fnref80"><sup>80</sup></a></p>
<h2 id="important-contributors-to-statistics">Important contributors to statistics</h2>
<ul>
<li><a href="Thomas_Bayes" title="wikilink">Thomas Bayes</a></li>
<li><a href="George_E._P._Box" title="wikilink">George E. P. Box</a></li>
<li><a href="Pafnuty_Chebyshev" title="wikilink">Pafnuty Chebyshev</a></li>
<li><a href="David_R._Cox" title="wikilink">David R. Cox</a></li>
<li><a href="Gertrude_Mary_Cox" title="wikilink">Gertrude Cox</a></li>
<li><a href="Harald_Cramér" title="wikilink">Harald Cramér</a></li>
<li><a href="Francis_Ysidro_Edgeworth" title="wikilink">Francis Ysidro Edgeworth</a></li>
<li><a href="Bradley_Efron" title="wikilink">Bradley Efron</a></li>
<li><a href="Bruno_de_Finetti" title="wikilink">Bruno de Finetti</a></li>
<li><a href="Ronald_A._Fisher" title="wikilink">Ronald A. Fisher</a></li>
<li><a href="Francis_Galton" title="wikilink">Francis Galton</a></li>
</ul>
<ul>
<li><a href="Carl_Friedrich_Gauss" title="wikilink">Carl Friedrich Gauss</a></li>
<li><a href="William_Sealey_Gosset" title="wikilink">William Sealey Gosset</a> ("Student")</li>
<li><a href="Andrey_Kolmogorov" title="wikilink">Andrey Kolmogorov</a></li>
<li><a href="Pierre-Simon_Laplace" title="wikilink">Pierre-Simon Laplace</a></li>
<li><a href="Erich_Leo_Lehmann" title="wikilink">Erich L. Lehmann</a></li>
<li><a href="Aleksandr_Lyapunov" title="wikilink">Aleksandr Lyapunov</a></li>
<li><a href="Prasanta_Chandra_Mahalanobis" title="wikilink">Prasanta Chandra Mahalanobis</a></li>
<li><a href="Abraham_De_Moivre" title="wikilink">Abraham De Moivre</a></li>
<li><a href="Jerzy_Neyman" title="wikilink">Jerzy Neyman</a></li>
<li><a href="Florence_Nightingale" title="wikilink">Florence Nightingale</a></li>
<li><a href="Blaise_Pascal" title="wikilink">Blaise Pascal</a></li>
</ul>
<ul>
<li><a href="Karl_Pearson" title="wikilink">Karl Pearson</a></li>
<li><a href="Charles_Sanders_Peirce" title="wikilink">Charles S. Peirce</a></li>
<li><a href="Adolphe_Quetelet" title="wikilink">Adolphe Quetelet</a></li>
<li><a href="C._R._Rao" title="wikilink">C. R. Rao</a></li>
<li><a href="Walter_A._Shewhart" title="wikilink">Walter A. Shewhart</a></li>
<li><a href="Charles_Spearman" title="wikilink">Charles Spearman</a></li>
<li><a href="Charles_Stein_(statistician)" title="wikilink">Charles Stein</a></li>
<li><a href="Thorvald_N._Thiele" title="wikilink">Thorvald N. Thiele</a></li>
<li><a href="John_Tukey" title="wikilink">John Tukey</a></li>
<li><a href="Abraham_Wald" title="wikilink">Abraham Wald</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li>

<p>(<a href="http://www.stat.berkeley.edu/~census/521.pdf">Revised version, 2002</a>)</p></li>
<li></li>
<li></li>
<li>Kotz, S., Johnson, N.L. (1992,1992,1997). <em>Breakthroughs in Statistics</em>, Vols I,II,III. Springer ISBN 0-387-94037-5, ISBN 0-387-94039-1, ISBN 0-387-94989-5</li>
<li></li>
<li><a href="David_Salsburg" title="wikilink">Salsburg, David</a> (2001). <em><a href="The_Lady_Tasting_Tea" title="wikilink">The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century</a></em>. ISBN 0-7167-4106-7</li>
<li></li>
<li>Stigler, Stephen M. (1999) <em>Statistics on the Table: The History of Statistical Concepts and Methods</em>. Harvard University Press. ISBN 0-674-83601-4</li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.jehps.net/publications.htm">JEHPS: Recent publications in the history of probability and statistics</a></li>
<li><a href="http://www.jehps.net/indexang.html">Electronic Journ@l for History of Probability and Statistics/Journ@l Electronique d'Histoire des Probabilités et de la Statistique</a></li>
<li><a href="http://www.economics.soton.ac.uk/staff/aldrich/Figures.htm">Figures from the History of Probability and Statistics (Univ. of Southampton)</a></li>
<li><a href="http://www.york.ac.uk/depts/maths/histstat">Materials for the History of Statistics (Univ. of York)</a></li>
<li><a href="http://www.economics.soton.ac.uk/staff/aldrich/Probability%20Earliest%20Uses.htm">Probability and Statistics on the Earliest Uses Pages (Univ. of Southampton)</a></li>
<li><a href="http://jeff560.tripod.com/stat.html">Earliest Uses of Symbols in Probability and Statistics</a> on <a href="http://jeff560.tripod.com/mathsym.html">Earliest Uses of Various Mathematical Symbols</a></li>
</ul>

<p>"</p>

<p><a href="Category:History_of_statistics" title="wikilink"> </a> <a href="Category:History_of_mathematics" title="wikilink">Statistics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4">Ibrahim A. Al-Kadi "The origins of cryptology: The Arab contributions", <em><a class="uri" href="Cryptologia" title="wikilink">Cryptologia</a></em>, 16(2) (April 1992) pp. 97–126.<a href="#fnref4">↩</a></li>
<li id="fn5">Villani, Giovanni. Encyclopædia Britannica. <a href="Encyclopædia_Britannica_2006_Ultimate_Reference_Suite_DVD" title="wikilink">Encyclopædia Britannica 2006 Ultimate Reference Suite DVD</a>. Retrieved on 2008-03-04.<a href="#fnref5">↩</a></li>
<li id="fn6"></li>
<li id="fn7"></li>
<li id="fn8">de Moivre, A. (1738) The doctrine of chances. Woodfall<a href="#fnref8">↩</a></li>
<li id="fn9">Wilson, Edwin Bidwell (1923) "First and second laws of error", <em><a href="Journal_of_the_American_Statistical_Association" title="wikilink">Journal of the American Statistical Association</a></em>, 18 (143), 841-851 <a href="#fnref9">↩</a></li>
<li id="fn10">Havil J (2003) <em>Gamma: Exploring Euler's Constant</em>. Princeton, NJ: Princeton University Press, p. 157<a href="#fnref10">↩</a></li>
<li id="fn11">Peirce CS (1873) Theory of errors of observations. Report of the Superintendent US Coast Survey, Washington, Government Printing Office. Appendix no. 21: 200-224<a href="#fnref11">↩</a></li>
<li id="fn12">Cochran W.G. (1978) "Laplace’s ratio estimators". pp 3-10. In David H.A., (ed). <em>Contributions to Survey Sampling and Applied Statistics: papers in honor of H. O. Hartley</em>. Academic Press, New York ISBN122047508,<a href="#fnref12">↩</a></li>
<li id="fn13">Keynes, JM (1921) A treatise on probability. Pt II Ch XVII §5 (p 201)<a href="#fnref13">↩</a></li>
<li id="fn14">Galton F (1881) Report of the Anthropometric Committee pp 245-260. Report of the 51st Meeting of the British Association for the Advancement of Science<a href="#fnref14">↩</a></li>
<li id="fn15">Stigler (1986, Chapter 5: Quetelet's Two Attempts)<a href="#fnref15">↩</a></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
<li id="fn17">(Stigler 1986, Chapter 9: The Next Generation: Edgeworth)<a href="#fnref17">↩</a></li>
<li id="fn18">Bellhouse DR (1988) A brief history of random sampling methods. Handbook of statistics. Vol 6 pp 1-14 Elsevier<a href="#fnref18">↩</a></li>
<li id="fn19">Bowley AL (1906) Address to the Economic Science and Statistics Section of the British Association for the Advancement of Science. J Roy Stat Soc 69: 548-557<a href="#fnref19">↩</a></li>
<li id="fn20">Galton F (1877) Typical laws of heredity. <em>Nature</em> 15: 492-553<a href="#fnref20">↩</a></li>
<li id="fn21">Galton F (1907) One Vote, One Value. <em>Nature</em> 75: 414<a href="#fnref21">↩</a></li>
<li id="fn22">Stigler (1986, Chapter 10: Pearson and Yule)<a href="#fnref22">↩</a></li>
<li id="fn23"><a href="#fnref23">↩</a></li>
<li id="fn24"><a href="#fnref24">↩</a></li>
<li id="fn25"><a href="#fnref25">↩</a></li>
<li id="fn26"></li>
<li id="fn27"></li>
<li id="fn28"><a href="#fnref28">↩</a></li>
<li id="fn29">Jolliffe, I. T. (2002). <em>Principal Component Analysis, 2nd ed</em>. New York: Springer-Verlag.<a href="#fnref29">↩</a></li>
<li id="fn30">Box, <em>R. A. Fisher</em>, pp 93–166<a href="#fnref30">↩</a></li>
<li id="fn31"><a href="#fnref31">↩</a></li>
<li id="fn32">Fisher RA (1925) Statistical methods for research workers, Edinburgh: Oliver &amp; Boyd<a href="#fnref32">↩</a></li>
<li id="fn33">Neyman, J (1934) On the two different aspects of the representative method: The method of stratified sampling and the method of purposive selection. <em><a href="Journal_of_the_Royal_Statistical_Society" title="wikilink">Journal of the Royal Statistical Society</a></em> 97 (4) 557-625 <a href="#fnref33">↩</a></li>
<li id="fn34"><a href="#fnref34">↩</a></li>
<li id="fn35"><a href="#fnref35">↩</a></li>
<li id="fn36"></li>
<li id="fn37"><a href="#fnref37">↩</a></li>
<li id="fn38"><a href="#fnref38">↩</a></li>
<li id="fn39"><a href="#fnref39">↩</a></li>
<li id="fn40"><a href="#fnref40">↩</a></li>
<li id="fn41"></li>
<li id="fn42"></li>
<li id="fn43"></li>
<li id="fn44"></li>
<li id="fn45">, actually published 1879, NOAA <a href="http://docs.lib.noaa.gov/rescue/cgs/001_pdf/CSC-0025.PDF#page=222">PDF Eprint</a>.<br/>
 Reprinted in <em><a href="Charles_Sanders_Peirce_bibliography#CP" title="wikilink">Collected Papers</a></em> <strong>7</strong>, paragraphs 139–157, also in <em><a href="Charles_Sanders_Peirce_bibliography#W" title="wikilink">Writings</a></em> <strong>4</strong>, pp. 72–78, and in <a href="#fnref45">↩</a></li>
<li id="fn46"><a href="#fnref46">↩</a></li>
<li id="fn47">Johnson, N.L. (1961). "Sequential analysis: a survey." <em><a href="Journal_of_the_Royal_Statistical_Society" title="wikilink">Journal of the Royal Statistical Society</a></em>, Series A. Vol. 124 (3), 372–411. (pages 375–376)<a href="#fnref47">↩</a></li>
<li id="fn48">Wald, A. (1945) "Sequential Tests of Statistical Hypotheses", <a href="Annals_of_Mathematical_Statistics" title="wikilink">Annals of Mathematical Statistics</a>, 16 (2), 117–186.<a href="#fnref48">↩</a></li>
<li id="fn49"><a href="Herman_Chernoff" title="wikilink">Chernoff, H.</a> (1972) <em>Sequential Analysis and Optimal Design</em>, <a href="Society_for_Industrial_and_Applied_Mathematics" title="wikilink">SIAM</a> Monograph. ISBN 978-0898710069<a href="#fnref49">↩</a></li>
<li id="fn50">Zacks, S. (1996) "Adaptive Designs for Parametric Models". In: Ghosh, S. and Rao, C. R., (Eds) (1996). "Design and Analysis of Experiments," <em>Handbook of Statistics</em>, Volume 13. North-Holland. ISBN 0-444-82061-2. (pages 151–180)<a href="#fnref50">↩</a></li>
<li id="fn51"><a href="#fnref51">↩</a></li>
<li id="fn52">Hald, Anders (1998) <em>A History of Mathematical Statistics</em>. New York: Wiley. <a href="#fnref52">↩</a></li>
<li id="fn53">Box, Joan Fisher (1978) <em>R. A. Fisher: The Life of a Scientist</em>, Wiley. ISBN 0-471-09300-9 (pp 93–166)<a href="#fnref53">↩</a></li>
<li id="fn54"><a href="#fnref54">↩</a></li>
<li id="fn55"><a href="#fnref55">↩</a></li>
<li id="fn56"><a href="#fnref56">↩</a></li>
<li id="fn57"><a href="#fnref57">↩</a></li>
<li id="fn58"><a href="#fnref58">↩</a></li>
<li id="fn59">Stigler (1986, Chapter 3: Inverse Probability)<a href="#fnref59">↩</a></li>
<li id="fn60"></li>
<li id="fn61">Hald (1998)<a href="#fnref61">↩</a></li>
<li id="fn62"></li>
<li id="fn63">Lucien Le Cam (1986) <em>Asymptotic Methods in Statistical Decision Theory</em>: Pages 336 and 618–621 (von Mises and Bernstein).<a href="#fnref63">↩</a></li>
<li id="fn64">Stephen. E. Fienberg, (2006) <a href="http://ba.stat.cmu.edu/journal/2006/vol01/issue01/fienberg.pdf">When did Bayesian Inference become "Bayesian"?</a> <em>Bayesian Analysis</em>, 1 (1), 1–40. See page 5.<a href="#fnref64">↩</a></li>
<li id="fn65">Stephen. E. Fienberg, <a href="http://ba.stat.cmu.edu/journal/2006/vol01/issue01/fienberg.pdf">When did Bayesian Inference become "Bayesian"?</a> <em>Bayesian Analysis</em> (2006).<a href="#fnref65">↩</a></li>
<li id="fn66">Aldrich, A. (2008) <a href="http://ba.stat.cmu.edu/journal/2008/vol03/issue01/aldrich.pdf">"R. A. Fisher on Bayes and Bayes' Theorem"</a>, <em>Bayesian analysis</em>, 3 (1),161–170<a href="#fnref66">↩</a></li>
<li id="fn67"></li>
<li id="fn68"><a href="#fnref68">↩</a></li>
<li id="fn69"></li>
<li id="fn70">Jeff Miller, <a href="http://jeff560.tripod.com/b.html">"Earliest Known Uses of Some of the Words of Mathematics (B)"</a> "The term Bayesian entered circulation around 1950. R. A. Fisher used it in the notes he wrote to accompany the papers in his Contributions to Mathematical Statistics (1950). Fisher thought Bayes's argument was all but extinct for the only recent work to take it seriously was <a href="Harold_Jeffreys" title="wikilink">Harold Jeffreys</a>'s Theory of Probability (1939). In 1951 L. J. Savage, reviewing Wald's Statistical Decisions Functions, referred to "modern, or unBayesian, statistical theory" ("The Theory of Statistical Decision," Journal of the American Statistical Association, 46, p. 58.). Soon after, however, Savage changed from being an unBayesian to being a Bayesian."<a href="#fnref70">↩</a></li>
<li id="fn71"><a href="#fnref71">↩</a></li>
<li id="fn72">Gillies, D. (2000), <em>Philosophical Theories of Probability</em>. Routledge. ISBN 0-415-18276-X pp 50–1<a href="#fnref72">↩</a></li>
<li id="fn73"></li>
<li id="fn74">E. T. Jaynes. <em>Probability Theory: The Logic of Science</em> Cambridge University Press, (2003). ISBN 0-521-59271-2<a href="#fnref74">↩</a></li>
<li id="fn75"><a href="#fnref75">↩</a></li>
<li id="fn76"></li>
<li id="fn77">Bernardo, J. M. and Smith, A. F. M. (1994). "Bayesian Theory". Chichester: Wiley.<a href="#fnref77">↩</a></li>
<li id="fn78">Wolpert, RL. (2004) <a href="http://projecteuclid.org/euclid.ss/1089808283">"A conversation with James O. Berger"</a>, <em>Statistical Science</em>, 9, 205–218  <a href="#fnref78">↩</a></li>
<li id="fn79"><a href="#fnref79">↩</a></li>
<li id="fn80">Bishop, C.M. (2007) <em>Pattern Recognition and Machine Learning</em>. Springer ISBN 978-0387310732<a href="#fnref80">↩</a></li>
</ol>
</section>
</body>
</html>
