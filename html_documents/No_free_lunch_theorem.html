<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="261">No free lunch theorem</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>No free lunch theorem</h1>
<hr/>

<p>In <a href="mathematical_folklore" title="wikilink">mathematical folklore</a>, the <strong>"no free lunch" theorem</strong> (sometimes pluralized) of <a href="David_Wolpert" title="wikilink">David Wolpert</a> and <a href="William_G._Macready" title="wikilink">William Macready</a> appears in the 1997 "No Free Lunch Theorems for Optimization".<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Wolpert had previously derived no free lunch theorems for machine learning (statistical inference).<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> In 2005, Wolpert and Macready themselves indicated that the first theorem in their paper "state[s] that any two <a href="Optimization_(mathematics)" title="wikilink">optimization</a> algorithms are equivalent when their performance is averaged across all possible problems".<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> The 1997 theorems of Wolpert and Macready are mathematically technical<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a>and some find them unintuitive. The folkloric "no free lunch" (NFL) theorem is an easily stated and easily understood consequence of theorems Wolpert and Macready actually prove. It is weaker than the proven theorems, and thus does not encapsulate them.</p>

<p>Various investigators have extended the work of Wolpert and Macready substantively. See <a href="No_free_lunch_in_search_and_optimization" title="wikilink">No free lunch in search and optimization</a> for treatment of the research area.</p>
<h2 id="original-nfl-theorems">Original NFL theorems</h2>

<p>Wolpert and Macready give two NFL theorems that are closely related to the folkloric theorem. In their paper, they state:</p>

<p><mtpl></mtpl></p>

<p>The theorem first hypothesizes <a href="objective_function" title="wikilink">objective functions</a> that do not change while optimization is in progress, and the second hypothesizes objective functions that may change.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<dl>
<dd><em>Theorem 1</em>: For any pair of algorithms <em>a</em><sub>1</sub> and <em>a</em><sub>2</sub>, at iteration step <em>m</em>
</dd>
</dl>
<dl>
<dd><dl>
<dd>

<math display="inline" id="No_free_lunch_theorem:0">
 <semantics>
  <mrow>
   <msub>
    <mo largeop="true" symmetric="true">∑</mo>
    <mi>f</mi>
   </msub>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msubsup>
     <mi>d</mi>
     <mi>m</mi>
     <mi>y</mi>
    </msubsup>
    <mo stretchy="false">|</mo>
    <mi>f</mi>
    <mo>,</mo>
    <mi>m</mi>
    <mo>,</mo>
    <msub>
     <mi>a</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">∑</mo>
    <mi>f</mi>
   </msub>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msubsup>
     <mi>d</mi>
     <mi>m</mi>
     <mi>y</mi>
    </msubsup>
    <mo stretchy="false">|</mo>
    <mi>f</mi>
    <mo>,</mo>
    <mi>m</mi>
    <mo>,</mo>
    <msub>
     <mi>a</mi>
     <mn>2</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <ci>f</ci>
    </apply>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>d</ci>
       <ci>m</ci>
      </apply>
      <ci>y</ci>
     </apply>
     <ci>normal-|</ci>
     <csymbol cd="unknown">f</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">m</csymbol>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>a</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <ci>f</ci>
    </apply>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>d</ci>
       <ci>m</ci>
      </apply>
      <ci>y</ci>
     </apply>
     <ci>normal-|</ci>
     <csymbol cd="unknown">f</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">m</csymbol>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>a</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-,</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{f}P(d_{m}^{y}|f,m,a_{1})=\sum_{f}P(d_{m}^{y}|f,m,a_{2}),
  </annotation>
 </semantics>
</math>


</dd>
</dl>
</dd>
</dl>

<p>where 

<math display="inline" id="No_free_lunch_theorem:1">
 <semantics>
  <msubsup>
   <mi>d</mi>
   <mi>m</mi>
   <mi>y</mi>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>d</ci>
     <ci>m</ci>
    </apply>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d_{m}^{y}
  </annotation>
 </semantics>
</math>

 denotes the ordered set of size 

<math display="inline" id="No_free_lunch_theorem:2">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 of the cost values 

<math display="inline" id="No_free_lunch_theorem:3">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 associated to input values 

<math display="inline" id="No_free_lunch_theorem:4">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in X
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="No_free_lunch_theorem:5">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>:</mo>
   <mrow>
    <mi>X</mi>
    <mo>→</mo>
    <mi>Y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f:X\rightarrow Y
  </annotation>
 </semantics>
</math>

 is the function being optimized and 

<math display="inline" id="No_free_lunch_theorem:6">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msubsup>
     <mi>d</mi>
     <mi>m</mi>
     <mi>y</mi>
    </msubsup>
    <mo stretchy="false">|</mo>
    <mi>f</mi>
    <mo>,</mo>
    <mi>m</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>d</ci>
       <ci>m</ci>
      </apply>
      <ci>y</ci>
     </apply>
     <ci>normal-|</ci>
     <csymbol cd="unknown">f</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">m</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(d_{m}^{y}|f,m,a)
  </annotation>
 </semantics>
</math>

 is the conditional probability of obtaining a given sequence of cost values from algorithm 

<math display="inline" id="No_free_lunch_theorem:7">
 <semantics>
  <mi>a</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>a</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a
  </annotation>
 </semantics>
</math>

 run 

<math display="inline" id="No_free_lunch_theorem:8">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 times on function 

<math display="inline" id="No_free_lunch_theorem:9">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

.</p>

<p>The theorem can be equivalently formulated as follows:</p>
<dl>
<dd><em>Theorem 1</em>: Given a finite set 

<math display="inline" id="No_free_lunch_theorem:10">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

 and a finite set 

<math display="inline" id="No_free_lunch_theorem:11">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 of real numbers, assume that 

<math display="inline" id="No_free_lunch_theorem:12">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>:</mo>
   <mrow>
    <mi>V</mi>
    <mo>→</mo>
    <mi>S</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>V</ci>
     <ci>S</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f:V\to S
  </annotation>
 </semantics>
</math>

 is chosen at random according to uniform distribution on the set 

<math display="inline" id="No_free_lunch_theorem:13">
 <semantics>
  <msup>
   <mi>V</mi>
   <mi>S</mi>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>V</ci>
    <ci>S</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V^{S}
  </annotation>
 </semantics>
</math>

 of all possible functions from 

<math display="inline" id="No_free_lunch_theorem:14">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

 to 

<math display="inline" id="No_free_lunch_theorem:15">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

. For the problem of optimizing 

<math display="inline" id="No_free_lunch_theorem:16">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 over the set 

<math display="inline" id="No_free_lunch_theorem:17">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

, then no algorithm performs better than blind search.
</dd>
</dl>

<p>Here, <em>blind search</em> means that at each step of the algorithm, the element 

<math display="inline" id="No_free_lunch_theorem:18">
 <semantics>
  <mrow>
   <mi>v</mi>
   <mo>∈</mo>
   <mi>V</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>v</ci>
    <ci>V</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   v\in V
  </annotation>
 </semantics>
</math>

 is chosen at random with uniform probability distribution from the elements of 

<math display="inline" id="No_free_lunch_theorem:19">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

 that have not been chosen previously.</p>

<p>In essence, this says that when all functions <em>f</em> are equally likely, the probability of observing an arbitrary sequence of <em>m</em> values in the course of optimization does not depend upon the algorithm. In the analytic framework of Wolpert and Macready, performance is a function of the sequence of observed values (and not e.g. of wall-clock time), so it follows easily that all algorithms have identically distributed performance when objective functions are drawn uniformly at random, and also that all algorithms have identical mean performance. But identical mean performance of all algorithms does not imply Theorem 1, and thus the folkloric theorem is not equivalent to the original theorem.</p>

<p>Theorem 2 establishes a similar, but "more subtle", NFL result for time-varying objective functions.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="intelligent-design-and-the-nfl-theorem">Intelligent design and the NFL theorem</h2>

<p>The folkloric NFL theorem is often invoked by <a href="intelligent_design" title="wikilink">intelligent design</a> proponent <a href="William_Dembski" title="wikilink">William Dembski</a> as supporting intelligent design and Dembski's concept of <a href="specified_complexity" title="wikilink">specified complexity</a> which he claims is evidence of design.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> Many in the scientific community have rejected both the notions of specified complexity and that the no free lunch theorem supports intelligent design.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<h2 id="notes">Notes</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.no-free-lunch.org/">No Free Lunch Theorems</a></li>
<li><a href="http://philosophy.wisc.edu/forster/papers/Krakow.pdf">No Free Lunches for Anyone, Bayesians Included (1999)</a> - a simple example illustrating the idea behind these theorems</li>
<li><a href="https://commons.wikimedia.org/wiki/File:No_free_lunch_theorem.svg">1</a> - graphics illustrating the theorem</li>
</ul>

<p>"</p>

<p><a href="Category:Scientific_folklore" title="wikilink">Category:Scientific folklore</a> <a href="Category:Philosophy_of_mathematics" title="wikilink">Category:Philosophy of mathematics</a> <a href="Category:Mathematical_theorems" title="wikilink">Category:Mathematical theorems</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Wolpert, D.H., Macready, W.G. (1997), "<a href="http://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf">No Free Lunch Theorems for Optimization</a>", <em>IEEE Transactions on Evolutionary Computation</em> <strong>1</strong>, 67.<a href="#fnref1">↩</a></li>
<li id="fn2">Wolpert, David (1996), "<a href="http://web.archive.org/web/20140111060917/http://engr.case.edu/ray_soumya/eecs440_fall13/lack_of_a_priori_distinctions_wolpert.pdf">The Lack of <em>A Priori</em> Distinctions between Learning Algorithms</a>", <em>Neural Computation</em>, pp. 1341-1390.<a href="#fnref2">↩</a></li>
<li id="fn3">Wolpert, D.H., and Macready, W.G. (2005) "Coevolutionary free lunches", <em>IEEE Transactions on Evolutionary Computation</em>, 9(6): 721-735<a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"></li>
<li id="fn6"></li>
<li id="fn7">Dembski, W. A. (2002) <em>No Free Lunch</em>, Rowman &amp; Littlefield<a href="#fnref7">↩</a></li>
<li id="fn8">Wolpert, D. (2003) "<a href="http://www.talkreason.org/articles/jello.cfm">William Dembski's treatment of the No Free Lunch theorems is written in jello</a>".<a href="#fnref8">↩</a></li>
<li id="fn9">Perakh, M. (2003) "<a href="http://www.talkreason.org/articles/orr.cfm">The No Free Lunch Theorems and Their Application to Evolutionary Algorithms</a>".<a href="#fnref9">↩</a></li>
</ol>
</section>
</body>
</html>
