<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="631">Empirical risk minimization</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Empirical risk minimization</h1>
<hr/>

<p><strong>Empirical risk minimization</strong> (ERM) is a principle in <a href="statistical_learning_theory" title="wikilink">statistical learning theory</a> which defines a family of learning algorithms and is used to give theoretical bounds on the performance of <a href="machine_learning" title="wikilink">learning algorithms</a>.</p>
<h2 id="background">Background</h2>

<p>Consider the following situation, which is a general setting of many <a href="supervised_learning" title="wikilink">supervised learning</a> problems. We have two spaces of objects 

<math display="inline" id="Empirical_risk_minimization:0">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Empirical_risk_minimization:1">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

 and would like to learn a function 

<math display="inline" id="Empirical_risk_minimization:2">
 <semantics>
  <mrow>
   <mpadded lspace="-1.7pt" width="-1.7pt">
    <mi>h</mi>
   </mpadded>
   <mo>:</mo>
   <mrow>
    <mi>X</mi>
    <mo>→</mo>
    <mi>Y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>h</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \!h:X\to Y
  </annotation>
 </semantics>
</math>

 (often called <em>hypothesis</em>) which outputs an object 

<math display="inline" id="Empirical_risk_minimization:3">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>∈</mo>
   <mi>Y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>y</ci>
    <ci>Y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y\in Y
  </annotation>
 </semantics>
</math>


, given 

<math display="inline" id="Empirical_risk_minimization:4">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in X
  </annotation>
 </semantics>
</math>

. To do so, we have at our disposal a <em>training set</em> of a few examples 

<math display="inline" id="Empirical_risk_minimization:5">
 <semantics>
  <mrow>
   <mrow>
    <mo lspace="0.8pt" stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mi>m</mi>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mi>m</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <cn type="integer">1</cn>
     </apply>
    </interval>
    <ci>normal-…</ci>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>m</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>m</ci>
     </apply>
    </interval>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \!(x_{1},y_{1}),\ldots,(x_{m},y_{m})
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Empirical_risk_minimization:6">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mi>i</mi>
   </msub>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>i</ci>
    </apply>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}\in X
  </annotation>
 </semantics>
</math>

 is an input and 

<math display="inline" id="Empirical_risk_minimization:7">
 <semantics>
  <mrow>
   <msub>
    <mi>y</mi>
    <mi>i</mi>
   </msub>
   <mo>∈</mo>
   <mi>Y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>i</ci>
    </apply>
    <ci>Y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}\in Y
  </annotation>
 </semantics>
</math>

 is the corresponding response that we wish to get from 

<math display="inline" id="Empirical_risk_minimization:8">
 <semantics>
  <mrow>
   <mpadded lspace="-1.7pt" width="-1.7pt">
    <mi>h</mi>
   </mpadded>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mi>i</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>h</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>i</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \!h(x_{i})
  </annotation>
 </semantics>
</math>


.</p>

<p>To put it more formally, we assume that there is a <a href="joint_probability_distribution" title="wikilink">joint probability distribution</a> 

<math display="inline" id="Empirical_risk_minimization:9">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(x,y)
  </annotation>
 </semantics>
</math>

 over 

<math display="inline" id="Empirical_risk_minimization:10">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Empirical_risk_minimization:11">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

, and that the training set consists of 

<math display="inline" id="Empirical_risk_minimization:12">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 instances 

<math display="inline" id="Empirical_risk_minimization:13">
 <semantics>
  <mrow>
   <mrow>
    <mo lspace="0.8pt" stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mi>m</mi>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mi>m</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <cn type="integer">1</cn>
     </apply>
    </interval>
    <ci>normal-…</ci>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>m</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>m</ci>
     </apply>
    </interval>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \!(x_{1},y_{1}),\ldots,(x_{m},y_{m})
  </annotation>
 </semantics>
</math>


 drawn <a class="uri" href="i.i.d." title="wikilink">i.i.d.</a> from 

<math display="inline" id="Empirical_risk_minimization:14">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(x,y)
  </annotation>
 </semantics>
</math>

. Note that the assumption of a joint probability distribution allows us to model uncertainty in predictions (e.g. from noise in data) because 

<math display="inline" id="Empirical_risk_minimization:15">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 is not a deterministic function of 

<math display="inline" id="Empirical_risk_minimization:16">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

, but rather a <a href="random_variable" title="wikilink">random variable</a> with <a href="conditional_distribution" title="wikilink">conditional distribution</a> 

<math display="inline" id="Empirical_risk_minimization:17">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo stretchy="false">|</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(y|x)
  </annotation>
 </semantics>
</math>

 for a fixed 

<math display="inline" id="Empirical_risk_minimization:18">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>


.</p>

<p>We also assume that we are given a non-negative real-valued <a href="loss_function" title="wikilink">loss function</a> 

<math display="inline" id="Empirical_risk_minimization:19">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>y</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <interval closure="open">
     <apply>
      <ci>normal-^</ci>
      <ci>y</ci>
     </apply>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L(\hat{y},y)
  </annotation>
 </semantics>
</math>

 which measures how different the prediction 

<math display="inline" id="Empirical_risk_minimization:20">
 <semantics>
  <mover accent="true">
   <mi>y</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{y}
  </annotation>
 </semantics>
</math>

 of a hypothesis is from the true outcome 

<math display="inline" id="Empirical_risk_minimization:21">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

. The <a href="Risk_(statistics)" title="wikilink">risk</a> associated with hypothesis 

<math display="inline" id="Empirical_risk_minimization:22">
 <semantics>
  <mrow>
   <mi>h</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>h</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h(x)
  </annotation>
 </semantics>
</math>

 is then defined as the <a href="Expected_value" title="wikilink">expectation</a> of the loss function:</p>

<p>

<math display="block" id="Empirical_risk_minimization:23">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>R</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>h</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mi>𝐄</mi>
     <mrow>
      <mo stretchy="false">[</mo>
      <mrow>
       <mi>L</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>h</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo stretchy="false">]</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mo largeop="true" symmetric="true">∫</mo>
     <mrow>
      <mi>L</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>h</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>,</mo>
       <mi>y</mi>
       <mo rspace="4.2pt" stretchy="false">)</mo>
      </mrow>
      <mi>d</mi>
      <mi>P</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo>,</mo>
       <mi>y</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>R</ci>
      <ci>h</ci>
     </apply>
     <apply>
      <times></times>
      <ci>𝐄</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <times></times>
        <ci>L</ci>
        <interval closure="open">
         <apply>
          <times></times>
          <ci>h</ci>
          <ci>x</ci>
         </apply>
         <ci>y</ci>
        </interval>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <int></int>
      <apply>
       <times></times>
       <ci>L</ci>
       <interval closure="open">
        <apply>
         <times></times>
         <ci>h</ci>
         <ci>x</ci>
        </apply>
        <ci>y</ci>
       </interval>
       <ci>d</ci>
       <ci>P</ci>
       <interval closure="open">
        <ci>x</ci>
        <ci>y</ci>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(h)=\mathbf{E}[L(h(x),y)]=\int L(h(x),y)\,dP(x,y).
  </annotation>
 </semantics>
</math>

</p>

<p>A loss function commonly used in theory is the <a href="0-1_loss_function" title="wikilink">0-1 loss function</a>

<math display="block" id="Empirical_risk_minimization:24">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>y</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>y</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mo>≠</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">L</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-^</ci>
      <ci>y</ci>
     </apply>
     <ci>normal-,</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">I</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-^</ci>
      <ci>y</ci>
     </apply>
     <neq></neq>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L(\hat{y},y)=I(\hat{y}\neq y)
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Empirical_risk_minimization:25">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi mathvariant="normal">…</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <ci>normal-…</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I(...)
  </annotation>
 </semantics>
</math>

 is the <a href="indicator_notation" title="wikilink">indicator notation</a>.</p>

<p>The ultimate goal of a learning algorithm is to find a hypothesis 

<math display="inline" id="Empirical_risk_minimization:26">
 <semantics>
  <msup>
   <mi>h</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>h</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h^{*}
  </annotation>
 </semantics>
</math>

 among a fixed class of functions 

<math display="inline" id="Empirical_risk_minimization:27">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℋ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℋ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

 for which the risk 

<math display="inline" id="Empirical_risk_minimization:28">
 <semantics>
  <mrow>
   <mi>R</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>h</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>R</ci>
    <ci>h</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(h)
  </annotation>
 </semantics>
</math>


 is minimal:</p>

<p>

<math display="block" id="Empirical_risk_minimization:29">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>h</mi>
     <mo>*</mo>
    </msup>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mi>arg</mi>
      <mrow>
       <munder>
        <mi>min</mi>
        <mrow>
         <mi>h</mi>
         <mo>∈</mo>
         <mi class="ltx_font_mathcaligraphic">ℋ</mi>
        </mrow>
       </munder>
       <mi>R</mi>
      </mrow>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>h</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>h</ci>
     <times></times>
    </apply>
    <apply>
     <times></times>
     <apply>
      <arg></arg>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <min></min>
        <apply>
         <in></in>
         <ci>h</ci>
         <ci>ℋ</ci>
        </apply>
       </apply>
       <ci>R</ci>
      </apply>
     </apply>
     <ci>h</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h^{*}=\arg\min_{h\in\mathcal{H}}R(h).
  </annotation>
 </semantics>
</math>

</p>
<h2 id="empirical-risk-minimization">Empirical risk minimization</h2>

<p>In general, the risk 

<math display="inline" id="Empirical_risk_minimization:30">
 <semantics>
  <mrow>
   <mi>R</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>h</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>R</ci>
    <ci>h</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(h)
  </annotation>
 </semantics>
</math>

 cannot be computed because the distribution 

<math display="inline" id="Empirical_risk_minimization:31">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(x,y)
  </annotation>
 </semantics>
</math>

 is unknown to the learning algorithm (this situation is referred to as <a href="agnostic_learning" title="wikilink">agnostic learning</a>). However, we can compute an approximation, called <em>empirical risk</em>, by averaging the loss function on the training set:</p>

<p>

<math display="block" id="Empirical_risk_minimization:32">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mpadded lspace="-1.7pt" width="-1.7pt">
       <mi>R</mi>
      </mpadded>
      <mtext>emp</mtext>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>h</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mfrac>
      <mn>1</mn>
      <mi>m</mi>
     </mfrac>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>m</mi>
      </munderover>
      <mrow>
       <mi>L</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>h</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>,</mo>
        <msub>
         <mi>y</mi>
         <mi>i</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>R</ci>
      <mtext>emp</mtext>
     </apply>
     <ci>h</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>m</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>m</ci>
      </apply>
      <apply>
       <times></times>
       <ci>L</ci>
       <interval closure="open">
        <apply>
         <times></times>
         <ci>h</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \!R_{\mbox{emp}}(h)=\frac{1}{m}\sum_{i=1}^{m}L(h(x_{i}),y_{i}).
  </annotation>
 </semantics>
</math>

</p>

<p><em>Empirical risk minimization</em> principle states that the learning algorithm should choose a hypothesis 

<math display="inline" id="Empirical_risk_minimization:33">
 <semantics>
  <mover accent="true">
   <mi>h</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>h</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{h}
  </annotation>
 </semantics>
</math>


 which minimizes the empirical risk:</p>

<p>

<math display="block" id="Empirical_risk_minimization:34">
 <semantics>
  <mrow>
   <mrow>
    <mover accent="true">
     <mi>h</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mi>arg</mi>
      <mrow>
       <munder>
        <mi>min</mi>
        <mrow>
         <mi>h</mi>
         <mo>∈</mo>
         <mi class="ltx_font_mathcaligraphic">ℋ</mi>
        </mrow>
       </munder>
       <msub>
        <mi>R</mi>
        <mtext>emp</mtext>
       </msub>
      </mrow>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>h</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>normal-^</ci>
     <ci>h</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <arg></arg>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <min></min>
        <apply>
         <in></in>
         <ci>h</ci>
         <ci>ℋ</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>R</ci>
        <mtext>emp</mtext>
       </apply>
      </apply>
     </apply>
     <ci>h</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{h}=\arg\min_{h\in\mathcal{H}}R_{\mbox{emp}}(h).
  </annotation>
 </semantics>
</math>

 Thus the learning algorithm defined by the ERM principle consists in solving the above <a href="Mathematical_optimization" title="wikilink">optimization</a> problem.</p>
<h2 id="properties">Properties</h2>
<h3 id="computational-complexity">Computational complexity</h3>

<p>Empirical risk minimization for a classification problem with <a href="0-1_loss_function" title="wikilink">0-1 loss function</a> is known to be an <a class="uri" href="NP-hard" title="wikilink">NP-hard</a> problem even for such relatively simple class of functions as <a href="linear_classifier" title="wikilink">linear classifiers</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Though, it can be solved efficiently when minimal empirical risk is zero, i.e. data is <a href="linearly_separable" title="wikilink">linearly separable</a>.</p>

<p>In practice, machine learning algorithms cope with that either by employing a convex approximation to 0-1 loss function (like <a href="hinge_loss" title="wikilink">hinge loss</a> for <a href="Support_vector_machine" title="wikilink">SVM</a>), which is easier to optimize, or by posing assumptions on the distribution 

<math display="inline" id="Empirical_risk_minimization:35">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(x,y)
  </annotation>
 </semantics>
</math>

 (and thus stop being agnostic learning algorithms to which the above result applies,)</p>
<h2 id="references">References</h2>
<h2 id="literature">Literature</h2>
<ul>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">V. Feldman, V. Guruswami, P. Raghavendra and Yi Wu (2009). <a href="http://www.almaden.ibm.com/cs/people/vitaly/papers/FGRW09_MonoHalf_FOCS.pdf"><em>Agnostic Learning of Monomials by Halfspaces is Hard.</em></a> (See the paper and references therein)<a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
