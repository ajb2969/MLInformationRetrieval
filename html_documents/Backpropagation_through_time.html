<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="32">Backpropagation through time</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Backpropagation through time</h1>
<hr/>

<p><strong>Backpropagation through time</strong> (BPTT) is a <a class="uri" href="gradient" title="wikilink">gradient</a>-based technique for training certain types of <a href="recurrent_neural_network" title="wikilink">recurrent neural networks</a>. It can be used to train <a href="Recurrent_neural_network#Elman_networks_and_Jordan_networks" title="wikilink">Elman networks</a>. The algorithm was independently derived by numerous researchers<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="algorithm">Algorithm</h2>
<figure><b>(Figure)</b>
<figcaption>thump|500px|BPTT unfolds a recurrent neural network through time.</figcaption>
</figure>

<p>To train a recurrent neural network using BPTT, some training data is needed. This data should be an ordered sequence of input-output pairs, 

<math display="inline" id="Backpropagation_through_time:0">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">⟨</mo>
    <msub>
     <mi>𝐚</mi>
     <mn>0</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>𝐲</mi>
     <mn>0</mn>
    </msub>
    <mo stretchy="false">⟩</mo>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">⟨</mo>
    <msub>
     <mi>𝐚</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>𝐲</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">⟩</mo>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">⟨</mo>
    <msub>
     <mi>𝐚</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>𝐲</mi>
     <mn>2</mn>
    </msub>
    <mo stretchy="false">⟩</mo>
   </mrow>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">⟨</mo>
    <msub>
     <mi>𝐚</mi>
     <mrow>
      <mi>n</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>𝐲</mi>
     <mrow>
      <mi>n</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo stretchy="false">⟩</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐚</ci>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐲</ci>
      <cn type="integer">0</cn>
     </apply>
    </list>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐚</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐲</ci>
      <cn type="integer">1</cn>
     </apply>
    </list>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐚</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐲</ci>
      <cn type="integer">2</cn>
     </apply>
    </list>
    <ci>normal-…</ci>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐚</ci>
      <apply>
       <minus></minus>
       <ci>n</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐲</ci>
      <apply>
       <minus></minus>
       <ci>n</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
    </list>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \langle\mathbf{a}_{0},\mathbf{y}_{0}\rangle,\langle\mathbf{a}_{1},\mathbf{y}_{%
1}\rangle,\langle\mathbf{a}_{2},\mathbf{y}_{2}\rangle,...,\langle\mathbf{a}_{n%
-1},\mathbf{y}_{n-1}\rangle
  </annotation>
 </semantics>
</math>

. Also, an initial value must be specified for 

<math display="inline" id="Backpropagation_through_time:1">
 <semantics>
  <msub>
   <mi>𝐱</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>𝐱</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}_{0}
  </annotation>
 </semantics>
</math>

. Typically, the vector with zero-magnitude is used for this purpose.</p>

<p>BPTT begins by unfolding a recurrent neural network through time as shown in this figure. This recurrent neural network contains two feed-forward neural networks, <em>f</em> and <em>g</em>. When the network is unfolded through time, the unfolded network contains <em>k</em> instances of <em>f</em> and one instance of <em>g</em>. In the example shown, the network has been unfolded to a depth of <em>k</em>=3.</p>

<p>Training then proceeds in a manner similar to training a feed-forward neural network with <a class="uri" href="backpropagation" title="wikilink">backpropagation</a>, except that each epoch must run through the observations, 

<math display="inline" id="Backpropagation_through_time:2">
 <semantics>
  <msub>
   <mi>𝐲</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>𝐲</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{y}_{t}
  </annotation>
 </semantics>
</math>

, in sequential order. Each training pattern consists of 

<math display="inline" id="Backpropagation_through_time:3">
 <semantics>
  <mrow>
   <mo stretchy="false">⟨</mo>
   <msub>
    <mi>𝐱</mi>
    <mi>t</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>𝐚</mi>
    <mi>t</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>𝐚</mi>
    <mrow>
     <mi>t</mi>
     <mo>+</mo>
     <mn>1</mn>
    </mrow>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>𝐚</mi>
    <mrow>
     <mi>t</mi>
     <mo>+</mo>
     <mn>2</mn>
    </mrow>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>𝐚</mi>
    <mrow>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mi>k</mi>
     </mrow>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>𝐲</mi>
    <mrow>
     <mi>t</mi>
     <mo>+</mo>
     <mi>k</mi>
    </mrow>
   </msub>
   <mo stretchy="false">⟩</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐱</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐚</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐚</ci>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐚</ci>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐚</ci>
     <apply>
      <minus></minus>
      <apply>
       <plus></plus>
       <ci>t</ci>
       <ci>k</ci>
      </apply>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐲</ci>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <ci>k</ci>
     </apply>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \langle\mathbf{x}_{t},\mathbf{a}_{t},\mathbf{a}_{t+1},\mathbf{a}_{t+2},...,%
\mathbf{a}_{t+k-1},\mathbf{y}_{t+k}\rangle
  </annotation>
 </semantics>
</math>


. (All of the actions for <em>k</em> time-steps are needed because the unfolded network contains inputs at each unfolded level.) Typically, <a class="uri" href="backpropagation" title="wikilink">backpropagation</a> is applied in an online manner to update the weights as each training pattern is presented.</p>

<p>After each pattern is presented, and the weights have been updated, the weights in each instance of f (

<math display="inline" id="Backpropagation_through_time:4">
 <semantics>
  <mrow>
   <msub>
    <mi>f</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>f</mi>
    <mn>2</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>f</mi>
    <mi>k</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <cn type="integer">2</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>k</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{1},f_{2},...,f_{k}
  </annotation>
 </semantics>
</math>

) are averaged together so that they all have the same weights. Also, 

<math display="inline" id="Backpropagation_through_time:5">
 <semantics>
  <msub>
   <mi>𝐱</mi>
   <mrow>
    <mi>t</mi>
    <mo>+</mo>
    <mn>1</mn>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>𝐱</ci>
    <apply>
     <plus></plus>
     <ci>t</ci>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}_{t+1}
  </annotation>
 </semantics>
</math>

 is calculated as 

<math display="inline" id="Backpropagation_through_time:6">
 <semantics>
  <mrow>
   <msub>
    <mi>𝐱</mi>
    <mrow>
     <mi>t</mi>
     <mo>+</mo>
     <mn>1</mn>
    </mrow>
   </msub>
   <mo>=</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>𝐱</mi>
      <mi>t</mi>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>𝐚</mi>
      <mi>t</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐱</ci>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>f</ci>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐱</ci>
       <ci>t</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐚</ci>
       <ci>t</ci>
      </apply>
     </interval>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}_{t+1}=f(\mathbf{x}_{t},\mathbf{a}_{t})
  </annotation>
 </semantics>
</math>

, which provides the information necessary so that the algorithm can move on to the next time-step, <em>t+1</em>.</p>
<h2 id="pseudo-code">Pseudo-code</h2>

<p>Pseudo-code for BPTT:</p>
<pre><code>Back_Propagation_Through_Time(a, y)   // a[t] is the input at time t. y[t] is the output
    Unfold the network to contain k instances of f
    do until stopping criteria is met:
        x = the zero-magnitude vector;// x is the current context
        for t from 0 to n - 1         // t is time. n is the length of the training sequence
            Set the network inputs to x, a[t], a[t+1], ..., a[t+k-1]
            p = forward-propagate the inputs over the whole unfolded network
            e = y[t+k] - p;           // error = target - prediction
            Back-propagate the error, e, back across the whole unfolded network
            Update all the weights in the network
            Average the weights in each instance of f together, so that each f is identical
            x = f(x);                 // compute the context for the next time-step</code></pre>
<h2 id="advantages">Advantages</h2>

<p>BPTT tends to be significantly faster for training recurrent neural networks than general-purpose optimization techniques such as <a href="evolutionary_programming" title="wikilink">evolutionary</a> optimization.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h2 id="disadvantages">Disadvantages</h2>

<p>BPTT has difficulty with local optima. With recurrent neural networks, local optima is a much more significant problem than it is with feed-forward neural networks.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> The recurrent feedback in such networks tends to create chaotic responses in the error surface which cause local optima to occur frequently, and in very poor locations on the error surface.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Backpropagation_through_structure" title="wikilink">Backpropagation through structure</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
</ol>
</section>
</body>
</html>
