<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1708">Contrast set learning</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Contrast set learning</h1>
<hr/>

<p><strong>Contrast set learning</strong> is a form of <a href="association_rule_learning" title="wikilink">association rule learning</a> that seeks to identify meaningful differences between separate groups by reverse-engineering the key predictors that identify for each particular group. For example, given a set of attributes for a pool of students (labeled by degree type), a contrast set learner would identify the <em>contrasting</em> features between students seeking bachelor's degrees and those working toward PhD degrees.</p>
<h2 id="overview">Overview</h2>

<p>A common practice in <a href="data_mining" title="wikilink">data mining</a> is to <a href="Statistical_classification" title="wikilink">classify</a>, to look at the attributes of an object or situation and make a guess at what category the observed item belongs to. As new evidence is examined (typically by feeding a <em>training set</em> to a learning <a class="uri" href="algorithm" title="wikilink">algorithm</a>), these guesses are reﬁned and improved. Contrast set learning works in the opposite direction. While classiﬁers read a collection of data and collect information that is used to place new data into a series of discrete categories, contrast set learning takes the category that an item belongs to and attempts to reverse engineer the statistical evidence that identifies an item as a member of a class. That is, contrast set learners seek rules associating attribute values with changes to the class distribution.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> They seek to identify the key predictors that contrast one classification from another.</p>

<p>For example, an aerospace engineer might record data on test launches of a new rocket. Measurements would be taken at regular intervals throughout the launch, noting factors such as the trajectory of the rocket, operating temperatures, external pressures, and so on. If the rocket launch fails after a number of successful tests, the engineer could use contrast set learning to distinguish between the successful and failed tests. A contrast set learner will produce a set of association rules that, when applied, will indicate the key predictors of each failed tests versus the successful ones (the temperature was too high, the wind pressure was too high, etc.).</p>

<p>Contrast set learning is a form of <a href="association_rule_learning" title="wikilink">association rule learning</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Association rule learners typically offer rules linking attributes commonly occurring together in a training set (for instance, people who are enrolled in four-year programs and take a full course load tend to also live near campus). Instead of ﬁnding rules that describe the current situation, contrast set learners seek rules that differ meaningfully in their distribution across groups (and thus, can be used as predictors for those groups).<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> For example, a contrast set learner could ask, “What are the key identifiers of a person with a bachelor's degree or a person with a PhD, and how do people with PhD's and bachelor’s degrees differ?”</p>

<p>Standard <a href="Classification_in_machine_learning" title="wikilink">classifier</a> algorithms, such as <a class="uri" href="C4.5" title="wikilink">C4.5</a>, have no concept of class importance (that is, they do not know if a class is "good" or "bad"). Such learners cannot bias or filter their predictions towards certain desired classes. As the goal of contrast set learning is to discover meaningful differences between groups, it is useful to be able to target the learned rules towards certain classifications. Several contrast set learners, such as MINWAL<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> or the family of TAR algorithms,<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> assign weights to each class in order to focus the learned theories toward outcomes that are of interest to a particular audience. Thus, contrast set learning can be though of as a form of weighted class learning.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h3 id="example-supermarket-purchases">Example: Supermarket Purchases</h3>

<p>The differences between standard classification, association rule learning, and contrast set learning can be illustrated with a simple supermarket metaphor. In the following small dataset, each row is a supermarket transaction and each "1" indicates that the item was purchased (a "0" indicates that the item was not purchased):</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">
<p><em>Hamburger</em></p></th>
<th style="text-align: left;">
<p><em>Potatoes</em></p></th>
<th style="text-align: left;">
<p><em>Foie Gras</em></p></th>
<th style="text-align: left;">
<p><em>Onions</em></p></th>
<th style="text-align: left;">
<p><em>Champagne</em></p></th>
<th style="text-align: left;">
<p><em>Purpose of Purchases</em></p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>Cookout</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>Cookout</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>Anniversary</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>Cookout</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>Frat Party</p></td>
</tr>
</tbody>
</table>

<p>Given this data,</p>
<ul>
<li>Association rule learning may discover that customers that buy onions and potatoes together are likely to also purchase hamburger meat.</li>
<li>Classification may discover that customers that bought onions, potatoes, and hamburger meats were purchasing items for a cookout.</li>
<li>Contrast set learning may discover that the major difference between customers shopping for a cookout and those shopping for an anniversary dinner are that customers acquiring items for a cookout purchase onions, potatoes, and hamburger meat (and <em>do not purchase</em> foie gras or champagne).</li>
</ul>
<h2 id="treatment-learning">Treatment Learning</h2>

<p>Treatment learning is a form of weighted contrast-set learning that takes a single <em>desirable</em> group and contrasts it against the remaining <em>undesirable</em> groups (the level of desirability is represented by weighted classes).<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> The resulting "treatment" suggests a set of rules that, when applied, will lead to the desired outcome.</p>

<p>Treatment learning differs from standard contrast set learning through the following constraints:</p>
<ul>
<li>Rather than seeking the differences between all groups, treatment learning specifies a particular group to focus on, applies a weight to this desired grouping, and lumps the remaining groups into one "undesired" category.</li>
<li>Treatment learning has a stated focus on minimal theories. In practice, treatment are limited to a maximum of four contraints (i.e., rather than stating all of the reasons that a rocket differs from a skateboard, a treatment learner will state one to four major differences that predict for rockets at a high level of statistical significance).</li>
</ul>

<p>This focus on simplicity is an important goal for treatment learners. Treatment learning seeks the <em>smallest</em> change that has the <em>greatest</em> impact on the class distribution.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>

<p>Conceptually, treatment learners explore all possible subsets of the range of values for all attributes. Such a search is often infeasible in practice, so treatment learning often focuses instead on quickly pruning and ignoring attribute ranges that, when applied, lead to a class distribution where the desired class is in the minority.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<h3 id="example-boston-housing-data">Example: Boston Housing Data</h3>

<p>The following example demonstrates the output of the treatment learner TAR3 on a dataset of housing data from the city of <a class="uri" href="Boston" title="wikilink">Boston</a> (a nontrivial public dataset with over 500 examples). In this dataset, a number of factors are collected for each house, and each house is classified according to its quality (low, medium-low, medium-high, and high). The <em>desired</em> class is set to "high," and all other classes are lumped together as undesirable.</p>

<p>The output of the treatment learner is as follows:</p>

<p><code>
 Baseline class distribution:
 low: 29%
 medlow: 29%
 medhigh: 21%
 high: 21%

 Suggested Treatment: [PTRATIO=[12.6..16), RM=[6.7..9.78)]

 New class distribution:
 low: 0%
 medlow: 0%
 medhigh: 3%
 high: 97%
</code></p>

<p>With no applied treatments (rules), the desired class represents only 21% of the class distribution. However, if we filter the data set for houses with 6.7 to 9.78 rooms and a neighborhood parent-teacher ratio of 12.6 to 16, then 97% of the remaining examples fall into the desired class (high quality houses).</p>
<h2 id="algorithms">Algorithms</h2>

<p>There are a number of algorithms that perform contrast set learning. The following subsections describe two examples.</p>
<h3 id="stucco">STUCCO</h3>

<p>The STUCCO contrast set learner<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> treats the task of learning from contrast sets as a <a href="Tree_traversal" title="wikilink">tree search</a> problem where the root node of the tree is an empty contrast set. Children are added by specializing the set with additional items picked through a canonical ordering of attributes (to avoid visiting the same nodes twice). Children are formed by appending terms that follow all existing terms in a given ordering. The formed tree is searched in a breadth-first manner. Given the nodes at each level, the dataset is scanned and the support is counted for each group. Each node is then examined to determine if it is significant and large, if it should be pruned, and if new children should be generated. After all significant contrast sets are located, a post-processor selects a subset to show to the user - the low order, simpler results are shown first, followed by the higher order results which are "surprising and significantly different.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a>"</p>

<p>The support calculation comes from testing a null hypothesis that the contrast set support is equal across all groups (i.e., that contrast set support is <em>independent of group membership</em>). The support count for each group is a frequency value that can be analyzed in a contingency table where each row represents the truth value of the contrast set and each column variable indicates the group membership frequency. If there is a difference in proportions between the contrast set frequencies and those of the null hypothesis, the algorithm must then determine if the differences in proportions represent a relation between variables or if it can be attributed to random causes. This can be determined through a <a href="Chi-squared_test" title="wikilink">chi-square test</a> comparing the observed frequency count to the expected count.</p>

<p>Nodes are pruned from the tree when all specializations of the node can never lead to a significant and large contrast set. The decision to prune is based on:</p>
<ul>
<li>The minimum deviation size: The maximum difference between the support of any two groups bust be greater than a user-specified threshold.</li>
<li>Expected cell frequencies: The expected cell frequencies of a contingency table can only decrease as the contrast set is specialized. When these frequencies are too small, the validity of the chi-square test is violated.</li>
<li>

<math display="inline" id="Contrast_set_learning:0">
 <semantics>
  <msup>
   <mi>χ</mi>
   <mn>2</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>χ</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \chi^{2}
  </annotation>
 </semantics>
</math>

 bounds: An upper bound is kept on the distribution of a statistic calculated when the null hypothesis is true. Nodes are pruned when it is no longer possible to meet this cutoff.</li>
</ul>
<h3 id="tar3">TAR3</h3>

<p>The TAR3<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a><a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> weighted contrast set learner is based on two fundamental concepts - the <strong>lift</strong> and <strong>support</strong> of a rule set.</p>

<p>The lift of a set of rules is the change that some decision makes to a set of examples after imposing that decision (i.e., how the class distribution shifts in response to the imposition of a rule). TAR3 seeks the smallest set of rules which induces the biggest changes in the sum of the weights attached to each class multiplied by the frequency at which each class occurs. The lift is calculated by dividing the score of the set in which the set of rules is imposed by the score of the baseline set (i.e., no rules are applied). Note that by reversing the lift scoring function, the TAR3 learner can also select for the remaining classes and reject the target class.</p>

<p>It is problematic to rely on the lift of a rule set alone. Incorrect or misleading data noise, if correlated with failing examples, may result in an overfitted rule set. Such an overfitted model may have a large lift score, but it does not accurately reﬂect the prevailing conditions within the dataset. To avoid overfitting, TAR3 utilizes a support threshold and rejects all rules that fall on the wrong side of this threshold. Given a target class, the support threshold is a user-supplied value (usually 0.2) which is compared to the ratio of the frequency of the target class when the rule set has been applied to the frequency of that class in the overall dataset. TAR3 rejects all sets of rules with support lower than this threshold.</p>

<p>By requiring both a high lift and a high support value, TAR3 not only returns ideal rule sets, but also favors smaller sets of rules. The fewer rules adopted, the more evidence that will exist supporting those rules.</p>

<p>The TAR3 algorithm only builds sets of rules from attribute value ranges with a high heuristic value. The algorithm determines which ranges to use by ﬁrst determining the lift score of each attribute’s value ranges. These individual scores are then sorted and converted into a cumulative probability distribution. TAR3 randomly selects values from this distribution, meaning that low-scoring ranges are unlikely to be selected. To build a candidate rule set, several ranges are selected and combined. These candidate rule sets are then scored and sorted. If no improvement is seen after a user-defined number of rounds, the algorithm terminates and returns the top-scoring rule sets.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Data_management" title="wikilink">Category:Data management</a> <a href="Category:Data_mining" title="wikilink">Category:Data mining</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"></li>
<li id="fn11"></li>
<li id="fn12"></li>
<li id="fn13"></li>
<li id="fn14"></li>
<li id="fn15"></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
</ol>
</section>
</body>
</html>
