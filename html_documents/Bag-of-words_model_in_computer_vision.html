<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1181">Bag-of-words model in computer vision</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Bag-of-words model in computer vision</h1>
<hr/>

<p>In <strong>computer vision</strong>, the <strong>bag-of-words model</strong> (BoW model) can be applied to <a href="image_classification" title="wikilink">image classification</a>, by treating image features as words. In document classification, a <a href="bag_of_words" title="wikilink">bag of words</a> is a sparse vector of occurrence counts of words; that is, a sparse <a class="uri" href="histogram" title="wikilink">histogram</a> over the vocabulary. In <a href="computer_vision" title="wikilink">computer vision</a>, a <em>bag of visual words</em> is a vector of occurrence counts of a vocabulary of local image features.</p>
<h2 id="representation-based-on-the-bow-model">Representation based on the BoW model</h2>
<h3 id="image-representation-based-on-the-bow-model">Image representation based on the BoW model</h3>

<p>To represent an image using BoW model, an image can be treated as a document. Similarly, "words" in images need to be defined too. To achieve this, it usually includes following three steps: <a href="Feature_detection_(computer_vision)" title="wikilink">feature detection</a>, feature description, and codebook generation.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> A definition of the BoW model can be the "histogram representation based on independent features".<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Content based image indexing and retrieval (CBIR) appears to be the early adopter of this image representation technique.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h4 id="feature-representation">Feature representation</h4>

<p>After feature detection, each image is abstracted by several local patches. Feature representation methods deal with how to represent the patches as numerical vectors. These vectors are called feature descriptors. A good descriptor should have the ability to handle intensity, rotation, scale and affine variations to some extent. One of the most famous descriptors is <a href="Scale-invariant_feature_transform" title="wikilink">Scale-invariant feature transform</a> (SIFT).<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> SIFT converts each patch to 128-dimensional vector. After this step, each image is a collection of vectors of the same dimension (128 for SIFT), where the order of different vectors is of no importance.</p>
<h4 id="codebook-generation">Codebook generation</h4>

<p>The final step for the BoW model is to convert vector represented patches to "codewords" (analogy to words in text documents), which also produces a "codebook" (analogy to a word dictionary). A codeword can be considered as a representative of several similar patches. One simple method is performing <a href="k-means_clustering" title="wikilink">k-means clustering</a> over all the vectors.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Codewords are then defined as the centers of the learned clusters. The number of the clusters is the codebook size (analogy to the size of the word dictionary).</p>

<p>Thus, each patch in an image is mapped to a certain codeword through the clustering process and the image can be represented by the <a class="uri" href="histogram" title="wikilink">histogram</a> of the codewords.</p>
<h2 id="learning-and-recognition-based-on-the-bow-model">Learning and recognition based on the BoW model</h2>

<p>Computer vision researchers have developed several learning methods to leverage the BoW model for image related task, such as <a href="object_categorization" title="wikilink">object categorization</a>. These methods can roughly be divided into two categories, generative and discriminative models. For multiple label categorization problem, the <a href="confusion_matrix" title="wikilink">confusion matrix</a> can be used as an evaluation metric.</p>
<h3 id="generative-models">Generative models</h3>

<p>Here are some notations for this section. Suppose the size of codebook is 

<math display="inline" id="Bag-of-words_model_in_computer_vision:0">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

.</p>
<ul>
<li>

<math display="inline" id="Bag-of-words_model_in_computer_vision:1">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

: each patch 

<math display="inline" id="Bag-of-words_model_in_computer_vision:2">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

 is a V-dimensional vector that has a single component that equals to one and all other components equal to zero (For k-means clustering setting, the single component equal one indicates the cluster that 

<math display="inline" id="Bag-of-words_model_in_computer_vision:3">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

 belongs to). The 

<math display="inline" id="Bag-of-words_model_in_computer_vision:4">
 <semantics>
  <mi>v</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>v</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   v
  </annotation>
 </semantics>
</math>

th codeword in the codebook can be represented as 

<math display="inline" id="Bag-of-words_model_in_computer_vision:5">
 <semantics>
  <mrow>
   <msup>
    <mi>w</mi>
    <mi>v</mi>
   </msup>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>w</ci>
     <ci>v</ci>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{v}=1
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Bag-of-words_model_in_computer_vision:6">
 <semantics>
  <mrow>
   <msup>
    <mi>w</mi>
    <mi>u</mi>
   </msup>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>w</ci>
     <ci>u</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{u}=0
  </annotation>
 </semantics>
</math>

 for 

<math display="inline" id="Bag-of-words_model_in_computer_vision:7">
 <semantics>
  <mrow>
   <mi>u</mi>
   <mo>‚â†</mo>
   <mi>v</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <neq></neq>
    <ci>u</ci>
    <ci>v</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   u\neq v
  </annotation>
 </semantics>
</math>

.</li>
</ul>
<ul>
<li>

<math display="inline" id="Bag-of-words_model_in_computer_vision:8">
 <semantics>
  <mi>ùê∞</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùê∞</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}
  </annotation>
 </semantics>
</math>

: each image is represented by 

<math display="inline" id="Bag-of-words_model_in_computer_vision:9">
 <semantics>
  <mrow>
   <mi>ùê∞</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">[</mo>
    <msub>
     <mi>w</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">‚ãØ</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mi>N</mi>
    </msub>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>ùê∞</ci>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-‚ãØ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>N</ci>
     </apply>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}=[w_{1},w_{2},\cdots,w_{N}]
  </annotation>
 </semantics>
</math>

, all the patches in an image</li>
<li>

<math display="inline" id="Bag-of-words_model_in_computer_vision:10">
 <semantics>
  <msub>
   <mi>d</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>d</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d_{j}
  </annotation>
 </semantics>
</math>

: the 

<math display="inline" id="Bag-of-words_model_in_computer_vision:11">
 <semantics>
  <mi>j</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>j</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   j
  </annotation>
 </semantics>
</math>

th image in an image collection</li>
<li>

<math display="inline" id="Bag-of-words_model_in_computer_vision:12">
 <semantics>
  <mi>c</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>c</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c
  </annotation>
 </semantics>
</math>

: category of the image</li>
<li>

<math display="inline" id="Bag-of-words_model_in_computer_vision:13">
 <semantics>
  <mi>z</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>z</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   z
  </annotation>
 </semantics>
</math>

: theme or topic of the patch</li>
<li>

<math display="inline" id="Bag-of-words_model_in_computer_vision:14">
 <semantics>
  <mi>œÄ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>œÄ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi
  </annotation>
 </semantics>
</math>

: mixture proportion</li>
</ul>

<p>Since the BoW model is an analogy to the BoW model in NLP, generative models developed in text domains can also be adapted in computer vision. Simple Na√Øve Bayes model and hierarchical Bayesian models are discussed.</p>
<h4 id="na√Øve-bayes">Na√Øve Bayes</h4>

<p>The simplest one is <a href="Na√Øve_Bayes" title="wikilink">Na√Øve Bayes</a> classifier.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> Using the language of <a href="graphical_models" title="wikilink">graphical models</a>, the Na√Øve Bayes classifier is described by the equation below. The basic idea (or assumption) of this model is that each category has its own distribution over the codebooks, and that the distributions of each category are observably different. Take a face category and a car category for an example. The face category may emphasize the codewords which represent "nose", "eye" and "mouth", while the car category may emphasize the codewords which represent "wheel" and "window". Given a collection of training examples, the classifier learns different distributions for different categories. The categorization decision is made by</p>
<ul>
<li>

<math display="inline" id="Bag-of-words_model_in_computer_vision:15">
 <semantics>
  <mrow>
   <msup>
    <mi>c</mi>
    <mo>*</mo>
   </msup>
   <mo>=</mo>
   <mi>arg</mi>
   <msub>
    <mi>max</mi>
    <mi>c</mi>
   </msub>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>c</mi>
    <mo stretchy="false">|</mo>
    <mi>ùê∞</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>arg</mi>
   <msub>
    <mi>max</mi>
    <mi>c</mi>
   </msub>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>c</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>ùê∞</mi>
    <mo stretchy="false">|</mo>
    <mi>c</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>arg</mi>
   <msub>
    <mi>max</mi>
    <mi>c</mi>
   </msub>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>c</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <msubsup>
    <mo largeop="true" symmetric="true">‚àè</mo>
    <mrow>
     <mi>n</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>N</mi>
   </msubsup>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <mi>c</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>c</ci>
     <times></times>
    </apply>
    <eq></eq>
    <arg></arg>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <max></max>
     <ci>c</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">w</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <arg></arg>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <max></max>
     <ci>c</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">w</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <arg></arg>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <max></max>
     <ci>c</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <csymbol cd="latexml">product</csymbol>
      <apply>
       <eq></eq>
       <ci>n</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>N</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>n</ci>
     </apply>
     <ci>normal-|</ci>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c^{*}=\arg\max_{c}p(c|\mathbf{w})=\arg\max_{c}p(c)p(\mathbf{w}|c)=\arg\max_{c}%
p(c)\prod_{n=1}^{N}p(w_{n}|c)
  </annotation>
 </semantics>
</math>

</li>
</ul>

<p>Since the Na√Øve Bayes classifier is simple yet effective, it is usually used as a baseline method for comparison.</p>
<h4 id="hierarchical-bayesian-models">Hierarchical Bayesian models</h4>

<p>The basic assumption of Na√Øve Bayes model does not hold sometimes. For example, a natural scene image may contain several different themes. <a href="Probabilistic_latent_semantic_analysis" title="wikilink">Probabilistic latent semantic analysis</a> (pLSA)<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> and <a href="latent_Dirichlet_allocation" title="wikilink">latent Dirichlet allocation</a> (LDA)<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> are two popular topic models from text domains to tackle the similar multiple "theme" problem. Take LDA for an example. To model natural scene images using LDA, an analogy is made like this (Figure 9):</p>
<ul>
<li>the image category is mapped to the document category;</li>
<li>the mixture proportion of themes maps the mixture proportion of topics;</li>
<li>the theme index is mapped to topic index;</li>
<li>the codeword is mapped to the word.</li>
</ul>

<p>This method shows very promising results in natural scene categorization on <a href="http://vision.stanford.edu/resources_links.html">13 Natural Scene Categories</a>.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h3 id="discriminative-models">Discriminative models</h3>

<p>Since images are represented based on the BoW model, any discriminative model suitable for text document categorization can be tried, such as <a href="support_vector_machine" title="wikilink">support vector machine</a> (SVM)<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> and <a class="uri" href="AdaBoost" title="wikilink">AdaBoost</a>.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> <a href="Kernel_trick" title="wikilink">Kernel trick</a> is also applicable when kernel based classifier is used, such as SVM. Pyramid match kernel is newly developed one based on the BoW model. The local feature approach of using BoW model representation learnt by machine learning classifiers with different kernels (e.g., EMD-kernel and 

<math display="inline" id="Bag-of-words_model_in_computer_vision:16">
 <semantics>
  <msup>
   <mi>X</mi>
   <mn>2</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>X</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X^{2}
  </annotation>
 </semantics>
</math>

 kernel) has been vastly tested in the area of texture and object recognition.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> Very promising results on a number of datasets have been reported. This approach<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> has achieved very impressive result in the <a href="http://www.pascal-network.org/challenges/VOC/">the PASCAL Visual Object Classes Challenge</a>.</p>
<h4 id="pyramid-match-kernel">Pyramid match kernel</h4>

<p>Pyramid match kernel<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> is a fast algorithm (linear complexity instead of classic one in quadratic complexity) kernel function (satisfying <a href="Mercer's_condition" title="wikilink">Mercer's condition</a>) which maps the BoW features, or set of features in high dimension, to multi-dimensional multi-resolution histograms. An advantage of these multi-resolution histograms is their ability to capture co-occurring features. The pyramid match kernel builds multi-resolution histograms by binning data points into discrete regions of increasing size. Thus, points that do not match at high resolutions have the chance to match at low resolutions. The pyramid match kernel performs an approximate similarity match, without explicit search or computation of distance. Instead, it intersects the histograms to approximate the optimal match. Accordingly, the computation time is only linear in the number of features. Compared with other kernel approaches, the pyramid match kernel is much faster, yet provides equivalent accuracy. The pyramid match kernel was applied to <a href="http://www.mis.informatik.tu-darmstadt.de/Research/Projects/categorization/eth80-db.html">ETH-80 database</a> and <a href="http://vision.cs.princeton.edu/resources_links.html">Caltech 101 database</a> with promising results.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a><a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></p>
<h2 id="limitations-and-recent-developments">Limitations and recent developments</h2>

<p>One of the notorious disadvantages of BoW is that it ignores the spatial relationships among the patches, which are very important in image representation. Researchers have proposed several methods to incorporate the spatial information. For feature level improvements, correlogram features can capture spatial co-occurrences of features.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> For generative models, relative positions<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a><a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> of codewords are also taken into account. The hierarchical shape and appearance model for human action<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> introduces a new part layer (<a href="Constellation_model" title="wikilink">Constellation model</a>) between the mixture proportion and the BoW features, which captures the spatial relationships among parts in the layer. For discriminative models, spatial pyramid match<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> performs pyramid matching by partitioning the image into increasingly fine sub-regions and compute histograms of local features inside each sub-region.</p>

<p>Furthermore, the BoW model has not been extensively tested yet for view point invariance and scale invariance, and the performance is unclear. Also the BoW model for object segmentation and localization is not well understood.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Part-based_models" title="wikilink">Part-based models</a></li>
<li><a href="Segmentation-based_object_categorization" title="wikilink">Segmentation-based object categorization</a></li>
<li><a href="Vector_space_model" title="wikilink">Vector space model</a></li>
<li><a href="Bag-of-words_model" title="wikilink">Bag-of-words model</a></li>
<li><a href="Feature_extraction" title="wikilink">Feature extraction</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://people.csail.mit.edu/fergus/iccv2005/bagwords.html">A demo for two bag-of-words classifiers</a> by L. Fei-Fei, R. Fergus, and A. Torralba.</li>
<li><a href="http://www.vision.caltech.edu/malaa/software/research/image-search/">Caltech Large Scale Image Search Toolbox</a>: a Matlab/C++ toolbox implementing Inverted File search for Bag of Words model. It also contains implementations for fast approximate nearest neighbor search using randomized <a href="k-d_tree" title="wikilink">k-d tree</a>, <a href="locality-sensitive_hashing" title="wikilink">locality-sensitive hashing</a>, and <a href="hierarchical_k-means" title="wikilink">hierarchical k-means</a>.</li>
</ul>

<p><a href="it:Modello_della_borsa_di_parole" title="wikilink">it:Modello della borsa di parole</a>"</p>

<p><a href="Category:Object_recognition_and_categorization" title="wikilink">Category:Object recognition and categorization</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
<li id="fn2"><a href="#fnref2">‚Ü©</a></li>
<li id="fn3"><a href="#fnref3">‚Ü©</a></li>
<li id="fn4"><a href="#fnref4">‚Ü©</a></li>
<li id="fn5"><a href="#fnref5">‚Ü©</a></li>
<li id="fn6"><a href="#fnref6">‚Ü©</a></li>
<li id="fn7"><a href="#fnref7">‚Ü©</a></li>
<li id="fn8"><a href="#fnref8">‚Ü©</a></li>
<li id="fn9"><a href="#fnref9">‚Ü©</a></li>
<li id="fn10"></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">‚Ü©</a></li>
<li id="fn13"><a href="#fnref13">‚Ü©</a></li>
<li id="fn14"></li>
<li id="fn15"><a href="#fnref15">‚Ü©</a></li>
<li id="fn16"></li>
<li id="fn17"><a href="#fnref17">‚Ü©</a></li>
<li id="fn18"><a href="#fnref18">‚Ü©</a></li>
<li id="fn19"><a href="#fnref19">‚Ü©</a></li>
<li id="fn20"><a href="#fnref20">‚Ü©</a></li>
<li id="fn21"><a href="#fnref21">‚Ü©</a></li>
<li id="fn22"><a href="#fnref22">‚Ü©</a></li>
<li id="fn23"></li>
</ol>
</section>
</body>
</html>
