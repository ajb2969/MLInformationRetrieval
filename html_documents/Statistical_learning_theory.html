<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1513">Statistical learning theory</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Statistical learning theory</h1>
<hr/>

<p><strong>Statistical learning theory</strong> is a framework for <a href="machine_learning" title="wikilink">machine learning</a> drawing from the fields of <a class="uri" href="statistics" title="wikilink">statistics</a> and <a href="functional_analysis" title="wikilink">functional analysis</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as <a href="computer_vision" title="wikilink">computer vision</a>, <a href="speech_recognition" title="wikilink">speech recognition</a>, <a class="uri" href="bioinformatics" title="wikilink">bioinformatics</a> and <a class="uri" href="baseball" title="wikilink">baseball</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="introduction">Introduction</h2>

<p>The goal of learning is prediction. Learning falls into many categories, including <a href="supervised_learning" title="wikilink">supervised learning</a>, <a href="unsupervised_learning" title="wikilink">unsupervised learning</a>, <a href="Online_machine_learning" title="wikilink">online learning</a>, and <a href="reinforcement_learning" title="wikilink">reinforcement learning</a>. From the perspective of statistical learning theory, supervised learning is best understood.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Supervised learning involves learning from a <a href="training_set" title="wikilink">training set</a> of data. Every point in the training is an input-output pair, where the input maps to an output. The learning problem consists of inferring the function that maps between the input and the output in a predictive fashion, such that the learned function can be used to predict output from future input.</p>

<p>Depending of the type of output, supervised learning problems are either problems of <a href="regression_analysis" title="wikilink">regression</a> or problems of <a href="Statistical_classification" title="wikilink">classification</a>. If the output takes a continuous range of values, it is a regression problem. Using <a href="Ohm's_Law" title="wikilink">Ohm's Law</a> as an example, a regression could be performed with voltage as input and current as output. The regression would find the functional relationship between voltage and current to be , such that</p>

<p>

<math display="block" id="Statistical_learning_theory:0">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mi>R</mi>
    </mfrac>
    <mi>V</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>I</ci>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>R</ci>
     </apply>
     <ci>V</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I=\frac{1}{R}V
  </annotation>
 </semantics>
</math>

 Classification problems are those for which the output will be an element from a discrete set of labels. Classification is very common for machine learning applications. In <a href="facial_recognition_system" title="wikilink">facial recognition</a>, for instance, a picture of a person's face would be the input, and the output label would be that person's name. The input would be represented by a large multidimensional vector, in which each dimension represents the value of one of the pixels.</p>

<p>After learning a function based on the training set data, that function is validated on a test set of data, data that did not appear in the training set.</p>
<h2 id="formal-description">Formal Description</h2>

<p>Take 

<math display="inline" id="Statistical_learning_theory:1">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 to be the vector space of all possible inputs, and 

<math display="inline" id="Statistical_learning_theory:2">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

 to be the vector space of all possible outputs. Statistical learning theory takes the perspective that there is some unknown probability distribution over the product space 

<math display="inline" id="Statistical_learning_theory:3">
 <semantics>
  <mrow>
   <mi>Z</mi>
   <mo>=</mo>
   <mrow>
    <mi>X</mi>
    <mo>⊗</mo>
    <mi>Y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>Z</ci>
    <apply>
     <csymbol cd="latexml">tensor-product</csymbol>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z=X\otimes Y
  </annotation>
 </semantics>
</math>

, i.e. there exists some unknown 

<math display="inline" id="Statistical_learning_theory:4">
 <semantics>
  <mrow>
   <mrow>
    <mi>p</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>z</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>p</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>p</ci>
     <ci>z</ci>
    </apply>
    <apply>
     <times></times>
     <ci>p</ci>
     <interval closure="open">
      <apply>
       <ci>normal-→</ci>
       <ci>x</ci>
      </apply>
      <ci>y</ci>
     </interval>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(z)=p(\vec{x},y)
  </annotation>
 </semantics>
</math>

. The training set is made up of 

<math display="inline" id="Statistical_learning_theory:5">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 samples from this probability distribution, and is notated</p>

<p>

<math display="block" id="Statistical_learning_theory:6">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mn>1</mn>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>y</mi>
      <mn>1</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mi>n</mi>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>y</mi>
      <mi>n</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mover accent="true">
      <mi>z</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mover accent="true">
      <mi>z</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>S</ci>
     <set>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <ci>normal-→</ci>
         <ci>x</ci>
        </apply>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <cn type="integer">1</cn>
       </apply>
      </interval>
      <ci>normal-…</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <ci>normal-→</ci>
         <ci>x</ci>
        </apply>
        <ci>n</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>n</ci>
       </apply>
      </interval>
     </set>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <set>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-→</ci>
        <ci>z</ci>
       </apply>
       <cn type="integer">1</cn>
      </apply>
      <ci>normal-…</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-→</ci>
        <ci>z</ci>
       </apply>
       <ci>n</ci>
      </apply>
     </set>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S=\{(\vec{x}_{1},y_{1}),\dots,(\vec{x}_{n},y_{n})\}=\{\vec{z}_{1},\dots,\vec{z%
}_{n}\}
  </annotation>
 </semantics>
</math>

 Every 

<math display="inline" id="Statistical_learning_theory:7">
 <semantics>
  <msub>
   <mover accent="true">
    <mi>x</mi>
    <mo stretchy="false">→</mo>
   </mover>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}_{i}
  </annotation>
 </semantics>
</math>

 is an input vector from the training data, and 

<math display="inline" id="Statistical_learning_theory:8">
 <semantics>
  <msub>
   <mi>y</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}
  </annotation>
 </semantics>
</math>

 is the output that corresponds to it.</p>

<p>In this formalism, the inference problem consists of finding a function 

<math display="inline" id="Statistical_learning_theory:9">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>:</mo>
   <mrow>
    <mi>X</mi>
    <mo>↦</mo>
    <mi>Y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>f</ci>
    <apply>
     <csymbol cd="latexml">maps-to</csymbol>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f:X\mapsto Y
  </annotation>
 </semantics>
</math>

 such that 

<math display="inline" id="Statistical_learning_theory:10">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>∼</mo>
   <mi>y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <apply>
     <times></times>
     <ci>f</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\vec{x})\sim y
  </annotation>
 </semantics>
</math>

. Let 

<math display="inline" id="Statistical_learning_theory:11">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℋ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℋ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

 be a space of functions 

<math display="inline" id="Statistical_learning_theory:12">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>:</mo>
   <mrow>
    <mi>X</mi>
    <mo>↦</mo>
    <mi>Y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>f</ci>
    <apply>
     <csymbol cd="latexml">maps-to</csymbol>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f:X\mapsto Y
  </annotation>
 </semantics>
</math>

 called the hypothesis space. The hypothesis space is the space of functions the algorithm will search through. Let 

<math display="inline" id="Statistical_learning_theory:13">
 <semantics>
  <mrow>
   <mi>V</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>V</ci>
    <interval closure="open">
     <apply>
      <times></times>
      <ci>f</ci>
      <apply>
       <ci>normal-→</ci>
       <ci>x</ci>
      </apply>
     </apply>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x}),y)
  </annotation>
 </semantics>
</math>

 be the <a href="loss_functional" title="wikilink">loss functional</a>, a metric for the difference between the predicted value 

<math display="inline" id="Statistical_learning_theory:14">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\vec{x})
  </annotation>
 </semantics>
</math>

 and the actual value 

<math display="inline" id="Statistical_learning_theory:15">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

. The <a href="expected_risk" title="wikilink">expected risk</a> is defined to be</p>

<p>

<math display="block" id="Statistical_learning_theory:16">
 <semantics>
  <mrow>
   <mrow>
    <mi>I</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>f</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">∫</mo>
     <mrow>
      <mi>X</mi>
      <mo>⊗</mo>
      <mi>Y</mi>
     </mrow>
    </msub>
    <mrow>
     <mi>V</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mover accent="true">
         <mi>x</mi>
         <mo stretchy="false">→</mo>
        </mover>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>,</mo>
      <mi>y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo>,</mo>
      <mi>y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>d</mi>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mi>d</mi>
     <mi>y</mi>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>I</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <ci>f</ci>
     </apply>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <int></int>
      <apply>
       <csymbol cd="latexml">tensor-product</csymbol>
       <ci>X</ci>
       <ci>Y</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>V</ci>
      <interval closure="open">
       <apply>
        <times></times>
        <ci>f</ci>
        <apply>
         <ci>normal-→</ci>
         <ci>x</ci>
        </apply>
       </apply>
       <ci>y</ci>
      </interval>
      <ci>p</ci>
      <interval closure="open">
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>y</ci>
      </interval>
      <ci>d</ci>
      <apply>
       <ci>normal-→</ci>
       <ci>x</ci>
      </apply>
      <ci>d</ci>
      <ci>y</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[f]=\displaystyle\int_{X\otimes Y}V(f(\vec{x}),y)p(\vec{x},y)d\vec{x}dy
  </annotation>
 </semantics>
</math>

 The target function, the best possible function 

<math display="inline" id="Statistical_learning_theory:17">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 that can be chosen, is given by the 

<math display="inline" id="Statistical_learning_theory:18">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 that satisfies</p>

<p>

<math display="block" id="Statistical_learning_theory:19">
 <semantics>
  <mrow>
   <munder>
    <mo movablelimits="false">inf</mo>
    <mrow>
     <mi>f</mi>
     <mo>∈</mo>
     <mi class="ltx_font_mathcaligraphic">ℋ</mi>
    </mrow>
   </munder>
   <mrow>
    <mi>I</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>f</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <csymbol cd="latexml">infimum</csymbol>
     <apply>
      <in></in>
      <ci>f</ci>
      <ci>ℋ</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>I</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <ci>f</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \inf_{f\in\mathcal{H}}I[f]
  </annotation>
 </semantics>
</math>

</p>

<p>Because the probability distribution 

<math display="inline" id="Statistical_learning_theory:20">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <interval closure="open">
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\vec{x},y)
  </annotation>
 </semantics>
</math>

 is unknown, a proxy measure for the expected risk must be used. This measure is based on the training set, a sample from this unknown probability distribution. It is called the <a href="empirical_risk" title="wikilink">empirical risk</a></p>

<p>

<math display="block" id="Statistical_learning_theory:21">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>I</mi>
     <mi>S</mi>
    </msub>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>f</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mi>n</mi>
    </mfrac>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </munderover>
     <mrow>
      <mi>V</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>f</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <msub>
          <mover accent="true">
           <mi>x</mi>
           <mo stretchy="false">→</mo>
          </mover>
          <mi>i</mi>
         </msub>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>,</mo>
       <msub>
        <mi>y</mi>
        <mi>i</mi>
       </msub>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>I</ci>
      <ci>S</ci>
     </apply>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <ci>f</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>n</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <ci>V</ci>
       <interval closure="open">
        <apply>
         <times></times>
         <ci>f</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <apply>
           <ci>normal-→</ci>
           <ci>x</ci>
          </apply>
          <ci>i</ci>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I_{S}[f]=\frac{1}{n}\displaystyle\sum_{i=1}^{n}V(f(\vec{x}_{i}),y_{i})
  </annotation>
 </semantics>
</math>

 A learning algorithm that chooses the function 

<math display="inline" id="Statistical_learning_theory:22">
 <semantics>
  <msub>
   <mi>f</mi>
   <mi>S</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <ci>S</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{S}
  </annotation>
 </semantics>
</math>

 that minimizes the empirical risk is called <a href="empirical_risk_minimization" title="wikilink">empirical risk minimization</a>.</p>
<h2 id="loss-functions">Loss Functions</h2>

<p>The choice of loss function is a determining factor on the function 

<math display="inline" id="Statistical_learning_theory:23">
 <semantics>
  <msub>
   <mi>f</mi>
   <mi>S</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <ci>S</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{S}
  </annotation>
 </semantics>
</math>

 that will be chosen by the learning algorithm. The loss function also affects the convergence rate for an algorithm. It is important for the loss function to be convex.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>Different loss functions are used depending on whether the problem is one of regression or one of classification.</p>
<h3 id="regression">Regression</h3>

<p>The most common loss function for regression is the square loss function. This familiar loss function is used in ordinary least squares regression. The form is:</p>

<p>

<math display="block" id="Statistical_learning_theory:24">
 <semantics>
  <mrow>
   <mrow>
    <mi>V</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>y</mi>
      <mo>-</mo>
      <mrow>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mover accent="true">
         <mi>x</mi>
         <mo stretchy="false">→</mo>
        </mover>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>V</ci>
     <interval closure="open">
      <apply>
       <times></times>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <minus></minus>
      <ci>y</ci>
      <apply>
       <times></times>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x}),y)=(y-f(\vec{x}))^{2}
  </annotation>
 </semantics>
</math>

</p>

<p>The absolute value loss is also sometimes used:</p>

<p>

<math display="block" id="Statistical_learning_theory:25">
 <semantics>
  <mrow>
   <mrow>
    <mi>V</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">|</mo>
    <mrow>
     <mi>y</mi>
     <mo>-</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo stretchy="false">|</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>V</ci>
     <interval closure="open">
      <apply>
       <times></times>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <abs></abs>
     <apply>
      <minus></minus>
      <ci>y</ci>
      <apply>
       <times></times>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x}),y)=|y-f(\vec{x})|
  </annotation>
 </semantics>
</math>

</p>
<h3 id="classification">Classification</h3>

<p>In some sense the 0-1 <a href="indicator_function" title="wikilink">indicator function</a> is the most natural loss function for classification. It takes the value 0 if the predicted output is the same as the actual output, and it takes the value 1 if the predicted output is different from the actual output. For binary classification, this is:</p>

<p>

<math display="block" id="Statistical_learning_theory:26">
 <semantics>
  <mrow>
   <mrow>
    <mi>V</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo>,</mo>
       <mi>y</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>θ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mo>-</mo>
      <mrow>
       <mi>y</mi>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mover accent="true">
         <mi>x</mi>
         <mo stretchy="false">→</mo>
        </mover>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>V</ci>
     <apply>
      <times></times>
      <ci>f</ci>
      <interval closure="open">
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>y</ci>
      </interval>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>θ</ci>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>y</ci>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x},y))=\theta(-yf(\vec{x}))
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Statistical_learning_theory:27">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta
  </annotation>
 </semantics>
</math>

 is the Heaviside step function.</p>

<p>The 0-1 loss function, however, is not convex. The hinge loss is thus often used:</p>

<p>

<math display="block" id="Statistical_learning_theory:28">
 <semantics>
  <mrow>
   <mrow>
    <mi>V</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo>,</mo>
       <mi>y</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mo>-</mo>
      <mrow>
       <mi>y</mi>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mover accent="true">
         <mi>x</mi>
         <mo stretchy="false">→</mo>
        </mover>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>+</mo>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>V</ci>
     <apply>
      <times></times>
      <ci>f</ci>
      <interval closure="open">
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>y</ci>
      </interval>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>y</ci>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
     <plus></plus>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x},y))=(-yf(\vec{x}))_{+}
  </annotation>
 </semantics>
</math>

</p>
<h2 id="regularization">Regularization</h2>
<figure><b>(Figure)</b>
<embed src="Overfitting on Training Set Data.pdf" title="This image represents an example of overfitting in machine learning. The red dots represent training set data. The green line represents the true functional relationship, while the blue line shows the learned function, which has fallen victim to overfitting."></embed><figcaption>This image represents an example of overfitting in machine learning. The red dots represent training set data. The green line represents the true functional relationship, while the blue line shows the learned function, which has fallen victim to overfitting.</figcaption>
</figure>

<p>In machine learning problems, a major problem that arises is that of overfitting. Because learning is a prediction problem, the goal is not to find a function that most closely fits the (previously observed) data, but to find one that will most accurately predict output from future input. Empirical risk minimization runs this risk of overfitting: finding a function that matches the data exactly but does not predict future output well.</p>

<p>Overfitting is symptomatic of unstable solutions; a small perturbation in the training set data would cause a large variation in the learned function. It can be shown that if the stability for the solution can be guaranteed, generalization and consistency are guaranteed as well.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> <a href="Regularization_(mathematics)" title="wikilink">Regularization</a> can solve the overfitting problem and give the problem stability.</p>

<p>Regularization can be accomplished by restricting the hypothesis space 

<math display="inline" id="Statistical_learning_theory:29">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℋ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℋ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

. A common example would be restricting 

<math display="inline" id="Statistical_learning_theory:30">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℋ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℋ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

 to linear functions: this can be seen as a reduction to the standard problem of <a href="linear_regression" title="wikilink">linear regression</a>. 

<math display="inline" id="Statistical_learning_theory:31">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℋ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℋ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

 could also be restricted to polynomial of degree 

<math display="inline" id="Statistical_learning_theory:32">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

, exponentials, or bounded functions on <a href="Lp_space" title="wikilink">L1</a>. Restriction of the hypothesis space avoids overfitting because the form of the potential functions are limited, and so does not allow for the choice of a function that gives empirical risk arbitrarily close to zero.</p>

<p>One example of regularization is <a href="Tikhonov_regularization" title="wikilink">Tikhonov regularization</a>. This consists of minimizing</p>

<p>

<math display="block" id="Statistical_learning_theory:33">
 <semantics>
  <mrow>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mi>n</mi>
    </mfrac>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </munderover>
     <mrow>
      <mi>V</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>f</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <msub>
          <mover accent="true">
           <mi>x</mi>
           <mo stretchy="false">→</mo>
          </mover>
          <mi>i</mi>
         </msub>
         <mo>,</mo>
         <msub>
          <mi>y</mi>
          <mi>i</mi>
         </msub>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mi>γ</mi>
    <msubsup>
     <mrow>
      <mo>∥</mo>
      <mi>f</mi>
      <mo>∥</mo>
     </mrow>
     <mi class="ltx_font_mathcaligraphic">ℋ</mi>
     <mn>2</mn>
    </msubsup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>n</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <ci>V</ci>
       <apply>
        <times></times>
        <ci>f</ci>
        <interval closure="open">
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <apply>
           <ci>normal-→</ci>
           <ci>x</ci>
          </apply>
          <ci>i</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>y</ci>
          <ci>i</ci>
         </apply>
        </interval>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>γ</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <ci>f</ci>
       </apply>
       <ci>ℋ</ci>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{1}{n}\displaystyle\sum_{i=1}^{n}V(f(\vec{x}_{i},y_{i}))+\gamma\|f\|_{%
\mathcal{H}}^{2}
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Statistical_learning_theory:34">
 <semantics>
  <mi>γ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>γ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma
  </annotation>
 </semantics>
</math>

 is a fixed and positive parameter, the regularization parameter. Tikhonov regularization ensures existence, uniqueness, and stability of the solution.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> </p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Reproducing_kernel_Hilbert_spaces" title="wikilink">Reproducing kernel Hilbert spaces</a> are a useful choice for 

<math display="inline" id="Statistical_learning_theory:35">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℋ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℋ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

.</li>
<li><a href="Proximal_gradient_methods_for_learning" title="wikilink">Proximal gradient methods for learning</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="Mehryar_Mohri" title="wikilink">Mehryar Mohri</a>, Afshin Rostamizadeh, Ameet Talwalkar (2012) <em>Foundations of Machine Learning</em>, The MIT Press ISBN 9780262018258.<a href="#fnref1">↩</a></li>
<li id="fn2">Gagan Sidhu, Brian Caffo. Exploiting pitcher decision-making using Reinforcement Learning. <em>Annals of Applied Statistics</em><a href="#fnref2">↩</a></li>
<li id="fn3">Tomaso Poggio, Lorenzo Rosasco, et al. <em>Statistical Learning Theory and Applications</em>, 2012, Class 1 <a href="http://www.mit.edu/~9.520/spring12/slides/class01/class01.pdf">1</a><a href="#fnref3">↩</a></li>
<li id="fn4">Rosasco, L., Vito, E.D., Caponnetto, A., Fiana, M., and Verri A. 2004. <em>Neural computation</em> Vol 16, pp 1063-1076<a href="#fnref4">↩</a></li>
<li id="fn5">Vapnik, V.N. and Chervonenkis, A.Y. 1971. On the uniform convergence of relative frequencies of events to their probabilities. <em>Theory of Probability and its Applications</em> Vol 16, pp 264-280.<a href="#fnref5">↩</a></li>
<li id="fn6">Mukherjee, S., Niyogi, P. Poggio, T., and Rifkin, R. 2006. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. <em>Advances in Computational Mathematics</em>. Vol 25, pp 161-193.<a href="#fnref6">↩</a></li>
<li id="fn7">Tomaso Poggio, Lorenzo Rosasco, et al. <em>Statistical Learning Theory and Applications</em>, 2012, Class 2 <a href="http://www.mit.edu/~9.520/spring12/slides/class02/class02.pdf">2</a><a href="#fnref7">↩</a></li>
</ol>
</section>
</body>
</html>
