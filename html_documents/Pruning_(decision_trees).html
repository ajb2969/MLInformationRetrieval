<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="313">Pruning (decision trees)</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Pruning (decision trees)</h1>
<hr/>

<p><strong>Pruning</strong> is a technique in <a href="machine_learning" title="wikilink">machine learning</a> that reduces the size of <a href="Decision_tree_learning" title="wikilink">decision trees</a> by removing sections of the tree that provide little power to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of <a class="uri" href="overfitting" title="wikilink">overfitting</a>.</p>
<h2 id="introduction">Introduction</h2>

<p>One of the questions that arises in a decision tree algorithm is the optimal size of the final tree. A tree that is too large risks <a class="uri" href="overfitting" title="wikilink">overfitting</a> the training data and poorly generalizing to new samples. A small tree might not capture important structural information about the sample space. However, it is hard to tell when a tree algorithm should stop because it is impossible to tell if the addition of a single extra node will dramatically decrease error. This problem is known as the <a href="horizon_effect" title="wikilink">horizon effect</a>. A common strategy is to grow the tree until each node contains a small number of instances then use pruning to remove nodes that do not provide additional information.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>Pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a <a href="cross-validation_(statistics)" title="wikilink">cross-validation</a> set. There are many techniques for tree pruning that differ in the measurement that is used to optimize performance.</p>
<h2 id="techniques">Techniques</h2>

<p>Pruning can occur in a top down or bottom up fashion. A top down pruning will traverse nodes and trim subtrees starting at the root, while a bottom up pruning will start at the leaf nodes. Below are several popular pruning algorithms.</p>
<h3 id="reduced-error-pruning">Reduced error pruning</h3>

<p>One of the simplest forms of pruning is reduced error pruning. Starting at the leaves, each node is replaced with its most popular class. If the prediction accuracy is not affected then the change is kept. While somewhat naive, reduced error pruning has the advantage of <strong>simplicity and speed</strong>.</p>
<h3 id="cost-complexity-pruning">Cost complexity pruning</h3>

<p>Cost complexity pruning generates a series of trees 

<math display="inline" id="Pruning_(decision_trees):0">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Pruning_(decision_trees):1">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

 is the initial tree and 

<math display="inline" id="Pruning_(decision_trees):2">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

 is the root alone. At step 

<math display="inline" id="Pruning_(decision_trees):3">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

 the tree is created by removing a subtree from tree 

<math display="inline" id="Pruning_(decision_trees):4">
 <semantics>
  <mrow>
   <mi>i</mi>
   <mo>-</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>i</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i-1
  </annotation>
 </semantics>
</math>

 and replacing it with a leaf node with value chosen as in the tree building algorithm. The subtree that is removed is chosen as follows. Define the error rate of tree 

<math display="inline" id="Pruning_(decision_trees):5">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

 over data set 

<math display="inline" id="Pruning_(decision_trees):6">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 as 

<math display="inline" id="Pruning_(decision_trees):7">
 <semantics>
  <mrow>
   <mi>e</mi>
   <mi>r</mi>
   <mi>r</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>T</mi>
    <mo>,</mo>
    <mi>S</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>e</ci>
    <ci>r</ci>
    <ci>r</ci>
    <interval closure="open">
     <ci>T</ci>
     <ci>S</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   err(T,S)
  </annotation>
 </semantics>
</math>

. The subtree that minimizes 

<math display="inline" id="Pruning_(decision_trees):8">
 <semantics>
  <mfrac>
   <mrow>
    <mrow>
     <mi>e</mi>
     <mi>r</mi>
     <mi>r</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>p</mi>
       <mi>r</mi>
       <mi>u</mi>
       <mi>n</mi>
       <mi>e</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>T</mi>
        <mo>,</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>,</mo>
      <mi>S</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mi>e</mi>
     <mi>r</mi>
     <mi>r</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>T</mi>
      <mo>,</mo>
      <mi>S</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mrow>
    <mrow>
     <mo stretchy="false">|</mo>
     <mrow>
      <mi>l</mi>
      <mi>e</mi>
      <mi>a</mi>
      <mi>v</mi>
      <mi>e</mi>
      <mi>s</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>T</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">|</mo>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mo stretchy="false">|</mo>
     <mrow>
      <mi>l</mi>
      <mi>e</mi>
      <mi>a</mi>
      <mi>v</mi>
      <mi>e</mi>
      <mi>s</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>p</mi>
        <mi>r</mi>
        <mi>u</mi>
        <mi>n</mi>
        <mi>e</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>T</mi>
         <mo>,</mo>
         <mi>t</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">|</mo>
    </mrow>
   </mrow>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>e</ci>
      <ci>r</ci>
      <ci>r</ci>
      <interval closure="open">
       <apply>
        <times></times>
        <ci>p</ci>
        <ci>r</ci>
        <ci>u</ci>
        <ci>n</ci>
        <ci>e</ci>
        <interval closure="open">
         <ci>T</ci>
         <ci>t</ci>
        </interval>
       </apply>
       <ci>S</ci>
      </interval>
     </apply>
     <apply>
      <times></times>
      <ci>e</ci>
      <ci>r</ci>
      <ci>r</ci>
      <interval closure="open">
       <ci>T</ci>
       <ci>S</ci>
      </interval>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <abs></abs>
      <apply>
       <times></times>
       <ci>l</ci>
       <ci>e</ci>
       <ci>a</ci>
       <ci>v</ci>
       <ci>e</ci>
       <ci>s</ci>
       <ci>T</ci>
      </apply>
     </apply>
     <apply>
      <abs></abs>
      <apply>
       <times></times>
       <ci>l</ci>
       <ci>e</ci>
       <ci>a</ci>
       <ci>v</ci>
       <ci>e</ci>
       <ci>s</ci>
       <apply>
        <times></times>
        <ci>p</ci>
        <ci>r</ci>
        <ci>u</ci>
        <ci>n</ci>
        <ci>e</ci>
        <interval closure="open">
         <ci>T</ci>
         <ci>t</ci>
        </interval>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{err(prune(T,t),S)-err(T,S)}{|leaves(T)|-|leaves(prune(T,t))|}
  </annotation>
 </semantics>
</math>

 is chosen for removal. The function 

<math display="inline" id="Pruning_(decision_trees):9">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mi>r</mi>
   <mi>u</mi>
   <mi>n</mi>
   <mi>e</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>T</mi>
    <mo>,</mo>
    <mi>t</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <ci>r</ci>
    <ci>u</ci>
    <ci>n</ci>
    <ci>e</ci>
    <interval closure="open">
     <ci>T</ci>
     <ci>t</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   prune(T,t)
  </annotation>
 </semantics>
</math>

 defines the tree gotten by pruning the subtrees 

<math display="inline" id="Pruning_(decision_trees):10">
 <semantics>
  <mi>t</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>t</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t
  </annotation>
 </semantics>
</math>

 from the tree 

<math display="inline" id="Pruning_(decision_trees):11">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

. Once the series of trees has been created, the best tree is chosen by generalized accuracy as measured by a training set or cross-validation.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Alpha-beta_pruning" title="wikilink">Alpha-beta pruning</a></li>
<li><a href="Artificial_neural_network" title="wikilink">Artificial neural network</a></li>
<li><a href="Null-move_heuristic" title="wikilink">Null-move heuristic</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><a href="Judea_Pearl" title="wikilink">Judea Pearl</a>, <em>Heuristics</em>, Addison-Wesley, 1984</li>
<li>Pessimistic Decision tree pruning based on Tree size<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></li>
</ul>
<h2 id="further-reading">Further reading</h2>
<ul>
<li><a href="http://www.almaden.ibm.com/cs/projects/iis/hdb/Publications/papers/kdd95_mdl.pdf">MDL based decision tree pruning</a></li>
<li><a href="http://www.cp.eng.chula.ac.th/~boonserm/publication/ijcnn_kc2001.pdf">Decision tree pruning using backpropagation neural networks</a></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.cis.upenn.edu/~mkearns/papers/pruning.pdf">Fast, Bottom-Up Decision Tree Pruning Algorithm</a></li>
<li><a href="http://www.math.tau.ac.il/~mansour/ml-course/scribe11.ps">Introduction to Decision tree pruning</a></li>
</ul>

<p>"</p>

<p><a href="Category:Decision_trees" title="wikilink">Category:Decision trees</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Tevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer: 2001, pp. 269-272<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
</ol>
</section>
</body>
</html>
