<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1237">Gustafson's law</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Gustafson's law</h1>
<hr/>
<figure><b>(Figure)</b>
<figcaption>Gustafson's Law</figcaption>
</figure>

<p><strong>Gustafson's Law</strong> (also known as <strong>Gustafson–Barsis' law</strong><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a>) is a law in <a href="computer_science" title="wikilink">computer science</a> which says that computations involving arbitrarily large data sets can be efficiently <a href="parallel_computing" title="wikilink">parallelized</a>. Gustafson's Law provides a counterpoint to <a href="Amdahl's_law" title="wikilink">Amdahl's law</a>, which describes a limit on the speed-up that parallelization can provide, given a fixed data set size. Gustafson's law was first described <a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> by <a href="John_L._Gustafson" title="wikilink">John L. Gustafson</a> and his colleague <a href="Edwin_H._Barsis" title="wikilink">Edwin H. Barsis</a>:</p>

<p>

<math display="block" id="Gustafson's_law:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>S</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>P</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>P</mi>
    <mo>-</mo>
    <mrow>
     <mi>α</mi>
     <mo>⋅</mo>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>P</mi>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>S</ci>
     <ci>P</ci>
    </apply>
    <apply>
     <minus></minus>
     <ci>P</ci>
     <apply>
      <ci>normal-⋅</ci>
      <ci>α</ci>
      <apply>
       <minus></minus>
       <ci>P</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S(P)=P-\alpha\cdot(P-1)
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>P</em> is the number of processors, <em>S</em> is the <a class="uri" href="speedup" title="wikilink">speedup</a>, and 

<math display="inline" id="Gustafson's_law:1">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 the non-parallelizable fraction of any parallel process.</p>

<p>Gustafson's law addresses the shortcomings of <a href="Amdahl's_law" title="wikilink">Amdahl's law</a>, which does not fully exploit the computing power that becomes available as the number of machines increases. Gustafson's Law instead proposes that programmers tend to set the size of problems to use the available equipment to solve problems within a practical fixed time. Therefore, if faster (more parallel) equipment is available, larger problems can be solved in the same time.</p>

<p>Accordingly, Gustafson called his metric <em>scaled speedup</em>, because in the above expression <em>S</em>(<em>P</em>) is the ratio of the total, single-process execution time to the per-process parallel execution time; the former scales with <em>P</em>, while the latter is assumed fixed or nearly so. This is in contrast to Amdahl's Law, which takes the single-process execution time to be the fixed quantity, and compares it to a shrinking per-process parallel execution time. Thus, Amdahl's law is based on the assumption of a fixed <a href="problem_size" title="wikilink">problem size</a>: it assumes the overall <a class="uri" href="workload" title="wikilink">workload</a> of a program does not change with respect to machine size (i.e., the number of processors). Both laws assume the parallelizable part is evenly distributed over <em>P</em> processors.</p>

<p>The impact of Gustafson's law was to shift research goals to select or reformulate problems so that solving a larger problem in the same amount of time would be possible. In a way the law redefines efficiency, due to the possibility that limitations imposed by the sequential part of a program may be countered by increasing the total amount of computation.</p>
<h2 id="derivation-of-gustafsons-law">Derivation of Gustafson's law</h2>

<p>The execution time of the program on a <a href="Parallel_computing" title="wikilink">parallel computer</a> is decomposed into:</p>

<p>

<math display="block" id="Gustafson's_law:2">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mrow>
    <mi>a</mi>
    <mo>+</mo>
    <mi>b</mi>
   </mrow>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <ci>a</ci>
    <ci>b</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (a+b)
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Gustafson's_law:3">
 <semantics>
  <mi>a</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>a</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a
  </annotation>
 </semantics>
</math>

 is the sequential time and 

<math display="inline" id="Gustafson's_law:4">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

 is the parallel time, on any of the <em>P</em> processors. (Overhead is ignored.)</p>

<p>The key assumption of Gustafson and Barsis is that the total amount of work to be done in parallel <em>varies linearly with the number of processors</em>. Then practical implication is of the single processor being more capable than the single processing assignment to be executed in parallel with (typically similar) other assignments. This implies that <em>b</em>, the per-process parallel time, should be held fixed as <em>P</em> is varied. The corresponding time for sequential processing is</p>

<p>

<math display="block" id="Gustafson's_law:5">
 <semantics>
  <mrow>
   <mrow>
    <mi>a</mi>
    <mo>+</mo>
    <mrow>
     <mi>P</mi>
     <mo>⋅</mo>
     <mi>b</mi>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <ci>a</ci>
    <apply>
     <ci>normal-⋅</ci>
     <ci>P</ci>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a+P\cdot b.
  </annotation>
 </semantics>
</math>

</p>

<p>Speedup is accordingly:</p>

<p>

<math display="block" id="Gustafson's_law:6">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>a</mi>
      <mo>+</mo>
      <mrow>
       <mi>P</mi>
       <mo>⋅</mo>
       <mi>b</mi>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>a</mi>
      <mo>+</mo>
      <mi>b</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <plus></plus>
     <ci>a</ci>
     <apply>
      <ci>normal-⋅</ci>
      <ci>P</ci>
      <ci>b</ci>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <ci>a</ci>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (a+P\cdot b)/(a+b).
  </annotation>
 </semantics>
</math>

</p>

<p>Defining</p>

<p>

<math display="block" id="Gustafson's_law:7">
 <semantics>
  <mrow>
   <mi>α</mi>
   <mo>=</mo>
   <mrow>
    <mi>a</mi>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>a</mi>
      <mo>+</mo>
      <mi>b</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>α</ci>
    <apply>
     <divide></divide>
     <ci>a</ci>
     <apply>
      <plus></plus>
      <ci>a</ci>
      <ci>b</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha=a/(a+b)
  </annotation>
 </semantics>
</math>

</p>

<p>to be the sequential fraction of the parallel execution time, we have</p>

<p>

<math display="block" id="Gustafson's_law:8">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>S</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>P</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mi>α</mi>
     <mo>+</mo>
     <mrow>
      <mi>P</mi>
      <mo>⋅</mo>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mi>α</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mi>P</mi>
     <mo>-</mo>
     <mrow>
      <mi>α</mi>
      <mo>⋅</mo>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>P</mi>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>S</ci>
      <ci>P</ci>
     </apply>
     <apply>
      <plus></plus>
      <ci>α</ci>
      <apply>
       <ci>normal-⋅</ci>
       <ci>P</ci>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <ci>α</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <minus></minus>
      <ci>P</ci>
      <apply>
       <ci>normal-⋅</ci>
       <ci>α</ci>
       <apply>
        <minus></minus>
        <ci>P</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S(P)=\alpha+P\cdot(1-\alpha)=P-\alpha\cdot(P-1).
  </annotation>
 </semantics>
</math>

</p>

<p>Thus, if 

<math display="inline" id="Gustafson's_law:9">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 is small, the <a class="uri" href="speedup" title="wikilink">speedup</a> is approximately <em>P</em>, as desired. It may even be the case that 

<math display="inline" id="Gustafson's_law:10">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 diminishes as <em>P</em> (together with the problem size) increases; if that holds true, then <em>S</em> approaches <em>P</em> monotonically with the growth of <em>P</em>.</p>

<p>Thus Gustafson's law seems to rescue <a href="Parallel_computing" title="wikilink">parallel processing</a> from <a href="Amdahl's_law" title="wikilink">Amdahl's law</a>. It is based on the idea that if the problem size is allowed to grow monotonically with <em>P</em>, then the sequential fraction of the workload would not ultimately come to dominate. This is enabled by means of having most of the assignments individually containable within a single processor's scope of processing; thus a single processor may provide for multiple assignments, while a single assignment shouldn't span more than a single processor. This is also the rule for relating projects with work-sites, having multiple projects per site, but only one site per project.</p>
<h2 id="a-driving-metaphor">A driving metaphor</h2>

<p>Amdahl's Law approximately suggests: </p>

<p>Gustafson's Law approximately states: </p>
<h2 id="applications">Applications</h2>
<h3 id="application-in-research">Application in research</h3>

<p>Amdahl's law presupposes that the computing requirements will stay the same, given increased processing power. In other words, an analysis of the same data will take less time given more computing power.</p>

<p>Gustafson, on the other hand, argues that more computing power will cause the data to be more carefully and fully analyzed: pixel by pixel or unit by unit, rather than on a larger scale. Where it would not have been possible or practical to simulate the impact of nuclear detonation on every building, car, and their contents (including furniture, structure strength, etc.) because such a calculation would have taken more time than was available to provide an answer, the increase in computing power will prompt researchers to add more data to more fully simulate more variables, giving a more accurate result.</p>
<h3 id="application-in-everyday-computer-systems">Application in everyday computer systems</h3>

<p>Amdahl's law reveals a limitation in, for example, the ability of multiple cores to reduce the time it takes for a computer to boot to its operating system and be ready for use. Assuming the boot process was mostly parallel, quadrupling computing power on a system that took one minute to load might reduce the boot time to just over fifteen seconds. But greater and greater parallelization would eventually fail to make bootup go any faster, if any part of the boot process were inherently sequential.</p>

<p>Gustafson's law argues that a fourfold increase in computing power would instead lead to a similar increase in expectations of what the system will be capable of. If the one-minute load time is acceptable to most users, then that is a starting point from which to increase the features and functions of the system. The time taken to boot to the operating system will be the same, i.e. one minute, but the new system would include more graphical or user-friendly features.</p>
<h2 id="limitations">Limitations</h2>

<p>Some problems do not have fundamentally larger datasets. As example, processing one data point per world citizen gets larger at only a few percent per year. The principal point of Gustafson's law is that such problems are not likely to be the most fruitful applications of parallelism.</p>

<p>Algorithms with nonlinear runtimes may find it hard to take advantage of parallelism "exposed" by Gustafson's law. Snyder<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> points out an O(N<sup>3</sup>) algorithm means that double the concurrency gives only about a 26% increase in problem size. Thus, while it may be possible to occupy vast concurrency, doing so may bring little advantage over the original, less concurrent solution—however in practice there have been massive improvements.</p>

<p>Hill and Marty<ref name="Hill&amp;Marty2008"><a href="http://www.cs.wisc.edu/multifacet/amdahl/">Amdahl's Law in the Multicore Era</a>, Mark D. Hill and Michael R. Marty, IEEE Computer, vol. 41, pp. 33–38, July 2008. Also UW CS-TR-2007-1593, April 2007. emphasize also that methods of speeding sequential execution are still needed, even for multicore machines. They point out that locally inefficient methods can be globally efficient when they reduce the sequential phase. Furthermore, Woo and Lee<ref name="Woo&amp;Lee2008"><a href="http://www.computer.org/portal/web/csdl/doi/10.1109/MC.2008.530">Extending Amdahl's Law for Energy-Efficient Computing in the Many-Core Era</a>, Dong Hyuk Woo and Hsien-Hsin S. Lee, IEEE Computer, vol. 41, No. 12, pp.24-31, December 2008. studied the implication of energy and power on future many-core processors based on Amdahl's Law, showing that an asymmetric many-core processor can achieve the best possible energy efficiency by activating an optimal number of cores given the amount of parallelism is known prior to execution.</ref></ref></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Scalable_parallelism" title="wikilink">Scalable parallelism</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>

<p>"</p>

<p><a href="Category:Analysis_of_parallel_algorithms" title="wikilink">Category:Analysis of parallel algorithms</a> <a href="Category:Programming_rules_of_thumb" title="wikilink">Category:Programming rules of thumb</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.85.6348">Reevaluating Amdahl's Law</a>, John L. Gustafson, <a href="Communications_of_the_ACM" title="wikilink">Communications of the ACM</a> 31(5), 1988. pp. 532-533. Also as a web page <a href="http://www.johngustafson.net/pubs/pub13/amdahl.htm">here</a><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="http://www.cs.washington.edu/homes/snyder/TypeArchitectures.pdf">Type Architectures, Shared Memory, and The Corollary of Modest Potential</a>, Lawrence Snyder, Ann. Rev. Comput. Sci. 1986. 1:289-317.<a href="#fnref3">↩</a></li>
</ol>
</section>
</body>
</html>
