<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="535">Chernoff bound</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Chernoff bound</h1>
<hr>In [[probability theory]], the '''Chernoff bound''', named after [[Herman Chernoff]] but due to Herman Rubin,<ref>{{cite book | url=htt<p>p://www.crcpress.com/product/isbn/9781482204964 | title=Past, Present, and Future of Statistics | chapter=A career in statistics | page=35 | publisher=CRC Press | last1=Chernoff | first1=Herman | editor-first1=Xihong | editor-last1=Lin | editor-first2=Christian | editor-last2=Genest | editor-first3=David L. | editor-last3=Banks | editor-first4=Geert | editor-last4=Molenberghs | editor-first5=David W. | editor-last5=Scott | editor-first6=Jane-Ling | editor-last6=Wang | year=2014 | isbn=9781482204964 | chapterurl=<a class="uri" href="http://nisla05.niss.org/copss/past-present-future-copss.pdf">http://nisla05.niss.org/copss/past-present-future-copss.pdf</a>}} gives exponentially decreasing bounds on tail distributions of sums of independent random variables. It is a sharper bound than the known first or second moment based tail bounds such as <a href="Markov's_inequality" title="wikilink">Markov's inequality</a> or <a href="Chebyshev_inequality" title="wikilink">Chebyshev inequality</a>, which only yield power-law bounds on tail decay. However, the Chernoff bound requires that the variates be independent – a condition that neither the Markov nor the Chebyshev inequalities require.</p>
<p>It is related to the (historically prior) <a href="Bernstein_inequalities_(probability_theory)" title="wikilink">Bernstein inequalities</a>, and to <a href="Hoeffding's_inequality" title="wikilink">Hoeffding's inequality</a>.</p>
<h2 id="example">Example</h2>
<p>Let <mtpl></mtpl> be independent <a href="Bernoulli_random_variable" title="wikilink">Bernoulli random variables</a>, each having probability <em>p</em> > 1/2 of being equal to 1. Then the probability of simultaneous occurrence of more than <em>n</em>/2 of the events  has an exact value <span class="LaTeX">$S$</span>, where</p>
<p><span class="LaTeX">$$S=\sum_{i = \lfloor \tfrac{n}{2} \rfloor + 1}^n \binom{n}{i}p^i (1 - p)^{n - i} .$$</span></p>
<p>The Chernoff bound shows that <span class="LaTeX">$S$</span> has the following lower bound:</p>
<p><span class="LaTeX">$$S \ge 1 - e^{-\frac{1}{2p}n \left(p - \frac{1}{2} \right)^2} .$$</span></p>
<p>Indeed, noticing that <span class="LaTeX">$μ = np$</span>, we get by the multiplicative form of Chernoff bound (see below or Corollary 13.3 in Sinclair's class notes),<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<p><span class="LaTeX">$$\begin{align}
\Pr\left (\sum_{k=1}^n X_k \le\left\lfloor \tfrac{n}{2}\right\rfloor \right ) &=\Pr\left (\sum_{k=1}^n X_k\le\left(1-\left(1-\tfrac{1}{2p}\right)\right)\mu\right ) \\
&\leq e^{-\frac{\mu}{2}\left(1-\frac{1}{2p}\right)^2} \\
&=e^{-\frac{n}{2p}\left(p-\frac{1}{2}\right)^2}
\end{align}$$</span></p>
<p>This result admits various generalizations as outlined below. One can encounter many flavours of Chernoff bounds: the original <em>additive form</em> (which gives a bound on the <a href="Approximation_error" title="wikilink">absolute error</a>) or the more practical <em>multiplicative form</em> (which bounds the <a href="Approximation_error" title="wikilink">error relative</a> to the mean).</p>
<h2 id="a-motivating-example">A motivating example</h2>
<p> The simplest case of Chernoff bounds is used to bound the success probability of majority agreement for <span class="LaTeX">$n$</span> independent, equally likely events.</p>
<p>A simple motivating example is to consider a biased coin. One side (say, Heads), is more likely to come up than the other, but you don't know which and would like to find out. The obvious solution is to flip it many times and then choose the side that comes up the most. But how many times do you have to flip it to be confident that you've chosen correctly?</p>
<p>In our example, let <mtpl></mtpl> denote the event that the <em>i</em>th coin flip comes up Heads; suppose that we want to ensure we choose the wrong side with at most a small probability <span class="LaTeX">$ε$</span>. Then, rearranging the above, we must have:</p>
<p><span class="LaTeX">$$n \geq \frac{1}{(p -\frac{1}{2})^2} \ln \frac{1}{\sqrt{\varepsilon}}.$$</span></p>
<p>If the coin is noticeably biased, say coming up on one side 60% of the time (<em>p</em> = .6), then we can guess that side with 95% (ε = .05) accuracy after 150 flips (<em>n</em> = 150). If it is 90% biased, then a mere 10 flips suffices. If the coin is only biased a tiny amount, like most real coins are, the number of necessary flips becomes much larger.</p>
<p>More practically, the Chernoff bound is used in <a href="randomized_algorithm" title="wikilink">randomized algorithms</a> (or in computational devices such as <a href="quantum_computer" title="wikilink">quantum computers</a>) to determine a bound on the number of runs necessary to determine a value by majority agreement, up to a specified probability. For example, suppose an algorithm (or machine) <em>A</em> computes the correct value of a function <em>f</em> with probability <em>p</em> > 1/2. If we choose <em>n</em> satisfying the inequality above, the probability that a majority exists and is equal to the correct value is at least 1 − ε, which for small enough ε is quite reliable. If <em>p</em> is a constant, ε diminishes exponentially with growing <em>n</em>, which is what makes algorithms in the complexity class <a href="BPP_(complexity)" title="wikilink">BPP</a> efficient.</p>
<p>Notice that if <em>p</em> is very close to 1/2, the necessary <span class="LaTeX">$n$</span> can become very large. For example, if <em>p</em> = 1/2 + 1/2<sup><em>m</em></sup>, as it might be in some <a href="PP_(complexity)" title="wikilink">PP</a> algorithms, the result is that <span class="LaTeX">$n$</span> is bounded below by an exponential function in <em>m</em>:</p>
<p><span class="LaTeX">$$n \geq 2^{2m} \ln \frac{1}{\sqrt{\varepsilon}}.$$</span></p>
<h2 id="the-first-step-in-the-proof-of-chernoff-bounds">The first step in the proof of Chernoff bounds</h2>
<p>The Chernoff bound for a random variable <span class="LaTeX">$X$</span>, which is the sum of <span class="LaTeX">$n$</span> independent random variables <mtpl></mtpl>, is obtained by applying <mtpl></mtpl> for some well-chosen value of <em>t</em>. This method was first applied by <a href="Sergei_Bernstein" title="wikilink">Sergei Bernstein</a> to prove the related <a href="Bernstein_inequalities_(probability_theory)" title="wikilink">Bernstein inequalities</a>.</p>
<p>From <a href="Markov's_inequality" title="wikilink">Markov's inequality</a> and using independence we can derive the following useful inequality:</p>
<p>For any <em>t</em> > 0,</p>
<p><span class="LaTeX">$$\Pr(X \ge a) = \Pr\left (e^{tX} \ge e^{ta}\right ) \le \frac{ E \left [e^{tX} \right ]}{e^{ta}} = e^{-ta}\mathrm{E} \left [\prod_i e^{tX_i} \right].$$</span></p>
<p>In particular optimizing over <em>t</em> and using independence of <mtpl></mtpl> we obtain,</p>
<p>Similarly,</p>
<p><span class="LaTeX">$$\Pr (X \le a) = \Pr\left (e^{-tX} \ge e^{-ta}\right)$$</span></p>
<p>and so,</p>
<p><span class="LaTeX">$$\Pr (X \le a) \leq \min_{t>0} e^{ta} \prod_i \mathrm{E} \left[e^{-tX_i} \right ]$$</span></p>
<h2 id="precise-statements-and-proofs">Precise statements and proofs</h2>
<h3 id="theorem-for-additive-form-absolute-error">Theorem for additive form (absolute error)</h3>
<p>The following Theorem is due to <a href="Wassily_Hoeffding" title="wikilink">Wassily Hoeffding</a> and hence is called Chernoff-Hoeffding theorem.</p>
<dl>
<dd><strong>Chernoff-Hoeffding Theorem.</strong> Suppose <mtpl></mtpl> are <a class="uri" href="i.i.d." title="wikilink">i.i.d.</a> random variables, taking values in  Let <mtpl> E[<em>X<sub>i</sub></em>]}}</mtpl> and <span class="LaTeX">$ε > 0$</span>. Then
<p>:<math>\begin{align}</math></p>
</dd>
</dl>
<p>\Pr \left (\frac{1}{n} \sum X_i \geq p + \varepsilon \right ) \leq \left (\left (\frac{p}{p + \varepsilon}\right )^{p+\varepsilon} {\left (\frac{1 - p}{1-p- \varepsilon}\right )}^{1 - p- \varepsilon}\right )^n &= e^{-D(p+\varepsilon\|p) n} \\ \Pr \left (\frac{1}{n} \sum X_i \leq p - \varepsilon \right ) \leq \left (\left (\frac{p}{p - \varepsilon}\right )^{p-\varepsilon} {\left (\frac{1 - p}{1-p+ \varepsilon}\right )}^{1 - p+ \varepsilon}\right )^n &= e^{-D(p-\varepsilon\|p) n} \end{align}</p>
<dl>
<dd>where
<p><span class="LaTeX">$$D(x\|y) = x \ln \frac{x}{y} + (1-x) \ln \left (\frac{1-x}{1-y} \right )$$</span></p>
</dd>
<dd>is the <a href="Kullback–Leibler_divergence" title="wikilink">Kullback–Leibler divergence</a> between <a href="Bernoulli_distribution" title="wikilink">Bernoulli distributed</a> random variables with parameters <em>x</em> and <em>y</em> respectively. If <span class="LaTeX">$p ≥ \frac{1}{2} ,$</span> then
<p><span class="LaTeX">$$\Pr\left ( X>np+x \right ) \leq \exp \left (-\frac{x^2}{2np(1-p)} \right ).$$</span></p>
</dd>
</dl>
<h4 id="proof">Proof</h4>
<p>Let <span class="LaTeX">$q = p + ε$</span>. Taking <span class="LaTeX">$a = nq$</span> in (), we obtain:</p>
<p><span class="LaTeX">$$\Pr\left ( \frac{1}{n} \sum X_i \ge q\right )\le \inf_{t>0} \frac{E \left[\prod e^{t X_i}\right]}{e^{tnq}} = \inf_{t>0} \left ( \frac{ E\left[e^{tX_i} \right] }{e^{tq}}\right )^n.$$</span></p>
<p>Now, knowing that <mtpl> 1) {{=}} <em>p</em>, Pr(<em>X<sub>i</sub></em> {{=}} 0) {{=}} 1 − <em>p</em>}}</mtpl>, we have</p>
<p><span class="LaTeX">$$\left (\frac{\mathrm{E}\left[e^{tX_i} \right] }{e^{tq}}\right )^n = \left (\frac{p e^t + (1-p)}{e^{tq} }\right )^n = \left ( pe^{(1-q)t} + (1-p)e^{-qt} \right )^n.$$</span></p>
<p>Therefore we can easily compute the infimum, using calculus:</p>
<p><span class="LaTeX">$$\frac{d}{dt} \left (pe^{(1-q)t} + (1-p)e^{-qt} \right) = (1-q)pe^{(1-q)t}-q(1-p)e^{-qt}$$</span></p>
<p>Setting the equation to zero and solving, we have</p>
<p><span class="LaTeX">$$\begin{align}
(1-q)pe^{(1-q)t} &= q(1-p)e^{-qt} \\
(1-q)pe^{t} &= q(1-p)
\end{align}$$</span></p>
<p>so that</p>
<p><span class="LaTeX">$$e^t = \frac{(1-p)q}{(1-q)p}.$$</span></p>
<p>Thus,</p>
<p><span class="LaTeX">$$t = \log\left(\frac{(1-p)q}{(1-q)p}\right).$$</span></p>
<p>As <span class="LaTeX">$q = p + ε > p$</span>, we see that <span class="LaTeX">$t > 0$</span>, so our bound is satisfied on <span class="LaTeX">$t$</span>. Having solved for <span class="LaTeX">$t$</span>, we can plug back into the equations above to find that</p>
<p><span class="LaTeX">$$\begin{align}
\log \left (pe^{(1-q)t} + (1-p)e^{-qt} \right ) &= \log \left ( e^{-qt}(1-p+pe^t) \right ) \\
&= \log\left (e^{-q \log\left(\frac{(1-p)q}{(1-q)p}\right)}\right) + \log\left(1-p+pe^{\log\left(\frac{1-p}{1-q}\right)}e^{\log\frac{q}{p}}\right ) \\
&= -q\log\frac{1-p}{1-q} -q \log\frac{q}{p} + \log\left(1-p+ p\left(\frac{1-p}{1-q}\right)\frac{q}{p}\right) \\
&= -q\log\frac{1-p}{1-q} -q \log\frac{q}{p} + \log\left(\frac{(1-p)(1-q)}{1-q}+\frac{(1-p)q}{1-q}\right) \\
&= -q \log\frac{q}{p} + \left ( -q\log\frac{1-p}{1-q} + \log\frac{1-p}{1-q} \right ) \\
&= -q\log\frac{q}{p} + (1-q)\log\frac{1-p}{1-q} \\
&= -D(q \| p).
\end{align}$$</span></p>
<p>We now have our desired result, that</p>
<p><span class="LaTeX">$$\Pr \left (\tfrac{1}{n}\sum X_i \ge p + \varepsilon\right ) \le e^{-D(p+\varepsilon\|p) n}.$$</span></p>
<p>To complete the proof for the symmetric case, we simply define the random variable <mtpl> 1 − <em>X<sub>i</sub></em>}}</mtpl>, apply the same proof, and plug it into our bound.</p>
<h4 id="simpler-bounds">Simpler bounds</h4>
<p>A simpler bound follows by relaxing the theorem using <mtpl> <em>p</em>) ≥ 2<em>x</em><sup>2</sup>}}</mtpl>, which follows from the <a href="Convex_function" title="wikilink">convexity</a> of <span class="LaTeX">$D ( p + x {{!!}} p )$</span> and the fact that</p>
<p><span class="LaTeX">$$\frac{d^2}{dx^2} D(p+x\|p) = \frac{1}{(p+x)(1-p-x)}\geq 4=\frac{d^2}{dx^2}(2x^2).$$</span></p>
<p>This result is a special case of <a href="Hoeffding's_inequality" title="wikilink">Hoeffding's inequality</a>. Sometimes, the bound</p>
<p><span class="LaTeX">$$D( (1+x) p \| p) \geq \tfrac{1}{4} x^2 p, \qquad -\tfrac{1}{2} \leq x \leq \tfrac{1}{2},$$</span></p>
<p>which is stronger for <span class="LaTeX">$p  is also used.</span></p>
<h3 id="theorem-for-multiplicative-form-of-chernoff-bound-relative-error">Theorem for multiplicative form of Chernoff bound (relative error)</h3>
<dl>
<dd><strong>Multiplicative Chernoff Bound.</strong> Suppose <mtpl></mtpl> are <a href="Statistical_independence" title="wikilink">independent</a> random variables taking values in  Let <span class="LaTeX">$X$</span> denote their sum and let <span class="LaTeX">$μ = E X X$</span> denote the sum's expected value. Then for any <span class="LaTeX">$δ > 0$</span>,
<p><span class="LaTeX">$$\Pr ( X > (1+\delta)\mu) < \left(\frac{e^\delta}{(1+\delta)^{(1+\delta)}}\right)^\mu.$$</span></p>
</dd>
</dl>
<h4 id="proof-1">Proof</h4>
<p>Set <mtpl> 1) {{=}} <em>p<sub>i</sub></em>}}</mtpl>. According to (),</p>
<p><span class="LaTeX">$$\begin{align}
\Pr (X > (1 + \delta)\mu) &\le \inf_{t > 0} \frac{\mathrm{E}\left[\prod_{i=1}^n\exp(tX_i)\right]}{\exp(t(1+\delta)\mu)}\\
& = \inf_{t > 0} \frac{\prod_{i=1}^n\mathrm{E}\left [e^{tX_i} \right]}{\exp(t(1+\delta)\mu)} \\
& = \inf_{t > 0} \frac{\prod_{i=1}^n\left[p_ie^t + (1-p_i)\right]}{\exp(t(1+\delta)\mu)}
\end{align}$$</span></p>
<p>The third line above follows because <span class="LaTeX">$e^{tX_i}$</span> takes the value <mtpl></mtpl> with probability <mtpl></mtpl> and the value 1 with probability <mtpl></mtpl>. This is identical to the calculation above in the proof of the <a href="#Theorem_for_additive_form_(absolute_error)" title="wikilink">Theorem for additive form (absolute error)</a>.</p>
<p>Rewriting <span class="LaTeX">$p_ie^t + (1-p_i)$</span> as <span class="LaTeX">$p_i(e^t-1) + 1$</span> and recalling that <span class="LaTeX">$1+x \le e^x$</span> (with strict inequality if <span class="LaTeX">$x > 0$</span>), we set <span class="LaTeX">$x = p_i(e^t-1)$</span>. The same result can be obtained by directly replacing <span class="LaTeX">$a$</span> in the equation for the Chernoff bound with <span class="LaTeX">$(1 + δ ) μ$</span>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<p>Thus,</p>
<p><span class="LaTeX">$$\Pr(X > (1+\delta)\mu) < \frac{\prod_{i=1}^n\exp(p_i(e^t-1))}{\exp(t(1+\delta)\mu)} = \frac{\exp\left((e^t-1)\sum_{i=1}^n p_i\right)}{\exp(t(1+\delta)\mu)}  = \frac{\exp((e^t-1)\mu)}{\exp(t(1+\delta)\mu)}.$$</span></p>
<p>If we simply set <span class="LaTeX">$t = log(1 + δ )$</span> so that <span class="LaTeX">$t > 0$</span> for <span class="LaTeX">$δ > 0$</span>, we can substitute and find</p>
<p><span class="LaTeX">$$\frac{\exp((e^t-1)\mu)}{\exp(t(1+\delta)\mu)} = \frac{\exp((1+\delta - 1)\mu)}{(1+\delta)^{(1+\delta)\mu}} = \left[\frac{e^\delta}{(1+\delta)^{(1+\delta)}}\right]^\mu$$</span></p>
<p>This proves the result desired. A similar proof strategy can be used to show that</p>
<p><span class="LaTeX">$$\Pr(X < (1-\delta)\mu) < \left[\frac{\exp(-\delta)}{(1-\delta)^{(1-\delta)}}\right]^\mu.$$</span></p>
<p>The above formula is often unwieldy in practice,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> so the following looser but more convenient bounds are often used:</p>
<p><span class="LaTeX">$$\Pr( X \ge (1+\delta)\mu) \le e^{-\frac{\delta^2\mu}{3}}, \qquad 0 < \delta < 1,$$</span></p>
<p><span class="LaTeX">$$\Pr( X \le (1-\delta)\mu) \le e^{-\frac{\delta^2\mu}{2}}, \qquad 0 < \delta < 1.$$</span></p>
<h3 id="better-chernoff-bounds-for-some-special-cases">Better Chernoff bounds for some special cases</h3>
<p>We can obtain stronger bounds using simpler proof techniques for some special cases of symmetric random variables.</p>
<p>Suppose <mtpl></mtpl> are independent random variables, and let <span class="LaTeX">$X$</span> denote their sum.</p>
<ul>
<li>If <span class="LaTeX">$\Pr(X_i = 1)  = \Pr(X_i = -1) = \tfrac{1}{2}$</span>. Then,</li>
</ul>
<dl>
<dd><dl>
<dd><span class="LaTeX">$\Pr( X \ge a) \le e^{\frac{-a^2}{2n}}, \qquad a > 0,$</span>
</dd>
</dl>
</dd>
<dd>and therefore also
<p><span class="LaTeX">$$\Pr( |X| \ge a) \le 2e^{\frac{-a^2}{2n}}, \qquad a > 0.$$</span></p>
</dd>
</dl>
<ul>
<li>If <span class="LaTeX">$\Pr(X_i = 1) = \Pr(X_i = 0) = \tfrac{1}{2}, \mathrm{E}[X] = \mu = \frac{n}{2}$</span> Then,</li>
</ul>
<dl>
<dd><dl>
<dd><span class="LaTeX">$\Pr( X \ge \mu+a) \le e^{\frac{-2a^2}{n}}, \qquad a > 0,$</span>
</dd>
<dd><span class="LaTeX">$\Pr( X \le \mu-a) \le e^{\frac{-2a^2}{n}}, \qquad 0 < a < \mu,$</span>
</dd>
</dl>
</dd>
</dl>
<h2 id="applications-of-chernoff-bound">Applications of Chernoff bound</h2>
<p>Chernoff bounds have very useful applications in <a href="set_balancing" title="wikilink">set balancing</a> and <a href="Packet_(information_technology)" title="wikilink">packet</a> <a class="uri" href="routing" title="wikilink">routing</a> in <a href="sparse_graph" title="wikilink">sparse</a> networks.</p>
<p>The set balancing problem arises while designing statistical experiments. Typically while designing a statistical experiment, given the features of each participant in the experiment, we need to know how to divide the participants into 2 disjoint groups such that each feature is roughly as balanced as possible between the two groups. Refer to this [<a class="uri" href="http://books.google.com/books?id=0bAYl6d7hvkC&printsec">http://books.google.com/books?id=0bAYl6d7hvkC&printsec</a>;=frontcover&source;=gbs_summary_r&cad;=0#PPA71,M1 book section] for more info on the problem.</p>
<p>Chernoff bounds are also used to obtain tight bounds for permutation routing problems which reduce <a href="network_congestion" title="wikilink">network congestion</a> while routing packets in sparse networks. Refer to this [<a class="uri" href="http://books.google.com/books?id=0bAYl6d7hvkC&printsec">http://books.google.com/books?id=0bAYl6d7hvkC&printsec</a>;=frontcover&source;=gbs_summary_r&cad;=0#PPA72,M1 book section] for a thorough treatment of the problem.</p>
<p>Chernoff bounds can be effectively used to evaluate the "robustness level" of an application/algorithm by exploring its perturbation space with randomization. <a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> The use of the Chernoff bound permits to abandon the strong -and mostly unrealistic- small perturbation hypothesis (the perturbation magnitude is small). The robustness level can be, in turn, used either to validate or reject a specific algorithmic choice, an hardware implementation or the appropriateness of a solution whose structural parameters are affected by uncertainties.</p>
<h2 id="matrix-chernoff-bound">Matrix Chernoff bound</h2>
<p><a href="Rudolf_Ahlswede" title="wikilink">Rudolf Ahlswede</a> and <a href="Andreas_Winter" title="wikilink">Andreas Winter</a> introduced  a Chernoff bound for matrix-valued random variables.</p>
<p>If <em>M</em> is distributed according to some distribution over <span class="LaTeX">$d × d$</span> matrices with zero mean, and if <mtpl></mtpl> are independent copies of <em>M</em> then for any <span class="LaTeX">$ε > 0$</span>,</p>
<p><span class="LaTeX">$$\Pr\left( \left\| \frac{1}{t} \sum_{i=1}^t M_i - \mathrm{E}[M] \right\|_2 > \varepsilon \right) \leq d \exp \left( -C \frac{\varepsilon^2 t}{\gamma^2} \right).$$</span></p>
<p>where <span class="LaTeX">$\lVert M \rVert_2 \leq \gamma$</span> holds almost surely and <em>C</em> > 0 is an absolute constant.</p>
<p>Notice that the number of samples in the inequality depends logarithmically on <em>d</em>. In general, unfortunately, such a dependency is inevitable: take for example a diagonal random sign matrix of dimension <em>d</em>. The operator norm of the sum of <em>t</em> independent samples is precisely the maximum deviation among <em>d</em> independent random walks of length <em>t</em>. In order to achieve a fixed bound on the maximum deviation with constant probability, it is easy to see that <em>t</em> should grow logarithmically with <em>d</em> in this scenario.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<p>The following theorem can be obtained by assuming <em>M</em> has low rank, in order to avoid the dependency on the dimensions.</p>
<h3 id="theorem-without-the-dependency-on-the-dimensions">Theorem without the dependency on the dimensions</h3>
<p>Let <span class="LaTeX">$0  and <em>M</em> be a random symmetric real matrix with <span class="LaTeX">$\| \mathrm{E}[M] \|_2 \leq 1$</span> and <span class="LaTeX">$\| M\|_2 \leq \gamma$</span> almost surely. Assume that each element on the support of <em>M</em> has at most rank <em>r</em>. Set</span></p>
<p><span class="LaTeX">$$t = \Omega \left( \frac{\gamma\log (\gamma/\varepsilon^2)}{\varepsilon^2} \right).$$</span> If <span class="LaTeX">$r \leq t$</span> holds almost surely, then</p>
<p><span class="LaTeX">$$\Pr\left(\left\| \frac{1}{t} \sum_{i=1}^t M_i - \mathrm{E}[M] \right\|_2 > \varepsilon \right) \leq \frac{1}{\mathbf{poly}(t)}$$</span></p>
<p>where <mtpl></mtpl> are i.i.d. copies of <em>M</em>.</p>
<h2 id="sampling-variant">Sampling variant</h2>
<p>The following variant of Chernoff's bound can be used to bound the probability that a majority in a population will become a minority in a sample, or vice versa.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<p>Suppose there is a general population <em>A</em> and a sub-population <em>B</em>⊆<em>A</em>. Mark the relative size of the sub-population (|<em>B</em>|/|<em>A</em>|) by <em>r</em>.</p>
<p>Suppose we pick an integer <em>k</em> and a random sample <em>S</em>⊂<em>A</em> of size <em>k</em>. Mark the relative size of the sub-population in the sample (|<em>B</em>∩<em>S</em>|/|<em>S</em>|) by <em>r<sub>S</sub></em>.</p>
<p>Then, for every fraction <em>d</em>∈[0,1]:</p>
<p><span class="LaTeX">$$\mathrm{Pr}\left(r_S < (1-d)\cdot r\right) < \exp\left(-r\cdot d^2 \cdot k/2\right)$$</span></p>
<p>In particular, if <em>B</em> is a majority in <em>A</em> (i.e. <em>r</em> > 0.5) we can bound the probability that <em>B</em> will remain minority in <em>S</em> (<em>r<sub>S</sub></em>>0.5) by taking: <em>d</em> = 1 - 1 / (2 <em>r</em>):<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<p><span class="LaTeX">$$\mathrm{Pr}\left(r_S > 0.5\right) > 1 - \exp\left(-r\cdot \left(1 - \frac{1}{2 r}\right)^2 \cdot k/2\right)$$</span></p>
<p>This bound is of course not tight at all. For example, when <em>r</em>=0.5 we get a trivial bound <em>Prob</em> > 0.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bennett's_inequality" title="wikilink">Bennett's inequality</a></li>
<li><a href="Bernstein_inequalities_(probability_theory)" title="wikilink">Bernstein inequalities (probability theory)</a></li>
<li><a href="Efron-Stein_inequality" title="wikilink">Efron-Stein inequality</a></li>
<li><a href="Hoeffding's_inequality" title="wikilink">Hoeffding's inequality</a></li>
<li><a href="Dvoretzky–Kiefer–Wolfowitz_inequality" title="wikilink">Dvoretzky–Kiefer–Wolfowitz inequality</a></li>
<li><a href="Markov's_inequality" title="wikilink">Markov's inequality</a></li>
<li><a href="Chebyshev's_inequality" title="wikilink">Chebyshev's inequality</a></li>
<li><a href="Concentration_inequality" title="wikilink">Concentration inequality</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>
<p>"</p>
<p><a href="Category:Probabilistic_inequalities" title="wikilink">Category:Probabilistic inequalities</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">Refer to the proof above<a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4">C.Alippi: "Randomized Algorithms" chapter in <em>Intelligence for Embedded Systems.</em> Springer, 2014, 283pp, ISBN 978-3-319-05278-6.<a href="#fnref4">↩</a></li>
<li id="fn5">*<a href="#fnref5">↩</a></li>
<li id="fn6">; lemma 6.1<a href="#fnref6">↩</a></li>
<li id="fn7">See graphs of: <a href="https://www.desmos.com/calculator/eqvyjug0re">the bound as a function of <em>r</em> when <em>k</em> changes</a> and <a href="https://www.desmos.com/calculator/nxurzg7bqj">the bound as a function of <em>k</em> when <em>r</em> changes</a>.<a href="#fnref7">↩</a></li>
</ol>
</section>
</body>
</html>
