<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="471">History of information theory</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>History of information theory</h1>
<hr/>

<p>The decisive event which established the discipline of <strong><a href="information_theory" title="wikilink">information theory</a></strong>, and brought it to immediate worldwide attention, was the publication of <a href="Claude_E._Shannon" title="wikilink">Claude E. Shannon</a>'s classic paper "<a href="A_Mathematical_Theory_of_Communication" title="wikilink">A Mathematical Theory of Communication</a>" in the <em><a href="Bell_System_Technical_Journal" title="wikilink">Bell System Technical Journal</a></em> in July and October 1948.</p>

<p>In this revolutionary and groundbreaking paper, the work for which Shannon had substantially completed at Bell Labs by the end of 1944, Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory, opening with the assertion that</p>
<dl>
<dd>"The fundamental problem of communication is that of reproducing at one point, either exactly or approximately, a message selected at another point."
</dd>
</dl>

<p>With it came the ideas of</p>
<ul>
<li>the <a href="information_entropy" title="wikilink">information entropy</a> and <a href="redundancy_(information_theory)" title="wikilink">redundancy</a> of a source, and its relevance through the <a href="Shannon's_source_coding_theorem" title="wikilink">source coding theorem</a>;</li>
<li>the <a href="mutual_information" title="wikilink">mutual information</a>, and the <a href="channel_capacity" title="wikilink">channel capacity</a> of a noisy channel, including the promise of perfect loss-free communication given by the <a href="noisy-channel_coding_theorem" title="wikilink">noisy-channel coding theorem</a>;</li>
<li>the practical result of the <a href="Shannon–Hartley_theorem" title="wikilink">Shannon–Hartley law</a> for the channel capacity of a Gaussian channel; and of course</li>
<li>the <a class="uri" href="bit" title="wikilink">bit</a> - a new way of seeing the most fundamental unit of information.</li>
</ul>
<h2 id="before-1948">Before 1948</h2>
<h3 id="early-telecommunications">Early telecommunications</h3>

<p>Some of the oldest methods of instant <a href="telecommunication" title="wikilink">telecommunications</a> implicitly use many of the ideas that would later be quantified in information theory. Modern <a class="uri" href="telegraphy" title="wikilink">telegraphy</a>, starting in the 1830s, used <a href="Morse_code" title="wikilink">Morse code</a>, in which <a href="Letter_frequencies" title="wikilink">more common letters</a> (like "E", which is expressed as one "dot") are transmitted more quickly than less common letters (like "J", which is expressed by one "dot" followed by three "dashes"). The idea of encoding information in this manner is the cornerstone of <a href="lossless_data_compression" title="wikilink">lossless data compression</a>. A hundred years later, <a href="frequency_modulation" title="wikilink">frequency modulation</a> illustrated that <a href="Bandwidth_(signal_processing)" title="wikilink">bandwidth</a> can be considered merely another degree of freedom. The <a class="uri" href="vocoder" title="wikilink">vocoder</a>, now largely looked at as an audio engineering curiosity, was originally designed in 1939 to use less bandwidth than that of an original message, in much the same way that <a href="mobile_phone" title="wikilink">mobile phones</a> now trade off voice quality with bandwidth.</p>
<h3 id="quantitative-ideas-of-information">Quantitative ideas of information</h3>

<p>The most direct antecedents of Shannon's work were two papers published in the 1920s by <a href="Harry_Nyquist" title="wikilink">Harry Nyquist</a> and <a href="Ralph_Hartley" title="wikilink">Ralph Hartley</a>, who were both still research leaders at Bell Labs when Shannon arrived in the early 1940s.</p>

<p>Nyquist's 1924 paper, <em>Certain Factors Affecting Telegraph Speed</em> is mostly concerned with some detailed engineering aspects of telegraph signals. But a more theoretical section discusses quantifying "intelligence" and the "line speed" at which it can be transmitted by a communication system, giving the relation</p>

<p>

<math display="block" id="History_of_information_theory:0">
 <semantics>
  <mrow>
   <mi>W</mi>
   <mo>=</mo>
   <mrow>
    <mi>K</mi>
    <mrow>
     <mi>log</mi>
     <mpadded width="+1.7pt">
      <mi>m</mi>
     </mpadded>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>W</ci>
    <apply>
     <times></times>
     <ci>K</ci>
     <apply>
      <log></log>
      <ci>m</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W=K\log m\,
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>W</em> is the speed of transmission of intelligence, <em>m</em> is the number of different voltage levels to choose from at each time step, and <em>K</em> is a constant.</p>

<p>Hartley's 1928 paper, called simply <em>Transmission of Information</em>, went further by using the word <em>information</em> (in a technical sense), and making explicitly clear that information in this context was a measurable quantity, reflecting only the receiver's ability to distinguish that one sequence of symbols had been intended by the sender rather than any other—quite regardless of any associated meaning or other psychological or semantic aspect the symbols might represent. This amount of information he quantified as</p>

<p>

<math display="block" id="History_of_information_theory:1">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mo>=</mo>
   <mrow>
    <mi>log</mi>
    <mpadded width="+1.7pt">
     <msup>
      <mi>S</mi>
      <mi>n</mi>
     </msup>
    </mpadded>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>H</ci>
    <apply>
     <log></log>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>S</ci>
      <ci>n</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H=\log S^{n}\,
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>S</em> was the number of possible symbols, and <em>n</em> the number of symbols in a transmission. The natural unit of information was therefore the decimal digit, much later renamed the <a href="Hartley_(unit)" title="wikilink">hartley</a> in his honour as a unit or scale or measure of information. The <a href="Hartley_information" title="wikilink">Hartley information</a>, <em>H</em><sub>0</sub>, is still used as a quantity for the logarithm of the total number of possibilities.</p>

<p>A similar unit of log<sub>10</sub> probability, the <em>ban</em>, and its derived unit the <a href="ban_(unit)" title="wikilink">deciban</a> (one tenth of a ban), were introduced by <a href="Alan_Turing" title="wikilink">Alan Turing</a> in 1940 as part of the statistical analysis of the breaking of the German second world war <a href="Cryptanalysis_of_the_Enigma" title="wikilink">Enigma</a> cyphers. The <em>decibannage</em> represented the reduction in (the logarithm of) the total number of possibilities (similar to the change in the Hartley information); and also the <a href="log-likelihood_ratio" title="wikilink">log-likelihood ratio</a> (or change in the <a href="weight_of_evidence" title="wikilink">weight of evidence</a>) that could be inferred for one hypothesis over another from a set of observations. The expected change in the weight of evidence is equivalent to what was later called the Kullback <a href="Kullback–Leibler_divergence#Discrimination_information" title="wikilink">discrimination information</a>.</p>

<p>But underlying this notion was still the idea of equal a-priori probabilities, rather than the information content of events of unequal probability; nor yet any underlying picture of questions regarding the communication of such varied outcomes.</p>
<h3 id="entropy-in-statistical-mechanics">Entropy in statistical mechanics</h3>

<p>One area where unequal probabilities were indeed well known was statistical mechanics, where <a href="Ludwig_Boltzmann" title="wikilink">Ludwig Boltzmann</a> had, in the context of his <a class="uri" href="H-theorem" title="wikilink">H-theorem</a> of 1872, first introduced the quantity</p>

<p>

<math display="block" id="History_of_information_theory:2">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <msub>
       <mi>f</mi>
       <mi>i</mi>
      </msub>
      <mrow>
       <mi>log</mi>
       <msub>
        <mi>f</mi>
        <mi>i</mi>
       </msub>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>H</ci>
    <apply>
     <minus></minus>
     <apply>
      <sum></sum>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>f</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <log></log>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H=-\sum f_{i}\log f_{i}
  </annotation>
 </semantics>
</math>

</p>

<p>as a measure of the breadth of the spread of states available to a single particle in a gas of like particles, where <em>f</em> represented the relative <a href="frequency_distribution" title="wikilink">frequency distribution</a> of each possible state. Boltzmann argued mathematically that the effect of collisions between the particles would cause the <em>H</em>-function to inevitably increase from any initial configuration until equilibrium was reached; and further identified it as an underlying microscopic rationale for the macroscopic <a href="Entropy_(classical_thermodynamics)" title="wikilink">thermodynamic entropy</a> of <a class="uri" href="Clausius" title="wikilink">Clausius</a>.</p>

<p>Boltzmann's definition was soon reworked by the American mathematical physicist <a href="J._Willard_Gibbs" title="wikilink">J. Willard Gibbs</a> into a general formula for statistical-mechanical entropy, no longer requiring identical and non-interacting particles, but instead based on the probability distribution <em>p<sub>i</sub></em> for the complete microstate <em>i</em> of the total system:</p>

<p>

<math display="block" id="History_of_information_theory:3">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <msub>
      <mi>k</mi>
      <mtext>B</mtext>
     </msub>
     <mrow>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <msub>
        <mi>p</mi>
        <mi>i</mi>
       </msub>
       <mrow>
        <mi>ln</mi>
        <mpadded width="+1.7pt">
         <msub>
          <mi>p</mi>
          <mi>i</mi>
         </msub>
        </mpadded>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>S</ci>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>k</ci>
       <mtext>B</mtext>
      </apply>
      <apply>
       <sum></sum>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>p</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <ln></ln>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>p</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S=-k_{\text{B}}\sum p_{i}\ln p_{i}\,
  </annotation>
 </semantics>
</math>

</p>

<p>This (Gibbs) entropy, from statistical mechanics, can be found to directly correspond to the Clausius's classical thermodynamic <a href="Entropy_(classical_thermodynamics)" title="wikilink">definition</a>.</p>

<p>Shannon himself was apparently not particularly aware of the <a href="Entropy_in_thermodynamics_and_information_theory" title="wikilink">close similarity</a> between his new measure and earlier work in thermodynamics, but <a href="John_von_Neumann" title="wikilink">John von Neumann</a> was. It is said that, when Shannon was deciding what to call his new measure and fearing the term 'information' was already over-used, von Neumann told him firmly: "You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one really knows what entropy really is, so in a debate you will always have the advantage."</p>

<p>(Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by <a href="Rolf_Landauer" title="wikilink">Rolf Landauer</a> in the 1960s, are explored further in the article <em><a href="Entropy_in_thermodynamics_and_information_theory" title="wikilink">Entropy in thermodynamics and information theory</a></em>).</p>
<h2 id="development-since-1948">Development since 1948</h2>

<p>The publication of Shannon's 1948 paper, "<a href="A_Mathematical_Theory_of_Communication" title="wikilink">A Mathematical Theory of Communication</a>", in the <em>Bell System Technical Journal</em> was the founding of information theory as we know it today. Many developments and applications of the theory have taken place since then, which have made many modern devices for data communication and storage such as <a href="CD-ROM" title="wikilink">CD-ROMs</a> and <a href="mobile_phone" title="wikilink">mobile phones</a> possible. Notable developments are listed in a <a href="timeline_of_information_theory" title="wikilink">timeline of information theory</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Timeline_of_information_theory" title="wikilink">Timeline of information theory</a></li>
<li><a href="Claude_Elwood_Shannon" title="wikilink">Shannon, CE.</a></li>
<li><a href="Ralph_Hartley" title="wikilink">Hartley, R.V.L.</a></li>
<li><a class="uri" href="H-theorem" title="wikilink">H-theorem</a></li>
</ul>

<p>"</p>

<p><a href="Category:Information_theory" title="wikilink">Category:Information theory</a></p>
</body>
</html>
