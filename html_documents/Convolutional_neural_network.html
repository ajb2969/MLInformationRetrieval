<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="465">Convolutional neural network</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Convolutional neural network</h1>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, a <strong>convolutional neural network</strong> (<strong>CNN</strong>, or <strong>ConvNet</strong>) is a type of <a href="Feedforward_neural_network" title="wikilink">feed-forward</a> <a href="artificial_neural_network" title="wikilink">artificial neural network</a> where the individual <a href="Artificial_neuron" title="wikilink">neurons</a> are tiled in such a way that they respond to overlapping regions in the <a href="visual_field" title="wikilink">visual field</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Convolutional networks were <a href="mathematical_biology" title="wikilink">inspired</a> by <a class="uri" href="biological" title="wikilink">biological</a> processes<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> and are variations of <a href="multilayer_perceptron" title="wikilink">multilayer perceptrons</a> which are designed to use minimal amounts of <a class="uri" href="preprocessing" title="wikilink">preprocessing</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> They are widely used models for image and video recognition.</p>
<h2 id="overview">Overview</h2>

<p>When used for image recognition, convolutional neural networks (CNNs) consist of multiple layers of small neuron collections which look at small portions of the input image, called <a href="receptive_fields" title="wikilink">receptive fields</a>. The results of these collections are then tiled so that they overlap to obtain a better representation of the original image; this is repeated for every such layer. Because of this, they are able to tolerate <a href="Translation_(geometry)" title="wikilink">translation</a> of the input image.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> Convolutional networks may include local or global pooling layers, which combine the outputs of neuron clusters.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> They also consist of various combinations of <a href="convolution" title="wikilink">convolutional</a> layers and fully connected layers, with <a href="pointwise_nonlinearity" title="wikilink">pointwise nonlinearity</a> applied at the end of or after each layer.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> It is inspired by biological processes. To avoid the situation that there exist billions of parameters if all layers are fully connected, the idea of using a convolution operation on small regions has been introduced. One major advantage of convolutional networks is the use of shared weight in convolutional layers, which means that the same filter (weights bank) is used for each pixel in the layer; this both reduces required memory size and improves performance.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>Some <a href="Time_delay_neural_network" title="wikilink">Time delay neural networks</a> also use a very similar architecture to convolutional neural networks, especially those for image recognition and/or <a href="image_classification" title="wikilink">classification</a> tasks, since the "tiling" of the neuron outputs can easily be carried out in timed stages in a manner useful for analysis of images.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>

<p>Compared to other image classification algorithms, convolutional neural networks use relatively little pre-processing. This means that the network is responsible for learning the filters that in traditional algorithms were hand-engineered. The lack of a dependence on prior-knowledge and the existence of difficult to design hand-engineered features is a major advantage for CNNs.</p>
<h2 id="history">History</h2>

<p>The design of convolutional neural networks follows the discovery of visual mechanisms in living organisms. In our brain, the visual cortex contains lots of cells. These cells are responsible for detecting light in small, overlapping sub-regions of the visual field, called receptive fields. These cells act as local filters over the input space. The more complex cells have larger receptive fields. A convolution operator is created to perform the same function by all of these cells.</p>

<p>The <a class="uri" href="neocognitron" title="wikilink">neocognitron</a>, a predecessor to convolutional networks,<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> was introduced in a 1980 paper by Kunihiko Fukushima.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> In 1988 they were separately developed, with explicit parallel and trainable convolutions for temporal signals, by Toshiteru Homma, Les Atlas, and Robert J. Marks II.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> Their design was later improved in 1998 by <a href="Yann_LeCun" title="wikilink">Yann LeCun</a>, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner,<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> generalized in 2003 by Sven Behnke,<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> and simplified by Patrice Simard, David Steinkraus, and John C. Platt in the same year.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> The famous LeNet-5 network can classify digits successfully, which is applied to recognize checking numbers. However, given more complex problems the breadth and depth of the network will continue to increase which would become limited by computing resources. The approach used by LeNet did not perform well with more complex problems.</p>

<p>With the rise of efficient GPU computing, it has become possible to train larger networks. In 2006 several publications described more efficient ways to train convolutional neural networks with more layers.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a><a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a><a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> In 2011, they were refined by Dan Ciresan et al. and were implemented on a GPU with impressive performance results.<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> In 2012, Dan Ciresan et al. significantly improved upon the best performance in the literature for multiple image <a href="database" title="wikilink">databases</a>, including the <a href="MNIST_database" title="wikilink">MNIST database</a>, the NORB database, the HWDB1.0 dataset (Chinese characters), the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images),<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> and the ImageNet dataset.<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a></p>
<h2 id="details">Details</h2>
<h3 id="backpropagation">Backpropagation</h3>

<p>When doing propagation, the momentum and weight decay values are chosen to reduce oscillation during <a href="stochastic_gradient_descent" title="wikilink">stochastic gradient descent</a>. See <a class="uri" href="Backpropagation" title="wikilink">Backpropagation</a> for more.</p>
<h3 id="different-types-of-layers">Different types of layers</h3>
<h4 id="convolutional-layer">Convolutional layer</h4>

<p>Unlike a hand-coded convolution <a href="Kernel_(image_processing)" title="wikilink">kernel</a> (Sobel, Prewitt, Roberts), in a convolutional neural net, the parameters of each convolution kernel are trained by the backpropagation algorithm. There are many convolution kernels in each layer, and each kernel is replicated over the entire image with the same parameters. The function of the convolution operators is to extract different features of the input. The capacity of a neural net varies, depending on the number of layers. The first convolution layers will obtain the low-level features, like edges, lines and corners. The more layers the network has, the higher-level features it will get.</p>
<h4 id="relu-layer">ReLU layer</h4>

<p>ReLU is the abbreviation of <a href="Rectifier_(neural_networks)" title="wikilink">Rectified Linear Units</a>. This is a layer of neurons that use the non-saturating <a href="activation_function" title="wikilink">activation function</a> 

<math display="inline" id="Convolutional_neural_network:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>m</mi>
    <mi>a</mi>
    <mi>x</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mn>0</mn>
     <mo>,</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>a</ci>
     <ci>x</ci>
     <interval closure="open">
      <cn type="integer">0</cn>
      <ci>x</ci>
     </interval>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=max(0,x)
  </annotation>
 </semantics>
</math>

. It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.</p>

<p>Other functions are used to increase nonlinearity. For example the saturating <a href="hyperbolic_tangent" title="wikilink">hyperbolic tangent</a> 

<math display="inline" id="Convolutional_neural_network:1">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>t</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>h</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <ci>t</ci>
     <ci>a</ci>
     <ci>n</ci>
     <ci>h</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=tanh(x)
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Convolutional_neural_network:2">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">|</mo>
    <mrow>
     <mi>t</mi>
     <mi>a</mi>
     <mi>n</mi>
     <mi>h</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo stretchy="false">|</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <abs></abs>
     <apply>
      <times></times>
      <ci>t</ci>
      <ci>a</ci>
      <ci>n</ci>
      <ci>h</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=|tanh(x)|
  </annotation>
 </semantics>
</math>

, and the <a href="sigmoid_function" title="wikilink">sigmoid function</a> 

<math display="inline" id="Convolutional_neural_network:3">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <msup>
       <mi>e</mi>
       <mrow>
        <mo>-</mo>
        <mi>x</mi>
       </mrow>
      </msup>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <plus></plus>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <apply>
        <minus></minus>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=(1+e^{-x})^{-1}
  </annotation>
 </semantics>
</math>

. Compared to tanh units, the advantage of ReLU is that the neural network trains several times faster.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a></p>
<h4 id="pooling-layer">Pooling layer</h4>

<p>In order to reduce variance, pooling layers compute the max or average value of a particular feature over a region of the image. This will ensure that the same result will be obtained, even when image features have small translations. This is an important operation for object classification and detection.</p>
<h4 id="dropout-method">Dropout method</h4>

<p>Since a fully connected layer occupies most of the parameters, it is prone to <a class="uri" href="overfitting" title="wikilink">overfitting</a>. The dropout method <a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> is introduced to prevent overfitting. At each training stage, individual nodes are either "dropped out" of the net with probability 1-p or kept with probability p, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.</p>

<p>In the training stages, the probability a hidden node will be retained (i.e. not dropped) is usually 0.5; for input nodes the retention probability should be much higher, intuitively because information is directly lost when input nodes are ignored.</p>

<p>At testing time after training has finished, we would ideally like to find a sample average of all possible 2^n dropped-out networks; unfortunately this is infeasable for large n. However, we can find an approximation by using the full network with each node's output weighted by a factor of p, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates 2^n neural nets, and as such allows for model combination, at test time only a single network needs to be tested.</p>

<p>By avoiding training all nodes on all training data, dropout decreases overfitting in neural nets. The method also significantly improves the speed of training. This makes model combination practical, even for deep neural nets.</p>
<h4 id="loss-layer">Loss layer</h4>

<p>It can use different loss functions for different tasks. Softmax loss is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in [0,1]. Euclidean loss is used for regressing to real-valued labels [-inf,inf]</p>
<h2 id="applications">Applications</h2>
<h3 id="image-recognition">Image recognition</h3>

<p>Convolutional neural networks are often used in image recognition systems. They have achieved an <a href="Per-comparison_error_rate" title="wikilink">error rate</a> of 0.23 percent on the MNIST database, which as of February 2012 is the lowest achieved on the database.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a> Another paper on using CNN for image classification reported that the learning process was "surprisingly fast"; in the same paper, the best published results at the time were achieved in the MNIST database and the NORB database.<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a></p>

<p>When applied to <a href="facial_recognition_system" title="wikilink">facial recognition</a>, they were able to contribute to a large decrease in error rate.<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> In another paper, they were able to achieve a 97.6 percent recognition rate on "5,600 still images of more than 10 subjects".<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a> CNNs have been used to assess <a href="video_quality" title="wikilink">video quality</a> in an objective way after being manually trained; the resulting system had a very low <a href="root_mean_square_error" title="wikilink">root mean square error</a>.<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a></p>

<p>The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, which is large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet<a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a> (which <a class="uri" href="DeepDream" title="wikilink">DeepDream</a> is based on) increased the mean average <a href="Precision_and_recall" title="wikilink">precision</a> of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. Performance of convolutional neural networks, on the ImageNet tests, is now close to that of humans.<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a> The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.</p>

<p>In 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded with competitive performance. The network trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.<a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a></p>
<h3 id="video-analysis">Video analysis</h3>

<p>Video is more complex than images since it has another (temporal) dimension. The common way is to fuse the features of different convolutional neural networks, which are responsible for spatial and temporal stream.<a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a><a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a></p>
<h3 id="natural-language-processing">Natural Language Processing</h3>

<p>Convolutional neural networks have also seen use in the field of <a href="natural_language_processing" title="wikilink">natural language processing</a> or NLP. Like the image classification problem, some NLP tasks can be formulated as assigning labels to words in a sentence. The neural network trained raw material fashion will extract the features of the sentences. Using some classifiers, it could predict new sentences.<a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a></p>
<h3 id="playing-go">Playing Go</h3>

<p>Convolutional neural networks have been used in <a href="computer_Go" title="wikilink">computer Go</a>. In December 2014, Christopher Clark and <a href="Amos_Storkey" title="wikilink">Amos Storkey</a> published a paper showing a convolutional network trained by supervised learning from a database of human professional games could outperform Gnu Go and win some games against <a href="Monte_Carlo_tree_search" title="wikilink">Monte Carlo tree search</a> Fuego 1.1 in a fraction of the time it took Fuego to play.<a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a> Shortly after it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a <a href="Go_ranks_and_ratings" title="wikilink">6 dan</a> human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program <a href="GNU_Go" title="wikilink">GNU Go</a> in 97% of games, and matched the performance of the <a href="Monte_Carlo_tree_search" title="wikilink">Monte Carlo tree search</a> program Fuego simulating ten thousand playouts (about a million positions) per move.<a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a></p>
<h2 id="fine-tuning">Fine-tuning</h2>

<p>For many applications, only a small amount of training data is available. Convolutional neural networks usually require a large amount of training data in order to avoid <a class="uri" href="overfitting" title="wikilink">overfitting</a>. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets.</p>
<h2 id="common-libraries">Common libraries</h2>
<ul>
<li>Caffe: Caffe (replacement of Decaf) has been the most popular library for convolutional neural networks. It is created by the Berkeley Vision and Learning Center (BVLC). The advantages are that it has cleaner architecture and faster speed. It supports both CPU and GPU, easily switching between them. It is developed in C++, and has Python and MATLAB wrappers. In the developing of Caffe, protobuf is used to make researchers tune the parameters easily as well as adding or removing layers.</li>
<li><a href="Torch_(machine_learning)" title="wikilink">Torch</a> (www.torch.ch): A scientific computing framework with wide support for machine learning algorithms, written in C and lua. The main author is Ronan Collobert, and it is now widely used at Facebook AI Research, Google DeepMind and Twitter, among others.</li>
<li>OverFeat: A pre-trained feature extractor by Pierre Sermanet.</li>
<li>Cuda-convnet: A convnet implementation in CUDA</li>
<li>MatConvnet</li>
<li><a href="Theano_(software)" title="wikilink">Theano</a>: written in Python with an API largely compatible with the popular Numpy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.</li>
<li><a class="uri" href="Deeplearning4j" title="wikilink">Deeplearning4j</a>: Deep learning in Java and Scala on GPU-enabled Spark</li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a class="uri" href="Neocognitron" title="wikilink">Neocognitron</a></li>
<li><a class="uri" href="Convolution" title="wikilink">Convolution</a></li>
<li><a href="Deep_learning" title="wikilink">Deep learning</a></li>
<li><a href="Time_delay_neural_network" title="wikilink">Time delay neural network</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://yann.lecun.com/exdb/lenet/">A demonstration of a convolutional network created for character recognition</a></li>
<li><a href="http://caffe.berkeleyvision.org/">Caffe</a></li>
<li><a href="http://www.mathworks.com/matlabcentral/fileexchange/24291-cnn-convolutional-neural-network-class">Matlab toolbox</a></li>
<li><a href="http://www.vlfeat.org/matconvnet/">MatConvnet</a></li>
<li><a href="http://deeplearning.net/software/theano">Theano</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial">UFLDL Tutorial</a></li>
<li><a href="http://deeplearning4j.org/convolutionalnets.html">Deeplearning4j's Convolutional Nets</a></li>
</ul>

<p>"</p>

<p><a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a> <a href="Category:Computational_neuroscience" title="wikilink">Category:Computational neuroscience</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">â©</a></li>
<li id="fn2"><a href="#fnref2">â©</a></li>
<li id="fn3"><a href="#fnref3">â©</a></li>
<li id="fn4"><a href="#fnref4">â©</a></li>
<li id="fn5"></li>
<li id="fn6"><a href="#fnref6">â©</a></li>
<li id="fn7"><a href="#fnref7">â©</a></li>
<li id="fn8"></li>
<li id="fn9"><a href="#fnref9">â©</a></li>
<li id="fn10"><a href="#fnref10">â©</a></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">â©</a></li>
<li id="fn13"><a href="#fnref13">â©</a></li>
<li id="fn14"><a href="#fnref14">â©</a></li>
<li id="fn15">S. Behnke. Hierarchical Neural Networks for Image Interpretation, volume 2766 of Lecture Notes in Computer Science. Springer, 2003.<a href="#fnref15">â©</a></li>
<li id="fn16">Simard, Patrice, David Steinkraus, and John C. Platt. "Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis." In ICDAR, vol. 3, pp. 958-962. 2003.<a href="#fnref16">â©</a></li>
<li id="fn17"><a href="#fnref17">â©</a></li>
<li id="fn18"><a href="#fnref18">â©</a></li>
<li id="fn19"><a href="#fnref19">â©</a></li>
<li id="fn20"><a href="#fnref20">â©</a></li>
<li id="fn21"></li>
<li id="fn22">10. Deng, Jia, et al. "Imagenet: A large-scale hierarchical image database."Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009.<a href="#fnref22">â©</a></li>
<li id="fn23"><a href="#fnref23">â©</a></li>
<li id="fn24"><a href="#fnref24">â©</a></li>
<li id="fn25"></li>
<li id="fn26"></li>
<li id="fn27"><a href="#fnref27">â©</a></li>
<li id="fn28"></li>
<li id="fn29"></li>
<li id="fn30"><a href="#fnref30">â©</a></li>
<li id="fn31">O. Russakovsky et al., "<a href="http://arxiv.org/abs/1409.0575">ImageNet Large Scale Visual Recognition Challenge</a>", 2014.<a href="#fnref31">â©</a></li>
<li id="fn32"><a href="#fnref32">â©</a></li>
<li id="fn33">Karpathy, Andrej, et al. "Large-scale video classification with convolutional neural networks." IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2014.<a href="#fnref33">â©</a></li>
<li id="fn34">Simonyan, Karen, and Andrew Zisserman. "Two-stream convolutional networks for action recognition in videos." arXiv preprint arXiv:1406.2199 (2014).<a href="#fnref34">â©</a></li>
<li id="fn35">Collobert, Ronan, and Jason Weston. "A unified architecture for natural language processing: Deep neural networks with multitask learning."Proceedings of the 25th international conference on Machine learning. ACM, 2008.<a href="#fnref35">â©</a></li>
<li id="fn36">Clark, C., Storkey A.J. (2014), "<a href="http://arxiv.org/abs/1412.3409">Teaching Deep Convolutional Networks to play Go</a>".<a href="#fnref36">â©</a></li>
<li id="fn37">Maddison C.J., Huang A., Sutskever I., Silver D. (2014), "<a href="http://arxiv.org/abs/1412.6564">Move evaluation in Go using deep convolutional neural networks</a>".<a href="#fnref37">â©</a></li>
</ol>
</section>
</body>
</html>
