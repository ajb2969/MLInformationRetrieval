<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="3">Mallows's Cp</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Mallows's Cp</h1>
<hr/>

<p><mtpl></mtpl> In <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>Mallows's <em>C<sub>p</sub></em></strong>,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> named for <a href="Colin_Lingwood_Mallows" title="wikilink">Colin Lingwood Mallows</a>, is used to assess the <a href="goodness_of_fit" title="wikilink">fit</a> of a <a href="regression_analysis" title="wikilink">regression model</a> that has been estimated using <a href="ordinary_least_squares" title="wikilink">ordinary least squares</a>. It is applied in the context of <a href="model_selection" title="wikilink">model selection</a>, where a number of <a href="dependent_and_independent_variables" title="wikilink">predictor variables</a> are available for predicting some outcome, and the goal is to find the best model involving a subset of these predictors.</p>

<p>Mallows's <em>C<sub>p</sub></em> has been shown to be equivalent to <a href="Akaike_information_criterion" title="wikilink">Akaike information criterion</a> in the special case of Gaussian <a href="linear_regression" title="wikilink">linear regression</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="definition-and-properties">Definition and properties</h2>

<p>Mallows's <em>C<sub>p</sub></em> addresses the issue of <a class="uri" href="overfitting" title="wikilink">overfitting</a>, in which model selection statistics such as the residual sum of squares always get smaller as more variables are added to a model. Thus, if we aim to select the model giving the smallest residual sum of squares, the model including all variables would always be selected. The <em>C<sub>p</sub></em> statistic calculated on a <a href="sample_(statistics)" title="wikilink">sample</a> of data estimates the <a href="mean_squared_prediction_error" title="wikilink">mean squared prediction error</a> (MSPE) as its <a href="statistical_population" title="wikilink">population</a> target</p>

<p>

<math display="block" id="Mallows's_Cp:0">
 <semantics>
  <mrow>
   <mi>E</mi>
   <munder>
    <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
    <mi>j</mi>
   </munder>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mover accent="true">
       <mi>Y</mi>
       <mo stretchy="false">^</mo>
      </mover>
      <mi>j</mi>
     </msub>
     <mo>-</mo>
     <mi>E</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>Y</mi>
       <mi>j</mi>
      </msub>
      <mo>∣</mo>
      <msub>
       <mi>X</mi>
       <mi>j</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mn>2</mn>
   </msup>
   <mo>/</mo>
   <msup>
    <mi>σ</mi>
    <mn>2</mn>
   </msup>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">E</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <ci>j</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-^</ci>
        <ci>Y</ci>
       </apply>
       <ci>j</ci>
      </apply>
      <minus></minus>
      <csymbol cd="unknown">E</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Y</ci>
        <ci>j</ci>
       </apply>
       <ci>normal-∣</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>X</ci>
        <ci>j</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
      <ci>normal-)</ci>
     </cerror>
     <cn type="integer">2</cn>
    </apply>
    <divide></divide>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>σ</ci>
     <cn type="integer">2</cn>
    </apply>
    <ci>normal-,</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   E\sum_{j}(\hat{Y}_{j}-E(Y_{j}\mid X_{j}))^{2}/\sigma^{2},
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Mallows's_Cp:1">
 <semantics>
  <msub>
   <mover accent="true">
    <mi>Y</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-^</ci>
     <ci>Y</ci>
    </apply>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{Y}_{j}
  </annotation>
 </semantics>
</math>

 is the fitted value from the regression model for the <em>j</em>th case, <em>E</em>(<em>Y</em><sub><em>j</em></sub> | <em>X</em><sub><em>j</em></sub>) is the expected value for the <em>j</em>th case, and σ<sup>2</sup> is the error variance (assumed constant across the cases). The MSPE will not automatically get smaller as more variables are added. The optimum model under this criterion is a compromise influenced by the sample size, the <a href="effect_size" title="wikilink">effect sizes</a> of the different predictors, and the degree of <a class="uri" href="collinearity" title="wikilink">collinearity</a> between them.</p>

<p>If <em>P</em> <a href="regressor" title="wikilink">regressors</a> are selected from a set of <em>K</em> &gt; <em>P</em>, the <em>C<sub>p</sub></em> statistic for that particular set of regressors is defined as:</p>

<p>

<math display="block" id="Mallows's_Cp:2">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>C</mi>
     <mi>p</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mfrac>
       <mrow>
        <mi>S</mi>
        <mi>S</mi>
        <msub>
         <mi>E</mi>
         <mi>p</mi>
        </msub>
       </mrow>
       <msup>
        <mi>S</mi>
        <mn>2</mn>
       </msup>
      </mfrac>
      <mo>-</mo>
      <mi>N</mi>
     </mrow>
     <mo>+</mo>
     <mrow>
      <mn>2</mn>
      <mi>P</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>C</ci>
     <ci>p</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <minus></minus>
      <apply>
       <divide></divide>
       <apply>
        <times></times>
        <ci>S</ci>
        <ci>S</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>E</ci>
         <ci>p</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>S</ci>
        <cn type="integer">2</cn>
       </apply>
      </apply>
      <ci>N</ci>
     </apply>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <ci>P</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   C_{p}={SSE_{p}\over S^{2}}-N+2P,
  </annotation>
 </semantics>
</math>

</p>

<p>where</p>
<ul>
<li>

<math display="inline" id="Mallows's_Cp:3">
 <semantics>
  <mrow>
   <mrow>
    <mi>S</mi>
    <mi>S</mi>
    <msub>
     <mi>E</mi>
     <mi>p</mi>
    </msub>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>N</mi>
    </msubsup>
    <msup>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msub>
        <mi>Y</mi>
        <mi>i</mi>
       </msub>
       <mo>-</mo>
       <msub>
        <mi>Y</mi>
        <mrow>
         <mi>p</mi>
         <mi>i</mi>
        </mrow>
       </msub>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mn>2</mn>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>S</ci>
     <ci>S</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>E</ci>
      <ci>p</ci>
     </apply>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>N</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <minus></minus>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Y</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Y</ci>
        <apply>
         <times></times>
         <ci>p</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   SSE_{p}=\sum_{i=1}^{N}(Y_{i}-Y_{pi})^{2}
  </annotation>
 </semantics>
</math>

 is the error <a href="sum_of_squares" title="wikilink">sum of squares</a> for the model with <em>P</em> <a href="regressor" title="wikilink">regressors</a>,</li>
<li><em>Y</em><sub>pi</sub> is the <a href="predict" title="wikilink">predicted</a> value of the <em>i</em>th observation of <em>Y</em> from the <em>P</em> <a href="regressor" title="wikilink">regressors</a>,</li>
<li><em>S</em><sup>2</sup> is the residual mean square after <a href="Regression_analysis" title="wikilink">regression</a> on the complete set of <em>K</em> <a href="regressor" title="wikilink">regressors</a> and can be estimated by <a href="mean_square_error" title="wikilink">mean square error</a> <em>MSE</em>,</li>
<li>and <em>N</em> is the <a href="sample_size" title="wikilink">sample size</a>.</li>
</ul>
<h2 id="limitations-of-cp">Limitations of <em>C<sub>p</sub></em></h2>

<p>The <em>C<sub>p</sub></em> criterion suffers from two main limitations<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<ol>
<li>the <em>C<sub>p</sub></em> approximation is only valid for large sample size;</li>
<li>the <em>C<sub>p</sub></em> cannot handle complex collections of models as in the variable selection (or <a href="feature_selection" title="wikilink">feature selection</a>) problem.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></li>
</ol>
<h2 id="practical-use">Practical use</h2>

<p>The <em>C<sub>p</sub></em> statistic is often used as a stopping rule for various forms of <a href="stepwise_regression" title="wikilink">stepwise regression</a>. Mallows proposed the statistic as a criterion for selecting among many alternative subset regressions. Under a model not suffering from appreciable lack of fit (bias), <em>C<sub>p</sub></em> has expectation nearly equal to <em>P</em>; otherwise the expectation is roughly <em>P</em> plus a positive bias term. Nevertheless, even though it has expectation greater than or equal to <em>P</em>, there is nothing to prevent <em>C<sub>p</sub></em> p'' p'' approaching <em>P</em>,<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> from above, for a list of subsets ordered by increasing <em>P</em>. In practice, the positive bias can be adjusted for by selecting a model from the ordered list of subsets, such that <em>C<sub>p</sub></em> p'' statistic is an estimate of the MSPE, using <em>C<sub>p</sub></em> for model selection does not completely guard against overfitting. For instance, it is possible that the selected model will be one in which the sample <em>C<sub>p</sub></em> was a particularly severe underestimate of the MSPE.</p>

<p>Model selection statistics such as <em>C<sub>p</sub></em> are generally not used blindly, but rather information about the field of application, the intended use of the model, and any known biases in the data are taken into account in the process of model selection.</p>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Regression_analysis" title="wikilink">Category:Regression analysis</a> <a href="Category:Regression_variable_selection" title="wikilink">Category:Regression variable selection</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><mtpl></mtpl><a href="#fnref1">↩</a></li>
<li id="fn2"><mtpl></mtpl><a href="#fnref2">↩</a></li>
<li id="fn3"><mtpl></mtpl><a href="#fnref3">↩</a></li>
<li id="fn4">Giraud, C. (2015), <em>Introdution to high-dimensional statistics</em>, Chapman &amp; Hall/CRC, ISBN 9781482237948<a href="#fnref4">↩</a></li>
<li id="fn5"></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
</ol>
</section>
</body>
</html>
