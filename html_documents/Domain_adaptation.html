<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1623">Domain adaptation</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Domain adaptation</h1>
<hr/>

<p><strong>Domain Adaptation</strong><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> is a field of associated with <a href="machine_learning" title="wikilink">machine learning</a> and <a href="inductive_transfer" title="wikilink">transfer learning</a>. This scenario arises when we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. For instance, one of the tasks of the common <a href="Anti-spam_techniques" title="wikilink">spam filtering problem</a> consists in adapting a model from one user (the source distribution) to a new one who receives significantly different emails (the target distribution). Note that, when more than one source distribution is available we talked about multi-source domain adaptation.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="formalization">Formalization</h2>

<p>Let 

<math display="inline" id="Domain_adaptation:0">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 be the input space (or description space) and let 

<math display="inline" id="Domain_adaptation:1">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

 be the output space (or label space). The objective of a machine learning algorithm is to learn a mathematical model (a hypothesis) 

<math display="inline" id="Domain_adaptation:2">
 <semantics>
  <mrow>
   <mi>h</mi>
   <mo>:</mo>
   <mrow>
    <mi>X</mi>
    <mo>→</mo>
    <mi>Y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>h</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h:X\to Y
  </annotation>
 </semantics>
</math>

 able to affect a label of 

<math display="inline" id="Domain_adaptation:3">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

 to an example from 

<math display="inline" id="Domain_adaptation:4">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

. This model is learned from a learning sample 

<math display="inline" id="Domain_adaptation:5">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mo>=</mo>
   <msubsup>
    <mrow>
     <mo stretchy="false">{</mo>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>y</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo stretchy="false">}</mo>
    </mrow>
    <mrow>
     <mi>i</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>m</mi>
   </msubsup>
   <mo>∈</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>X</mi>
      <mo>×</mo>
      <mi>Y</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>m</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>S</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <set>
        <interval closure="open">
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>y</ci>
          <ci>i</ci>
         </apply>
        </interval>
       </set>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>m</ci>
     </apply>
    </apply>
    <apply>
     <in></in>
     <share href="#.cmml">
     </share>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <times></times>
       <ci>X</ci>
       <ci>Y</ci>
      </apply>
      <ci>m</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S=\{(x_{i},y_{i})\}_{i=1}^{m}\in(X\times Y)^{m}
  </annotation>
 </semantics>
</math>

.</p>

<p>Usually in <a href="supervised_learning" title="wikilink">supervised learning</a> (without domain adaptation), we suppose that the examples 

<math display="inline" id="Domain_adaptation:6">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mi>i</mi>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mi>i</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>∈</mo>
   <mi>S</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>i</ci>
     </apply>
    </interval>
    <ci>S</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{i},y_{i})\in S
  </annotation>
 </semantics>
</math>

 are drawn i.i.d. from a distribution 

<math display="inline" id="Domain_adaptation:7">
 <semantics>
  <msub>
   <mi>D</mi>
   <mi>S</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>D</ci>
    <ci>S</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{S}
  </annotation>
 </semantics>
</math>

 of support 

<math display="inline" id="Domain_adaptation:8">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>×</mo>
   <mi>Y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>X</ci>
    <ci>Y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X\times Y
  </annotation>
 </semantics>
</math>

 (unknown and fixed). The objective is then to learn 

<math display="inline" id="Domain_adaptation:9">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

 (from 

<math display="inline" id="Domain_adaptation:10">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

) such that it commits the less error as possible for labeling new examples coming from the distribution 

<math display="inline" id="Domain_adaptation:11">
 <semantics>
  <msub>
   <mi>D</mi>
   <mi>S</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>D</ci>
    <ci>S</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{S}
  </annotation>
 </semantics>
</math>

.</p>

<p>The main difference between supervised learning and domain adaptation is that in the latter situation we study two different (but related) distributions 

<math display="inline" id="Domain_adaptation:12">
 <semantics>
  <msub>
   <mi>D</mi>
   <mi>S</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>D</ci>
    <ci>S</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{S}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Domain_adaptation:13">
 <semantics>
  <msub>
   <mi>D</mi>
   <mi>T</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>D</ci>
    <ci>T</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{T}
  </annotation>
 </semantics>
</math>

 on 

<math display="inline" id="Domain_adaptation:14">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>×</mo>
   <mi>Y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>X</ci>
    <ci>Y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X\times Y
  </annotation>
 </semantics>
</math>

. The domain adaptation task consists then to transfer knowledge from the source domain 

<math display="inline" id="Domain_adaptation:15">
 <semantics>
  <msub>
   <mi>D</mi>
   <mi>S</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>D</ci>
    <ci>S</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{S}
  </annotation>
 </semantics>
</math>

to the target one 

<math display="inline" id="Domain_adaptation:16">
 <semantics>
  <msub>
   <mi>D</mi>
   <mi>T</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>D</ci>
    <ci>T</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{T}
  </annotation>
 </semantics>
</math>

. The goal is then to learn 

<math display="inline" id="Domain_adaptation:17">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

 (from labeled or unlabeled samples coming from the two domains) such that it commits the less error as possible on the target domain 

<math display="inline" id="Domain_adaptation:18">
 <semantics>
  <msub>
   <mi>D</mi>
   <mi>T</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>D</ci>
    <ci>T</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{T}
  </annotation>
 </semantics>
</math>

.</p>

<p>The major issue is the following: if a model is learned from a source domain, what is its capacity to correctly label data coming from the target domain?</p>
<h2 id="the-different-types-of-domain-adaptation">The different types of domain adaptation</h2>

<p>There are several contexts of domain adaptation. They differ in the informations considered for the target task.</p>
<ol>
<li>The <strong>unsupervised domain adaptation</strong>: the learning sample contains a set of labeled source examples, a set of unlabeled source examples and an unlabeled set of target examples.</li>
<li>The <strong>semi-supervised domain adaptation</strong>: in this situation, we also consider a "small" set of labeled target examples.</li>
<li>The <strong>supervised domain adaptation</strong>: all the examples considered are supposed to be labeled.</li>
</ol>
<h2 id="three-algorithmic-principles">Three algorithmic principles</h2>
<h3 id="reweighting-algorithms">Reweighting algorithms</h3>

<p>The objective is to reweight the source labeled sample such that it "looks like" the target sample (in term of the error measure considered)<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h3 id="iterative-algorithms">Iterative algorithms</h3>

<p>A method for adapting consists in iteratively "auto-labeled" the target examples. The principle is simple:</p>
<ol>
<li>a model 

<math display="inline" id="Domain_adaptation:19">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

 is learned from the labeled examples;</li>
<li>

<math display="inline" id="Domain_adaptation:20">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

 automatically labels some target examples;</li>
<li>a new model is learned from the new labeled examples.</li>
</ol>

<p>Note that there exists other iterative approaches, but they usually need target labeled examples.</p>
<h3 id="search-of-a-common-representation-space">Search of a common representation space</h3>

<p>The goal is to find or construct a common representation space for the two domains. The objective is to obtain a space in which the domains are close to each other while keeping good performances on the source labeling task.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
</ol>
</section>
</body>
</html>
