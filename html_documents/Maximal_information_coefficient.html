<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1733">Maximal information coefficient</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Maximal information coefficient</h1>
<hr/>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, the <strong>maximal information coefficient (MIC)</strong> is a measure of the strength of the linear or non-linear association between two variables <em>X</em> and <em>Y</em>.</p>

<p>The MIC belongs to the maximal information-based nonparametric exploration (MINE) class of statistics.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> In a simulation study, MIC outperformed some selected low power tests,<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> however concerns have been raised regarding reduced <a href="statistical_power" title="wikilink">statistical power</a> in detecting some associations in settings with low sample size when compared to powerful methods such as <a href="distance_correlation" title="wikilink">distance correlation</a> and HHG.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Comparisons with these methods, in which MIC was outperformed, were made in <a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> and.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> It is claimed<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> that MIC approximately satisfies a property called <em>equitability</em> which is illustrated by selected simulation studies.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> It was later proved that no non-trivial coefficient can exactly satisfy the <em>equitability</em> property as defined by Reshef et al.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> Some criticisms of MIC are addressed by Reshef et al. in further studies published on arXiv.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h2 id="overview">Overview</h2>

<p>The maximal information coefficient uses <a href="Data_binning" title="wikilink">binning</a> as a means to apply <a href="Mutual_Information" title="wikilink">mutual information</a> on continuous random variables. Binning has been used for some time as a way of applying mutual information to continuous distributions; what MIC contributes in addition is a methodology for selecting the number of bins and picking a maximum over many possible grids.</p>

<p>The rationale is that the bins for both variables should be chosen in such a way that the mutual information between the variables be maximal. That is achieved whenever 

<math display="inline" id="Maximal_information_coefficient:0">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">H</mi>
    <mrow>
     <mo>(</mo>
     <msub>
      <mi>X</mi>
      <mi>b</mi>
     </msub>
     <mo>)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">H</mi>
    <mrow>
     <mo>(</mo>
     <msub>
      <mi>Y</mi>
      <mi>b</mi>
     </msub>
     <mo>)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">H</mi>
    <mrow>
     <mo>(</mo>
     <msub>
      <mi>X</mi>
      <mi>b</mi>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>Y</mi>
      <mi>b</mi>
     </msub>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>normal-H</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>X</ci>
       <ci>b</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>normal-H</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Y</ci>
       <ci>b</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>normal-H</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>X</ci>
        <ci>b</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Y</ci>
        <ci>b</ci>
       </apply>
      </interval>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{H}\left(X_{b}\right)=\mathrm{H}\left(Y_{b}\right)=\mathrm{H}\left(X_{b%
},Y_{b}\right)
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> Thus, when the mutual information is maximal over a binning of the data, we should expect that the following two properties hold, as much as made possible by the own nature of the data. First, the bins would have roughly the same size, because the entropies 

<math display="inline" id="Maximal_information_coefficient:1">
 <semantics>
  <mrow>
   <mi mathvariant="normal">H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>X</mi>
     <mi>b</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-H</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>X</ci>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{H}(X_{b})
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Maximal_information_coefficient:2">
 <semantics>
  <mrow>
   <mi mathvariant="normal">H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>Y</mi>
     <mi>b</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-H</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Y</ci>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{H}(Y_{b})
  </annotation>
 </semantics>
</math>

 are maximized by equal-sized binning. And second, each bin of <em>X</em> will roughly correspond to a bin in <em>Y</em>.</p>

<p>Because the variables X and Y are reals, it is almost always possible to create exactly one bin for each (<em>x</em>,<em>y</em>) datapoint, and that would yield a very high value of the MI. To avoid forming this kind of trivial partitioning, the authors of the paper propose taking a number of bins 

<math display="inline" id="Maximal_information_coefficient:3">
 <semantics>
  <msub>
   <mi>n</mi>
   <mi>x</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>n</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{x}
  </annotation>
 </semantics>
</math>

 for <em>X</em> and 

<math display="inline" id="Maximal_information_coefficient:4">
 <semantics>
  <msub>
   <mi>n</mi>
   <mi>y</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>n</ci>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{y}
  </annotation>
 </semantics>
</math>

 whose product is relatively small compared with the size N of the data sample. Concretely, they propose:</p>

<p>

<math display="inline" id="Maximal_information_coefficient:5">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>n</mi>
     <mi>x</mi>
    </msub>
    <mo>×</mo>
    <msub>
     <mi>n</mi>
     <mi>y</mi>
    </msub>
   </mrow>
   <mo>≤</mo>
   <msup>
    <mi mathvariant="normal">N</mi>
    <mn>0.6</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <ci>x</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <ci>y</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>normal-N</ci>
     <cn type="float">0.6</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{x}\times n_{y}\leq\mathrm{N}^{0.6}
  </annotation>
 </semantics>
</math>

</p>

<p>In some cases it is possible to achieve a good correspondence between 

<math display="inline" id="Maximal_information_coefficient:6">
 <semantics>
  <msub>
   <mi>X</mi>
   <mi>b</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>X</ci>
    <ci>b</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X_{b}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Maximal_information_coefficient:7">
 <semantics>
  <msub>
   <mi>Y</mi>
   <mi>b</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Y</ci>
    <ci>b</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y_{b}
  </annotation>
 </semantics>
</math>

 with numbers as low as 

<math display="inline" id="Maximal_information_coefficient:8">
 <semantics>
  <mrow>
   <msub>
    <mi>n</mi>
    <mi>x</mi>
   </msub>
   <mo>=</mo>
   <mn>2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>n</ci>
     <ci>x</ci>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{x}=2
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Maximal_information_coefficient:9">
 <semantics>
  <mrow>
   <msub>
    <mi>n</mi>
    <mi>y</mi>
   </msub>
   <mo>=</mo>
   <mn>2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>n</ci>
     <ci>y</ci>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{y}=2
  </annotation>
 </semantics>
</math>

, while in other cases the number of bins required may be higher. The maximum for 

<math display="inline" id="Maximal_information_coefficient:10">
 <semantics>
  <mrow>
   <mi mathvariant="normal">I</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>X</mi>
     <mi>b</mi>
    </msub>
    <mo>;</mo>
    <msub>
     <mi>Y</mi>
     <mi>b</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-I</ci>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>X</ci>
      <ci>b</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>b</ci>
     </apply>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{I}(X_{b};Y_{b})
  </annotation>
 </semantics>
</math>

 is determined by H(X), which is in turn determined by the number of bins in each axis, therefore, the mutual information value will be dependent on the number of bins selected for each variable. In order to compare mutual information values obtained with partitions of different sizes, the mutual information value is normalized by dividing by the maximum achieveable value for the given partition size. Entropy is maximized by uniform probability distributions, or in this case, bins with the same number of elements. Also, joint entropy is minimized by having a one-to-one correspondence between bins. If we substitute such values in the formula 

<math display="inline" id="Maximal_information_coefficient:11">
 <semantics>
  <mrow>
   <mrow>
    <mi>I</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>X</mi>
     <mo>;</mo>
     <mi>Y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mrow>
      <mi>H</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>X</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>+</mo>
     <mrow>
      <mi>H</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>Y</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mi>H</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>X</mi>
      <mo>,</mo>
      <mi>Y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>I</ci>
     <list>
      <ci>X</ci>
      <ci>Y</ci>
     </list>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <plus></plus>
      <apply>
       <times></times>
       <ci>H</ci>
       <ci>X</ci>
      </apply>
      <apply>
       <times></times>
       <ci>H</ci>
       <ci>Y</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>H</ci>
      <interval closure="open">
       <ci>X</ci>
       <ci>Y</ci>
      </interval>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I(X;Y)=H(X)+H(Y)-H(X,Y)
  </annotation>
 </semantics>
</math>

, we can see that the maximum value achieveable by the MI for a given pair 

<math display="inline" id="Maximal_information_coefficient:12">
 <semantics>
  <mrow>
   <msub>
    <mi>n</mi>
    <mi>x</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>n</mi>
    <mi>y</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>n</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>n</ci>
     <ci>y</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{x},n_{y}
  </annotation>
 </semantics>
</math>

 of bin counts is 

<math display="inline" id="Maximal_information_coefficient:13">
 <semantics>
  <mrow>
   <mi>log</mi>
   <mrow>
    <mi>min</mi>
    <mrow>
     <mo>(</mo>
     <msub>
      <mi>n</mi>
      <mi>x</mi>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>n</mi>
      <mi>y</mi>
     </msub>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <log></log>
    <apply>
     <min></min>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <ci>x</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <ci>y</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \log\min\left(n_{x},n_{y}\right)
  </annotation>
 </semantics>
</math>

. Thus, this value is used as a normalizing divisor for each pair of bin counts.</p>

<p>Last, the normalized maximal mutual information value for different combinations of 

<math display="inline" id="Maximal_information_coefficient:14">
 <semantics>
  <msub>
   <mi>n</mi>
   <mi>x</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>n</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{x}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Maximal_information_coefficient:15">
 <semantics>
  <msub>
   <mi>n</mi>
   <mi>y</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>n</ci>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{y}
  </annotation>
 </semantics>
</math>

 is tabulated, and the maximum value in the table selected as the value of the statistic.</p>

<p>It is important to note that trying all possible binning schemes that satisfy 

<math display="inline" id="Maximal_information_coefficient:16">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>n</mi>
     <mi>x</mi>
    </msub>
    <mo>×</mo>
    <msub>
     <mi>n</mi>
     <mi>y</mi>
    </msub>
   </mrow>
   <mo>≤</mo>
   <msup>
    <mi mathvariant="normal">N</mi>
    <mn>0.6</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <ci>x</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <ci>y</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>normal-N</ci>
     <cn type="float">0.6</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{x}\times n_{y}\leq\mathrm{N}^{0.6}
  </annotation>
 </semantics>
</math>

 is computationally unfeasible even for small n. Therefore in practice the authors apply a heuristic which may or may not find the true maximum.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Information_theory" title="wikilink">Category:Information theory</a> <a href="Category:Covariance_and_correlation" title="wikilink">Category:Covariance and correlation</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">Reshef et al. 2011<a href="#fnref2">↩</a></li>
<li id="fn3"><a href="http://biomet.oxfordjournals.org/content/100/2/503">A consistent multivariate test of association based on ranks of distances</a><a href="#fnref3">↩</a></li>
<li id="fn4">Noah Simon and Robert Tibshirani, <a href="http://www-stat.stanford.edu/~tibs/reshef/comment.pdf">Comment on “Detecting Novel Associations in Large Data Sets” by Reshef et al., Science Dec. 16, 2011</a><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="http://ie.technion.ac.il/~gorfinm/files/science6.pdf">Comment on "Detecting Novel Associations in Large Data Sets"</a><a href="#fnref5">↩</a></li>
<li id="fn6"></li>
<li id="fn7"></li>
<li id="fn8"></li>
<li id="fn9"><a href="http://arxiv.org/abs/1301.7745v1">Equitability, mutual information, and the maximal information coefficient by Justin B. Kinney, Gurinder S. Atwal, arXiv Jan. 31, 2013</a><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="http://arxiv.org/abs/1301.6314v1">Equitability Analysis of the Maximal Information Coefficient, with Comparisons by David Reshef, Yakir Reshef, Michael Mitzenmacher, Pardis Sabeti, arXiv Jan. 27, 2013</a><a href="#fnref10">↩</a></li>
<li id="fn11">The "b" subscripts have been used to emphasize that the mutual information is calculated using the bins<a href="#fnref11">↩</a></li>
</ol>
</section>
</body>
</html>
