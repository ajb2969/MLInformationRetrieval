<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="206">Transfer entropy</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Transfer entropy</h1>
<hr>'''Transfer entropy''' is a [[non-parametric statistics|non-parametric statistic]] measuring the amount of directed (time-asymmetric) transfer of [[information]] between two [[random process]]es.<ref>{{cite journal|last=Schreiber|first=Thomas|title=Measuring Information Transfer|journal=Physical Review Letters|date=1 July 2000|volume=85|issue=2|pages=461–464|doi=10.1103/PhysRevLett.85.461|url=http://prl.aps.org/abstract/PRL/v85/i2/p461_1}}</ref><ref name="Scholarpedia">{{cite encyclopedia |year= 2007 |title = Grang
<p>er causality |last= Seth |first=Anil|encyclopedia=<a class="uri" href="Scholarpedia" title="wikilink">Scholarpedia</a> |url=<a class="uri" href="http://www.scholarpedia.org/article/Granger_causality|doi=10.4249/scholarpedia.1667">http://www.scholarpedia.org/article/Granger_causality|doi=10.4249/scholarpedia.1667</a> }}<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Transfer entropy from a process <em>X</em> to another process <em>Y</em> is the amount of uncertainty reduced in future values of <em>Y</em> by knowing the past values of <em>X</em> given past values of <em>Y</em>. More specifically, if 

<math display="inline" id="Transfer_entropy:0">
 <semantics>
  <msub>
   <mi>X</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>X</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X_{t}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Transfer_entropy:1">
 <semantics>
  <msub>
   <mi>Y</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Y</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y_{t}
  </annotation>
 </semantics>
</math>

 for 

<math display="inline" id="Transfer_entropy:2">
 <semantics>
  <mrow>
   <mi>t</mi>
   <mo>∈</mo>
   <mi>ℕ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>t</ci>
    <ci>ℕ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t\in\mathbb{N}
  </annotation>
 </semantics>
</math>

 denote two random processes and the amount of information is measured using <a href="Shannon's_entropy" title="wikilink">Shannon's entropy</a>, the transfer entropy can be written as:</p>

<p>

<math display="block" id="Transfer_entropy:3">
 <semantics>
  <mrow>
   <msub>
    <mi>T</mi>
    <mrow>
     <mi>X</mi>
     <mo>→</mo>
     <mi>Y</mi>
    </mrow>
   </msub>
   <mo>=</mo>
   <mi>H</mi>
   <mrow>
    <mo>(</mo>
    <msub>
     <mi>Y</mi>
     <mi>t</mi>
    </msub>
    <mo>∣</mo>
    <msub>
     <mi>Y</mi>
     <mrow>
      <mrow>
       <mi>t</mi>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
      <mo>:</mo>
      <mrow>
       <mi>t</mi>
       <mo>-</mo>
       <mi>L</mi>
      </mrow>
     </mrow>
    </msub>
    <mo>)</mo>
   </mrow>
   <mo>-</mo>
   <mi>H</mi>
   <mrow>
    <mo>(</mo>
    <msub>
     <mi>Y</mi>
     <mi>t</mi>
    </msub>
    <mo>∣</mo>
    <msub>
     <mi>Y</mi>
     <mrow>
      <mrow>
       <mi>t</mi>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
      <mo>:</mo>
      <mrow>
       <mi>t</mi>
       <mo>-</mo>
       <mi>L</mi>
      </mrow>
     </mrow>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>X</mi>
     <mrow>
      <mrow>
       <mi>t</mi>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
      <mo>:</mo>
      <mrow>
       <mi>t</mi>
       <mo>-</mo>
       <mi>L</mi>
      </mrow>
     </mrow>
    </msub>
    <mo>)</mo>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>T</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>X</ci>
      <ci>Y</ci>
     </apply>
    </apply>
    <eq></eq>
    <csymbol cd="unknown">H</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>t</ci>
     </apply>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <apply>
       <ci>normal-:</ci>
       <apply>
        <minus></minus>
        <ci>t</ci>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <minus></minus>
        <ci>t</ci>
        <ci>L</ci>
       </apply>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <minus></minus>
    <csymbol cd="unknown">H</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>t</ci>
     </apply>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <apply>
       <ci>normal-:</ci>
       <apply>
        <minus></minus>
        <ci>t</ci>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <minus></minus>
        <ci>t</ci>
        <ci>L</ci>
       </apply>
      </apply>
     </apply>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>X</ci>
      <apply>
       <ci>normal-:</ci>
       <apply>
        <minus></minus>
        <ci>t</ci>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <minus></minus>
        <ci>t</ci>
        <ci>L</ci>
       </apply>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-,</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T_{X\rightarrow Y}=H\left(Y_{t}\mid Y_{t-1:t-L}\right)-H\left(Y_{t}\mid Y_{t-1%
:t-L},X_{t-1:t-L}\right),
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>H</em>(<em>X</em>) is Shannon entropy of <em>X</em>. The above definition of transfer entropy has been extended by other types of <a href="entropy_(information_theory)" title="wikilink">entropy</a> measures such as <a href="Rényi_entropy" title="wikilink">Rényi entropy</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>Transfer entropy is <a href="conditional_mutual_information" title="wikilink">conditional mutual information</a>,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> with the history of the influenced variable 

<math display="inline" id="Transfer_entropy:4">
 <semantics>
  <msub>
   <mi>Y</mi>
   <mrow>
    <mrow>
     <mi>t</mi>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
    <mo>:</mo>
    <mrow>
     <mi>t</mi>
     <mo>-</mo>
     <mi>L</mi>
    </mrow>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Y</ci>
    <apply>
     <ci>normal-:</ci>
     <apply>
      <minus></minus>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <minus></minus>
      <ci>t</ci>
      <ci>L</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y_{t-1:t-L}
  </annotation>
 </semantics>
</math>

 in the condition. Transfer entropy reduces to <a href="Granger_causality" title="wikilink">Granger causality</a> for <a href="Autoregressive_model" title="wikilink">vector auto-regressive processes</a>.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Hence, it is advantageous when the model assumption of Granger causality doesn't hold, for example, analysis of <a href="non-linear_regression" title="wikilink">non-linear signals</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> However, it usually requires more samples for accurate estimation.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> While it was originally defined for <a href="bivariate_analysis" title="wikilink">bivariate analysis</a>, transfer entropy has been extended to <a href="Multivariate_analysis" title="wikilink">multivariate</a> forms, either conditioning on other potential source variables<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> or considering transfer from a collection of sources,<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> although these forms require more samples again.</p>

<p>Transfer entropy has been used for estimation of <a href="functional_connectivity" title="wikilink">functional connectivity</a> of <a class="uri" href="neurons" title="wikilink">neurons</a><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> and <a href="social_influence" title="wikilink">social influence</a> in <a href="social_networks" title="wikilink">social networks</a>.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Conditional_mutual_information" title="wikilink">Conditional mutual information</a></li>
<li><a class="uri" href="Causality" title="wikilink">Causality</a></li>
<li><a href="Causality_(physics)" title="wikilink">Causality (physics)</a></li>
<li><a href="Structural_equation_modeling" title="wikilink">Structural equation modeling</a></li>
<li><a href="Rubin_causal_model" title="wikilink">Rubin causal model</a></li>
<li><a href="Mutual_Information" title="wikilink">Mutual Information</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li>

<p>, a toolbox, developed in <a class="uri" href="C++" title="wikilink">C++</a> and <a class="uri" href="MATLAB" title="wikilink">MATLAB</a>, for computation of transfer entropy between spike trains.</p></li>
<li>

<p>, a toolbox, developed in <a href="Java_(programming_language)" title="wikilink">Java</a> and usable in <a class="uri" href="MATLAB" title="wikilink">MATLAB</a>, <a href="GNU_Octave" title="wikilink">GNU Octave</a> and <a href="Python_(programming_language)" title="wikilink">Python</a>, for computation of transfer entropy and related information-theoretic measures in both discrete and continuous-valued data.</p></li>
</ul>

<p>"</p>

<p><a class="uri" href="Category:Causality" title="wikilink">Category:Causality</a> <a href="Category:Nonlinear_time_series_analysis" title="wikilink">Category:Nonlinear time series analysis</a> <a href="Category:Non-parametric_statistics" title="wikilink">Category:Non-parametric statistics</a> <a href="Category:Entropy_and_information" title="wikilink">Category:Entropy and information</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
</ol>
</section>
</ref></hr></body>
</html>
