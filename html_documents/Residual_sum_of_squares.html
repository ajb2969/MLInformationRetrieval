<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="745">Residual sum of squares</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Residual sum of squares</h1>
<hr/>
<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, the <strong>residual sum of squares (RSS)</strong>, also known as the <strong>sum of squared residuals (SSR)</strong> or the <strong>sum of squared errors of prediction (SSE)</strong>, is the <a href="summation" title="wikilink">sum</a> of the <a href="square_(arithmetic)" title="wikilink">squares</a> of <a href="errors_and_residuals_in_statistics" title="wikilink">residuals</a> (deviations of predicted from actual empirical values of data). It is a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. It is used as an <a href="optimality_criterion" title="wikilink">optimality criterion</a> in parameter selection and <a href="model_selection" title="wikilink">model selection</a>.</p>
<p>In general, <a href="total_sum_of_squares" title="wikilink">total sum of squares</a> = <a href="explained_sum_of_squares" title="wikilink">explained sum of squares</a> + <strong>residual sum of squares</strong>. For a proof of this in the multivariate <a href="ordinary_least_squares" title="wikilink">ordinary least squares</a> (OLS) case, see <a href="Explained_sum_of_squares#Partitioning_in_the_general_OLS_model" title="wikilink">partitioning in the general OLS model</a>.</p>
<h2 id="one-explanatory-variable">One explanatory variable</h2>
<p>In a model with a single explanatory variable, RSS is given by</p>
<p><span class="LaTeX">$$RSS = \sum_{i=1}^n (y_i - f(x_i))^2,$$</span></p>
<p>where <em>y</em><sub><em>i</em></sub> is the <em>i</em> <sup>th</sup> value of the variable to be predicted, <em>x</em><sub><em>i</em></sub> is the <em>i</em> <sup>th</sup> value of the explanatory variable, and <span class="LaTeX">$f(x_i)$</span> is the predicted value of <em>y</em><sub><em>i</em></sub> (also termed <span class="LaTeX">$\hat{y_i}$</span>). In a standard linear simple <a href="regression_model" title="wikilink">regression model</a>, <span class="LaTeX">$y_i = a+bx_i+\varepsilon_i\,$</span>, where <em>a</em> and <em>b</em> are <a href="coefficient" title="wikilink">coefficients</a>, <em>y</em> and <em>x</em> are the <a class="uri" href="regressand" title="wikilink">regressand</a> and the <a class="uri" href="regressor" title="wikilink">regressor</a>, respectively, and ε is the <a href="errors_and_residuals_in_statistics" title="wikilink">error term</a>. The sum of squares of residuals is the sum of squares of <a href="estimator" title="wikilink">estimates</a> of ε<sub><em>i</em></sub>; that is</p>
<p><span class="LaTeX">$$RSS = \sum_{i=1}^n (\varepsilon_i)^2 = \sum_{i=1}^n (y_i - (\alpha + \beta x_i))^2,$$</span></p>
<p>where <span class="LaTeX">$\alpha$</span> is the estimated value of the constant term <span class="LaTeX">$a$</span> and <span class="LaTeX">$\beta$</span> is the estimated value of the slope coefficient <em>b</em>.</p>
<h2 id="matrix-expression-for-the-ols-residual-sum-of-squares">Matrix expression for the OLS residual sum of squares</h2>
<p>The general regression model with <em>n</em> observations and <em>k</em> explanators, the first of which is a constant unit vector whose coefficient is the regression intercept, is</p>
<p><span class="LaTeX">$$y = X \beta + e$$</span></p>
<p>where <em>y</em> is an <em>n</em> × 1 vector of dependent variable observations, each column of the <em>n</em> × <em>k</em> matrix <em>X</em> is a vector of observations on one of the <em>k</em> explanators, <span class="LaTeX">$\beta$</span> is a <em>k</em> × 1 vector of true coefficients, and <em>e</em> is an <em>n</em>× 1 vector of the true underlying errors. The <a href="ordinary_least_squares" title="wikilink">ordinary least squares</a> estimator for <span class="LaTeX">$\beta$</span> is</p>
<p><span class="LaTeX">$$\hat \beta = (X^T X)^{-1}X^T y.$$</span></p>
<p>The residual vector <span class="LaTeX">$\hat e$</span> is <span class="LaTeX">$y - X \hat \beta = y - X (X^T X)^{-1}X^T y$</span>, so the residual sum of squares <span class="LaTeX">$\hat e ^T \hat e$</span> is, after simplification,</p>
<p><span class="LaTeX">$$RSS = y^T y - y^T X(X^T X)^{-1} X^T y = y^T [I - X(X^T X)^{-1} X^T] y = y^T [I - H] y$$</span> , where H is the <a href="hat_matrix" title="wikilink">hat matrix</a>, or the prediction matrix in linear regression.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Sum_of_squares_(statistics)" title="wikilink">Sum of squares (statistics)</a></li>
<li><a href="Squared_deviations" title="wikilink">Squared deviations</a></li>
<li><a href="Errors_and_residuals_in_statistics" title="wikilink">Errors and residuals in statistics</a></li>
<li><a href="Lack-of-fit_sum_of_squares" title="wikilink">Lack-of-fit sum of squares</a></li>
<li><a href="Degrees_of_freedom_(statistics)#Sum_of_squares_and_degrees_of_freedom" title="wikilink">Degrees of freedom (statistics)#Sum of squares and degrees of freedom</a></li>
<li><a href="Chi-squared_distribution#Applications" title="wikilink">Chi-squared distribution#Applications</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li></li>
</ul>
<p>"</p>
<p><a href="Category:Regression_analysis" title="wikilink">Category:Regression analysis</a> <a href="Category:Least_squares" title="wikilink">Category:Least squares</a></p>
</body>
</html>
