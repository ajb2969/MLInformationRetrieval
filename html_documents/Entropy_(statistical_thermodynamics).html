<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1690">Entropy (statistical thermodynamics)</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Entropy (statistical thermodynamics)</h1>
<hr/>
<p>In <a href="classical_physics" title="wikilink">classical</a> <a href="statistical_mechanics" title="wikilink">statistical mechanics</a>, the <a class="uri" href="entropy" title="wikilink">entropy</a> function earlier introduced by Clausius is interpreted as <strong>statistical entropy</strong> using <a href="probability_theory" title="wikilink">probability theory</a>. The statistical entropy perspective was introduced in 1870 with the work of the Austrian physicist <a href="Ludwig_Boltzmann" title="wikilink">Ludwig Boltzmann</a>.</p>
<h2 id="gibbs-entropy-formula">Gibbs Entropy Formula</h2>
<p>The macroscopic state of the system is defined by a distribution on the <a href="microstate_(statistical_mechanics)" title="wikilink">microstates</a> that are accessible to a system in the course of its <a href="thermal_fluctuations" title="wikilink">thermal fluctuations</a>. So the entropy is defined for two different levels of description of the given system. At one of these levels, the entropy is given by the Gibbs entropy formula, named after <a href="Josiah_Willard_Gibbs" title="wikilink">J. Willard Gibbs</a>. For a classical system (i.e., a collection of classical particles) with a discrete set of microstates, if <span class="LaTeX">$E_i$</span> is the energy of microstate <em>i</em>, and <span class="LaTeX">$p_i$</span> is the probability that it occurs during the system's fluctuations, then the entropy of the system is</p>
<p><span class="LaTeX">$$S = -k_\text{B}\,\sum_i p_i \ln \,p_i$$</span></p>
<div style=" width: 320px; float: right; margin: 0 0 1em 1em; border-style: solid; border-width: 1px; padding: 1em; font-size: 90%">
<p><strong>Entropy changes for systems in a canonical state</strong></p>
<p>A system with a well-defined temperature, i.e., one in thermal equilibrium with a thermal reservoir, has a probability of being in a microstate <em>i</em> given by <a href="Boltzmann's_distribution" title="wikilink">Boltzmann's distribution</a>.</p>
<p>Changes in the entropy caused by changes in the external constraints are then given by:</p>
<p><span class="LaTeX">$$dS = -k_\text{B}\,\sum_i dp_i \ln p_i$$</span></p>
<p><span class="LaTeX">$$\,\,\, = -k_\text{B}\,\sum_i dp_i (-E_i/k_\text{B}T -\ln Z)$$</span></p>
<p><span class="LaTeX">$$\,\,\, = \sum_i E_i dp_i / T$$</span></p>
<p><span class="LaTeX">$$\,\,\, = \sum_i [d (E_i p_i) - (dE_i) p_i] / T$$</span></p>
<p>where we have twice used the conservation of probability, <mtpl></mtpl>.</p>
<p>Now, <mtpl></mtpl> is the expectation value of the change in the total energy of the system.</p>
<p>If the changes are sufficiently slow, so that the system remains in the same microscopic state, but the state slowly (and reversibly) changes, then <mtpl></mtpl> is the expectation value of the work done on the system through this reversible process, <em>dw</em><sub>rev</sub>.</p>
<p>But from the first law of thermodynamics, . Therefore,</p>
<p><span class="LaTeX">$$dS = \frac{\delta\langle q_\text{rev} \rangle}{T}$$</span></p>
<p>In the <a href="thermodynamic_limit" title="wikilink">thermodynamic limit</a>, the fluctuation of the macroscopic quantities from their average values becomes negligible; so this reproduces the definition of entropy from classical thermodynamics, given above.</p>
</div>
<p>The quantity <span class="LaTeX">$k_\text{B}$</span> is a <a href="physical_constant" title="wikilink">physical constant</a> known as <a href="Boltzmann_constant" title="wikilink">Boltzmann's constant</a>, which, like the entropy, has units of <a href="heat_capacity" title="wikilink">heat capacity</a>. The <a href="Natural_logarithm" title="wikilink">logarithm</a> is <a href="Dimensionless_number" title="wikilink">dimensionless</a>.</p>
<p>This definition remains meaningful even when the system is far away from equilibrium. Other definitions assume that the system is in <a href="thermal_equilibrium" title="wikilink">thermal equilibrium</a>, either as an <a href="isolated_system" title="wikilink">isolated system</a>, or as a system in exchange with its surroundings. The set of microstates (with probability distribution) on which the sum is done is called a <a href="statistical_ensemble" title="wikilink">statistical ensemble</a>. Each type of <a href="statistical_ensemble" title="wikilink">statistical ensemble</a> (micro-canonical, canonical, grand-canonical, etc.) describes a different configuration of the system's exchanges with the outside, varying from a completely isolated system to a system that can exchange one or more quantities with a reservoir, like energy, volume or molecules. In every ensemble, the <a href="thermodynamic_equilibrium" title="wikilink">equilibrium</a> configuration of the system is dictated by the maximization of the entropy of the union of the system and its reservoir, according to the <a href="second_law_of_thermodynamics" title="wikilink">second law of thermodynamics</a> (see the <a href="statistical_mechanics" title="wikilink">statistical mechanics</a> article).</p>
<p>Neglecting <a href="correlation" title="wikilink">correlations</a> (or, more generally, <a href="Statistical_independence" title="wikilink">statistical dependencies</a>) between the states of individual particles will lead to an incorrect probability distribution on the microstates and thence to an overestimate of the entropy.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Such correlations occur in any system with nontrivially interacting particles, that is, in all systems more complex than an <a href="ideal_gas" title="wikilink">ideal gas</a>.</p>
<p>This <em>S</em> is almost universally called simply the <em>entropy</em>. It can also be called the <em>statistical entropy</em> or the <em>thermodynamic entropy</em> without changing the meaning. Note the above expression of the statistical entropy is a discretized version of <a href="Shannon_entropy" title="wikilink">Shannon entropy</a>. The <a href="von_Neumann_entropy" title="wikilink">von Neumann entropy</a> formula is an extension of the Gibbs entropy formula to the <a href="Quantum_mechanics" title="wikilink">quantum mechanical</a> case.</p>
<p>It has been shown<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> that the Gibb's Entropy is equal to the classical "heat engine" entropy characterized by <span class="LaTeX">$dS = \frac{\delta Q}{T} \!$</span></p>
<h2 id="boltzmanns-principle">Boltzmann's principle</h2>
<p>In Boltzmann's definition, entropy is a measure of the number of possible microscopic states (or <strong>microstates</strong>) of a system in <a href="thermodynamic_equilibrium" title="wikilink">thermodynamic equilibrium</a>, consistent with its macroscopic thermodynamic properties (or <strong>macrostate</strong>). To understand what microstates and macrostates are, consider the example of a <a class="uri" href="gas" title="wikilink">gas</a> in a container. At a microscopic level, the gas consists of a <a href="Avogadro's_number" title="wikilink">vast number</a> of freely moving <a href="atom" title="wikilink">atoms</a>, which occasionally collide with one another and with the walls of the container. The microstate of the system is a description of the <a href="position_(vector)" title="wikilink">positions</a> and <a href="momentum" title="wikilink">momenta</a> of all the atoms. In principle, all the physical properties of the system are determined by its microstate. However, because the number of atoms is so large, the details of the motion of individual atoms is mostly irrelevant to the behavior of the system as a whole. Provided the system is in thermodynamic equilibrium, the system can be adequately described by a handful of macroscopic quantities, called "thermodynamic variables": the total <a class="uri" href="energy" title="wikilink">energy</a> <em>E</em>, <a class="uri" href="volume" title="wikilink">volume</a> <em>V</em>, <a class="uri" href="pressure" title="wikilink">pressure</a> <em>P</em>, <a class="uri" href="temperature" title="wikilink">temperature</a> <em>T</em>, and so forth. The macrostate of the system is a description of its thermodynamic variables.</p>
<p>There are three important points to note. Firstly, to specify any one microstate, we need to write down an impractically long list of numbers, whereas specifying a macrostate requires only a few numbers (<em>E</em>, <em>V</em>, etc.). However, and this is the second point, the usual <a href="thermodynamic_equations" title="wikilink">thermodynamic equations</a> only describe the macrostate of a system adequately when this system is in equilibrium; non-equilibrium situations can generally <em>not</em> be described by a small number of variables. As a simple example, consider adding a drop of food coloring to a glass of water. The food coloring diffuses in a complicated matter, which is in practice very difficult to precisely predict. However, after sufficient time has passed the system will reach a uniform color, which is much less complicated to describe. Actually, the macroscopic state of the system will be described by a small number of variables only if the system is at global <a href="thermodynamic_equilibrium" title="wikilink">thermodynamic equilibrium</a>. Thirdly, more than one microstate can correspond to a single macrostate. In fact, for any given macrostate, there will be a huge number of microstates that are consistent with the given values of <em>E</em>, <em>V</em>, etc.</p>
<p>We are now ready to provide a definition of entropy. The entropy <em>S</em> is defined as</p>
<p><span class="LaTeX">$$S = k_\text{B} \ln \Omega$$</span> where</p>
<dl>
<dd><em>k</em><sub>B</sub> is <a href="Boltzmann_constant" title="wikilink">Boltzmann's constant</a> and
</dd>
<dd><em><span class="LaTeX">$\Omega$</span></em> is the number of microstates consistent with the given macrostate.
</dd>
</dl>
<p>The statistical entropy reduces to Boltzmann's entropy when all the accessible microstates of the system are equally likely. It is also the configuration corresponding to the maximum of a system's entropy for a given set of accessible <a href="microstate_(statistical_mechanics)" title="wikilink">microstates</a>, in other words the macroscopic configuration in which the lack of information is maximal. As such, according to the <a href="second_law_of_thermodynamics" title="wikilink">second law of thermodynamics</a>, it is the <a href="thermodynamic_equilibrium" title="wikilink">equilibrium</a> configuration of an isolated system. Boltzmann's entropy is the expression of entropy at thermodynamic equilibrium in the micro-canonical ensemble.</p>
<p>This postulate, which is known as Boltzmann's principle, may be regarded as the foundation of <a href="statistical_mechanics" title="wikilink">statistical mechanics</a>, which describes thermodynamic systems using the statistical behaviour of its constituents. It turns out that <em>S</em> is itself a thermodynamic property, just like <em>E</em> or <em>V</em>. Therefore, it acts as a link between the microscopic world and the macroscopic. One important property of <em>S</em> follows readily from the definition: since Ω is a <a href="natural_number" title="wikilink">natural number</a> (1,2,3,...), <em>S</em> is either <em>zero</em> or <em>positive</em> (, .)</p>
<h3 id="ensembles">Ensembles</h3>
<p>The various ensembles used in statistical thermodynamics are linked to the entropy by the following relations:</p>
<p><span class="LaTeX">$$S=k_\text{B} \ln \Omega_{\rm mic} = k_\text{B} (\ln Z_{\rm can} + \beta \bar E) = k_\text{B} (\ln \mathcal{Z}_{\rm gr} + \beta (\bar E - \mu \bar N))$$</span> <span class="LaTeX">$\Omega_{\rm mic}$</span> is the <a href="microcanonical_ensemble" title="wikilink">microcanonical partition function</a><br/>
<span class="LaTeX">$Z_{\rm can}$</span> is the <a href="canonical_ensemble" title="wikilink">canonical partition function</a><br/>
<span class="LaTeX">$\mathcal{Z}_{\rm gr}$</span> is the <a href="grand_canonical_ensemble" title="wikilink">grand canonical partition function</a></p>
<h2 id="lack-of-knowledge-and-the-second-law-of-thermodynamics">Lack of knowledge and the second law of thermodynamics</h2>
<p>We can view <em>Ω</em> as a measure of our lack of knowledge about a system. As an illustration of this idea, consider a set of 100 <a href="coin" title="wikilink">coins</a>, each of which is either <a href="coin_flipping" title="wikilink">heads up or tails up</a>. The macrostates are specified by the total number of heads and tails, whereas the microstates are specified by the facings of each individual coin. For the macrostates of 100 heads or 100 tails, there is exactly one possible configuration, so our knowledge of the system is complete. At the opposite extreme, the macrostate which gives us the least knowledge about the system consists of 50 heads and 50 tails in any order, for which there are 100,891,344,545,564,193,334,812,497,256 (<a href="combination" title="wikilink">100 choose 50</a>) ≈ 10<sup>29</sup> possible microstates.</p>
<p>Even when a system is entirely isolated from external influences, its microstate is constantly changing. For instance, the particles in a gas are constantly moving, and thus occupy a different position at each moment of time; their momenta are also constantly changing as they collide with each other or with the container walls. Suppose we prepare the system in an artificially highly ordered equilibrium state. For instance, imagine dividing a container with a partition and placing a gas on one side of the partition, with a vacuum on the other side. If we remove the partition and watch the subsequent behavior of the gas, we will find that its microstate evolves according to some chaotic and unpredictable pattern, and that on average these microstates will correspond to a more disordered macrostate than before. It is <em>possible</em>, but <em>extremely unlikely</em>, for the gas molecules to bounce off one another in such a way that they remain in one half of the container. It is overwhelmingly probable for the gas to spread out to fill the container evenly, which is the new equilibrium macrostate of the system.</p>
<p>This is an example illustrating the <a href="second_law_of_thermodynamics" title="wikilink">Second Law of Thermodynamics</a>:</p>
<dl>
<dd><em>the total entropy of any isolated thermodynamic system tends to increase over time, approaching a maximum value</em>.
</dd>
</dl>
<p>Since its discovery, this idea has been the focus of a great deal of thought, some of it confused. A chief point of confusion is the fact that the Second Law applies only to <em>isolated</em> systems. For example, the <a class="uri" href="Earth" title="wikilink">Earth</a> is not an isolated system because it is constantly receiving energy in the form of <a class="uri" href="sunlight" title="wikilink">sunlight</a>. In contrast, the <a class="uri" href="universe" title="wikilink">universe</a> may be considered an isolated system, so that its total entropy is constantly increasing.</p>
<h2 id="counting-of-microstates">Counting of microstates</h2>
<p>In <a href="classical_mechanics" title="wikilink">classical</a> <a href="statistical_mechanics" title="wikilink">statistical mechanics</a>, the number of microstates is actually <a href="Uncountable_set" title="wikilink">uncountably infinite</a>, since the properties of classical systems are continuous. For example, a microstate of a classical ideal gas is specified by the positions and momenta of all the atoms, which range continuously over the <a href="real_number" title="wikilink">real numbers</a>. If we want to define Ω, we have to come up with a method of grouping the microstates together to obtain a countable set. This procedure is known as <a href="coarse_graining" title="wikilink">coarse graining</a>. In the case of the ideal gas, we count two states of an atom as the "same" state if their positions and momenta are within <em>δx</em> and <em>δp</em> of each other. Since the values of <em>δx</em> and <em>δp</em> can be chosen arbitrarily, the entropy is not uniquely defined. It is defined only up to an additive constant. (As we will see, the <a href="Entropy_(classical_thermodynamics)" title="wikilink">thermodynamic definition of entropy</a> is also defined only up to a constant.)</p>
<p>This ambiguity can be resolved with <a href="quantum_mechanics" title="wikilink">quantum mechanics</a>. The <a href="quantum_state" title="wikilink">quantum state</a> of a system can be expressed as a superposition of "basis" states, which can be chosen to be energy <a href="eigenstate" title="wikilink">eigenstates</a> (i.e. eigenstates of the quantum <a href="Hamiltonian_(quantum_mechanics)" title="wikilink">Hamiltonian</a>). Usually, the quantum states are discrete, even though there may be an infinite number of them. For a system with some specified energy <em>E</em>, one takes Ω to be the number of energy eigenstates within a macroscopically small energy range between <em>E</em> and . In the <a href="thermodynamical_limit" title="wikilink">thermodynamical limit</a>, the specific entropy becomes independent on the choice of <em>δE</em>.</p>
<p>An important result, known as <a href="Nernst's_theorem" title="wikilink">Nernst's theorem</a> or the <a href="third_law_of_thermodynamics" title="wikilink">third law of thermodynamics</a>, states that the entropy of a system at <a href="absolute_zero" title="wikilink">zero absolute temperature</a> is a well-defined constant. This is because a system at zero temperature exists in its lowest-energy state, or <a href="ground_state" title="wikilink">ground state</a>, so that its entropy is determined by the <a href="Hamiltonian_(quantum_mechanics)" title="wikilink">degeneracy</a> of the ground state. Many systems, such as <a href="crystal" title="wikilink">crystal lattices</a>, have a unique ground state, and (since ) this means that they have zero entropy at absolute zero. Other systems have more than one state with the same, lowest energy, and have a non-vanishing "zero-point entropy". For instance, ordinary <a class="uri" href="ice" title="wikilink">ice</a> has a zero-point entropy of , because its underlying <a href="crystal_structure" title="wikilink">crystal structure</a> possesses multiple configurations with the same energy (a phenomenon known as <a href="geometrical_frustration" title="wikilink">geometrical frustration</a>).</p>
<p>The third law of thermodynamics states that the entropy of a perfect crystal at absolute zero, or 0 <a class="uri" href="kelvin" title="wikilink">kelvin</a> is zero. This means that in a perfect crystal, at 0 kelvin, nearly all molecular motion should cease in order to achieve ΔS=0. A perfect crystal is one in which the internal lattice structure is the same at all times; in other words, it is fixed and non-moving, and does not have rotational or vibrational energy. This means that there is only one way in which this order can be attained: when every particle of the structure is in its proper place.</p>
<p>However, the <a href="quantum_harmonic_oscillator" title="wikilink">oscillator equation</a> for predicting quantized vibrational levels shows that even when the vibrational quantum number is 0, the molecule still has vibrational energy. This means that no matter how cold the temperature gets, the lattice will always vibrate. This is in keeping with the Heisenberg uncertainty principle, which states that both the position and the momentum of a particle cannot be known precisely, at a given time:</p>
<p><span class="LaTeX">$$E_\nu=h\nu_0(n+\begin{matrix} \frac{1}{2} \end{matrix})$$</span></p>
<p>where <span class="LaTeX">$h$</span> is Planck's constant, <span class="LaTeX">$\nu_0$</span> is the characteristic frequency of the vibration, and <span class="LaTeX">$n$</span> is the vibrational quantum number. Note that even when <span class="LaTeX">$n=0$</span> (the <a href="zero-point_energy" title="wikilink">zero-point energy</a>), <span class="LaTeX">$E_n$</span> does not equal 0.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Boltzmann_constant" title="wikilink">Boltzmann constant</a></li>
<li><a href="Configuration_entropy" title="wikilink">Configuration entropy</a></li>
<li><a href="Conformational_entropy" title="wikilink">Conformational entropy</a></li>
<li><a class="uri" href="Enthalpy" title="wikilink">Enthalpy</a></li>
<li><a class="uri" href="Entropy" title="wikilink">Entropy</a></li>
<li><a href="Entropy_(classical_thermodynamics)" title="wikilink">Entropy (classical thermodynamics)</a></li>
<li><a href="Entropy_(energy_dispersal)" title="wikilink">Entropy (energy dispersal)</a></li>
<li><a href="Entropy_of_mixing" title="wikilink">Entropy of mixing</a></li>
<li><a href="Entropy_(order_and_disorder)" title="wikilink">Entropy (order and disorder)</a></li>
<li><a href="Entropy_(information_theory)" title="wikilink">Entropy (information theory)</a></li>
<li><a href="History_of_entropy" title="wikilink">History of entropy</a></li>
<li><a href="Information_theory" title="wikilink">Information theory</a></li>
<li><a href="Thermodynamic_free_energy" title="wikilink">Thermodynamic free energy</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li>Boltzmann, Ludwig (1896, 1898). Vorlesungen über Gastheorie : 2 Volumes - Leipzig 1895/98 UB: O 5262-6. English version: Lectures on gas theory. Translated by Stephen G. Brush (1964) Berkeley: University of California Press; (1995) New York: Dover ISBN 0-486-68455-5</li>
</ul>
<p>"</p>
<p><a href="Category:Thermodynamic_entropy" title="wikilink">Category:Thermodynamic entropy</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">E.T. Jaynes; Gibbs vs Boltzmann Entropies; American Journal of Physics, 391, 1965<a href="#fnref1">↩</a></li>
<li id="fn2"></li>
</ol>
</section>
</body>
</html>
