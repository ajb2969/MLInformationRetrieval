<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1473">Multiple kernel learning</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Multiple kernel learning</h1>
<hr/>

<p><strong>Multiple kernel learning</strong> refers to a set of machine learning methods that use a predefined set of <a href="Kernel_method" title="wikilink">kernels</a> and learn an optimal linear or non-linear combination of kernels as part of the algorithm. Reasons to use multiple kernel learning include a) the ability to select for an optimal kernel and parameters from a larger set of kernels, reducing bias due to kernel selection while allowing for more automated machine learning methods, and b) combining data from different sources (e.g. sound and images from a video) that have different notions of similarity and thus require different kernels. Instead of creating a new kernel, multiple kernel algorithms can be used to combine kernels already established for each individual data source.</p>

<p>Multiple kernel learning approaches been used in many applications, such as event recognition in video..,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> object recognition in images,<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> and biomedical data fusion.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="algorithms">Algorithms</h2>

<p>Multiple kernel learning algorithms have been developed for supervised, semi-supervised, as well as unsupervised learning. Most work has been done on the supervised learning case with linear combinations of kernels, however, many algorithms have been developed. The basic idea behind multiple kernel learning algorithms is to add an extra parameter to the minimization problem of the learning algorithm. As an example, consider the case of supervised learning of a linear combination of a set of 

<math display="inline" id="Multiple_kernel_learning:0">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 kernels 

<math display="inline" id="Multiple_kernel_learning:1">
 <semantics>
  <mi>K</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>K</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K
  </annotation>
 </semantics>
</math>

. We introduce a new kernel 

<math display="inline" id="Multiple_kernel_learning:2">
 <semantics>
  <mrow>
   <msup>
    <mi>K</mi>
    <mo>′</mo>
   </msup>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>n</mi>
    </msubsup>
    <mrow>
     <msub>
      <mi>β</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>K</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>K</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>n</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>β</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>K</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K^{\prime}=\sum_{i=1}^{n}\beta_{i}K_{i}
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Multiple_kernel_learning:3">
 <semantics>
  <msub>
   <mi>β</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>β</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta_{i}
  </annotation>
 </semantics>
</math>

 is a vector of coefficients for each kernel. Because the kernels are additive (due to properties of <a href="reproducing_kernel_Hilbert_spaces" title="wikilink">reproducing kernel Hilbert spaces</a>), this new function is still a kernel. For a set of data 

<math display="inline" id="Multiple_kernel_learning:4">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 with labels 

<math display="inline" id="Multiple_kernel_learning:5">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

, the minimization problem can then be written as</p>

<p>

<math display="block" id="Multiple_kernel_learning:6">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <munder>
      <mi>min</mi>
      <mrow>
       <mi>β</mi>
       <mo>,</mo>
       <mi>c</mi>
      </mrow>
     </munder>
     <merror class="ltx_ERROR undefined undefined">
      <mtext>\Epsilon</mtext>
     </merror>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>Y</mi>
     <mo>,</mo>
     <mrow>
      <msup>
       <mi>K</mi>
       <mo>′</mo>
      </msup>
      <mi>c</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mi>R</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>K</mi>
     <mo>,</mo>
     <mi>c</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <list>
        <ci>β</ci>
        <ci>c</ci>
       </list>
      </apply>
      <mtext>\Epsilon</mtext>
     </apply>
     <interval closure="open">
      <ci>Y</ci>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>K</ci>
        <ci>normal-′</ci>
       </apply>
       <ci>c</ci>
      </apply>
     </interval>
    </apply>
    <apply>
     <times></times>
     <ci>R</ci>
     <interval closure="open">
      <ci>K</ci>
      <ci>c</ci>
     </interval>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{\beta,c}\Epsilon(Y,K^{\prime}c)+R(K,c)
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Multiple_kernel_learning:7">
 <semantics>
  <merror class="ltx_ERROR undefined undefined">
   <mtext>\Epsilon</mtext>
  </merror>
  <annotation-xml encoding="MathML-Content">
   <mtext>\Epsilon</mtext>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Epsilon
  </annotation>
 </semantics>
</math>

 is an error function and 

<math display="inline" id="Multiple_kernel_learning:8">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

 is a regularization term. 

<math display="inline" id="Multiple_kernel_learning:9">
 <semantics>
  <merror class="ltx_ERROR undefined undefined">
   <mtext>\Epsilon</mtext>
  </merror>
  <annotation-xml encoding="MathML-Content">
   <mtext>\Epsilon</mtext>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Epsilon
  </annotation>
 </semantics>
</math>

 is typically the square loss function (<a href="Tikhonov_regularization" title="wikilink">Tikhonov regularization</a>) or the hinge loss function (for <a href="Support_vector_machine" title="wikilink">SVM</a> algorithms), and 

<math display="inline" id="Multiple_kernel_learning:10">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

 is usually an 

<math display="inline" id="Multiple_kernel_learning:11">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mi>n</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{n}
  </annotation>
 </semantics>
</math>

 norm or some combination of the norms (i.e. <a href="elastic_net_regularization" title="wikilink">elastic net regularization</a>). This optimization problem can then be solved by standard optimization methods. Adaptations of existing techniques such as the Sequential Minimal Optimization have also been developed for multiple kernel SVM-based methods.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h3 id="supervised-learning">Supervised learning</h3>

<p>For supervised learning, there are many other algorithms that use different methods to learn the form of the kernel. The following categorization has been proposed by Gonen and Alpaydın (2011)<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h4 id="fixed-rules-approaches">Fixed rules approaches</h4>

<p>Fixed rules approaches such as the linear combination algorithm described above use rules to set the combination of the kernels. These do not require parameterization and use rules like summation and multiplication to combine the kernels. The weighting is learned in the algorithm. Other examples of fixed rules include pairwise kernels, which are of the form</p>

<p>

<math display="block" id="Multiple_kernel_learning:12">
 <semantics>
  <mrow>
   <mrow>
    <mi>k</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>1</mn>
        <mi>i</mi>
       </mrow>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>1</mn>
        <mi>j</mi>
       </mrow>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo>,</mo>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>2</mn>
        <mi>i</mi>
       </mrow>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>2</mn>
        <mi>j</mi>
       </mrow>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>k</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>1</mn>
        <mi>i</mi>
       </mrow>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>2</mn>
        <mi>i</mi>
       </mrow>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>k</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>1</mn>
        <mi>j</mi>
       </mrow>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>2</mn>
        <mi>j</mi>
       </mrow>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>k</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>1</mn>
        <mi>i</mi>
       </mrow>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>2</mn>
        <mi>j</mi>
       </mrow>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>k</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>1</mn>
        <mi>j</mi>
       </mrow>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>2</mn>
        <mi>i</mi>
       </mrow>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>k</ci>
     <interval closure="open">
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">1</cn>
         <ci>i</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">1</cn>
         <ci>j</ci>
        </apply>
       </apply>
      </interval>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">2</cn>
         <ci>i</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">2</cn>
         <ci>j</ci>
        </apply>
       </apply>
      </interval>
     </interval>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <ci>k</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">1</cn>
         <ci>i</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">2</cn>
         <ci>i</ci>
        </apply>
       </apply>
      </interval>
      <ci>k</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">1</cn>
         <ci>j</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">2</cn>
         <ci>j</ci>
        </apply>
       </apply>
      </interval>
     </apply>
     <apply>
      <times></times>
      <ci>k</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">1</cn>
         <ci>i</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">2</cn>
         <ci>j</ci>
        </apply>
       </apply>
      </interval>
      <ci>k</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">1</cn>
         <ci>j</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">2</cn>
         <ci>i</ci>
        </apply>
       </apply>
      </interval>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k((x_{1i},x_{1j}),(x_{2i},x_{2j}))=k(x_{1i},x_{2i})k(x_{1j},x_{2j})+k(x_{1i},x%
_{2j})k(x_{1j},x_{2i})
  </annotation>
 </semantics>
</math>

. These pairwise approaches have been used in predicting protein-protein interactions.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h4 id="heuristic-approaches">Heuristic approaches</h4>

<p>These algorithms use a combination function that is parameterized. The parameters are generally defined for each individual kernel based on single-kernel performance or some computation from the kernel matrix. Examples of these include the kernel from Tenabe et al. (2008).<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> Letting 

<math display="inline" id="Multiple_kernel_learning:13">
 <semantics>
  <msub>
   <mi>π</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>π</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi_{m}
  </annotation>
 </semantics>
</math>

 be the accuracy obtained using only 

<math display="inline" id="Multiple_kernel_learning:14">
 <semantics>
  <msub>
   <mi>K</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>K</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K_{m}
  </annotation>
 </semantics>
</math>

, and letting 

<math display="inline" id="Multiple_kernel_learning:15">
 <semantics>
  <mi>δ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>δ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \delta
  </annotation>
 </semantics>
</math>

 be a threshold less than the minimum of the single-kernel accuracies, we can define</p>

<p>

<math display="block" id="Multiple_kernel_learning:16">
 <semantics>
  <mrow>
   <msub>
    <mi>β</mi>
    <mi>m</mi>
   </msub>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <msub>
      <mi>π</mi>
      <mi>m</mi>
     </msub>
     <mo>-</mo>
     <mi>δ</mi>
    </mrow>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>h</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </msubsup>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msub>
        <mi>π</mi>
        <mi>h</mi>
       </msub>
       <mo>-</mo>
       <mi>δ</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>β</ci>
     <ci>m</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>π</ci>
       <ci>m</ci>
      </apply>
      <ci>δ</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>h</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <minus></minus>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>π</ci>
        <ci>h</ci>
       </apply>
       <ci>δ</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta_{m}=\frac{\pi_{m}-\delta}{\sum_{h=1}^{n}(\pi_{h}-\delta)}
  </annotation>
 </semantics>
</math>

</p>

<p>Other approaches use a definition of kernel similarity, such as</p>

<p>

<math display="block" id="Multiple_kernel_learning:17">
 <semantics>
  <mrow>
   <msub>
    <mi>β</mi>
    <mi>m</mi>
   </msub>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>A</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>K</mi>
       <mi>m</mi>
      </msub>
      <mo>,</mo>
      <mrow>
       <mi>Y</mi>
       <msup>
        <mi>Y</mi>
        <mi>T</mi>
       </msup>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>h</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </msubsup>
     <mrow>
      <mi>A</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>K</mi>
        <mi>h</mi>
       </msub>
       <mo>,</mo>
       <mrow>
        <mi>Y</mi>
        <msup>
         <mi>Y</mi>
         <mi>T</mi>
        </msup>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>β</ci>
     <ci>m</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>A</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>K</ci>
        <ci>m</ci>
       </apply>
       <apply>
        <times></times>
        <ci>Y</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>Y</ci>
         <ci>T</ci>
        </apply>
       </apply>
      </interval>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>h</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <ci>A</ci>
       <interval closure="open">
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>K</ci>
         <ci>h</ci>
        </apply>
        <apply>
         <times></times>
         <ci>Y</ci>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>Y</ci>
          <ci>T</ci>
         </apply>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta_{m}=\frac{A(K_{m},YY^{T})}{\sum_{h=1}^{n}A(K_{h},YY^{T})}
  </annotation>
 </semantics>
</math>

</p>

<p>Using this measure, Qui and Lane (2009)<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> used the following heuristic to define</p>

<p>

<math display="block" id="Multiple_kernel_learning:18">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <munder>
      <mi>max</mi>
      <mrow>
       <mrow>
        <mrow>
         <mi>β</mi>
         <mo>,</mo>
         <mrow>
          <mo>tr</mo>
          <mrow>
           <mo stretchy="false">(</mo>
           <msubsup>
            <mi>K</mi>
            <mrow>
             <mi>t</mi>
             <mi>r</mi>
             <mi>a</mi>
            </mrow>
            <mo>′</mo>
           </msubsup>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
        </mrow>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mo>,</mo>
       <mrow>
        <msup>
         <mi>K</mi>
         <mo>′</mo>
        </msup>
        <mo>≥</mo>
        <mn>0</mn>
       </mrow>
      </mrow>
     </munder>
     <mi>A</mi>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <msubsup>
      <mi>K</mi>
      <mrow>
       <mi>t</mi>
       <mi>r</mi>
       <mi>a</mi>
      </mrow>
      <mo>′</mo>
     </msubsup>
     <mo>,</mo>
     <mrow>
      <mi>Y</mi>
      <msup>
       <mi>Y</mi>
       <mi>T</mi>
      </msup>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <max></max>
      <apply>
       <csymbol cd="ambiguous">formulae-sequence</csymbol>
       <apply>
        <eq></eq>
        <list>
         <ci>β</ci>
         <apply>
          <ci>tr</ci>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <apply>
            <csymbol cd="ambiguous">superscript</csymbol>
            <ci>K</ci>
            <ci>normal-′</ci>
           </apply>
           <apply>
            <times></times>
            <ci>t</ci>
            <ci>r</ci>
            <ci>a</ci>
           </apply>
          </apply>
         </apply>
        </list>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <geq></geq>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>K</ci>
         <ci>normal-′</ci>
        </apply>
        <cn type="integer">0</cn>
       </apply>
      </apply>
     </apply>
     <ci>A</ci>
    </apply>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>K</ci>
       <ci>normal-′</ci>
      </apply>
      <apply>
       <times></times>
       <ci>t</ci>
       <ci>r</ci>
       <ci>a</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>Y</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>Y</ci>
       <ci>T</ci>
      </apply>
     </apply>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \max_{\beta,\operatorname{tr}(K^{\prime}_{tra})=1,K^{\prime}\geq 0}A(K^{\prime%
}_{tra},YY^{T}).
  </annotation>
 </semantics>
</math>

</p>
<h4 id="optimization-approaches">Optimization approaches</h4>

<p>These approaches solve an optimization problem to determine parameters for the kernel combination function. This has been done with similarity measures and structural risk minimization approaches. For similarity measures such as the one defined above, the problem can be formulated as follows:<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>

<p>

<math display="inline" id="Multiple_kernel_learning:19">
 <semantics>
  <msubsup>
   <mi>K</mi>
   <mrow>
    <mi>t</mi>
    <mi>r</mi>
    <mi>a</mi>
   </mrow>
   <mo>′</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>K</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <times></times>
     <ci>t</ci>
     <ci>r</ci>
     <ci>a</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K^{\prime}_{tra}
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Multiple_kernel_learning:20">
 <semantics>
  <mrow>
   <mi>ω</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>K</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>ω</ci>
    <ci>K</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \omega(K)
  </annotation>
 </semantics>
</math>

 is the kernel of the training set.</p>

<p><a href="Structural_risk_minimization" title="wikilink">Structural risk minimization</a> approaches that have been used include linear approaches, such as that used by Lanckriet et al. (2002).<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> We can define the implausibility of a kernel 

<math display="block" id="Multiple_kernel_learning:21">
 <semantics>
  <mrow>
   <mrow>
    <munder>
     <mi>min</mi>
     <mrow>
      <mrow>
       <mo>tr</mo>
       <mrow>
        <mo stretchy="false">(</mo>
        <msubsup>
         <mi>K</mi>
         <mrow>
          <mi>t</mi>
          <mi>r</mi>
          <mi>a</mi>
         </mrow>
         <mo>′</mo>
        </msubsup>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>=</mo>
      <mi>c</mi>
     </mrow>
    </munder>
    <mi>ω</mi>
   </mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <msubsup>
     <mi>K</mi>
     <mrow>
      <mi>t</mi>
      <mi>r</mi>
      <mi>a</mi>
     </mrow>
     <mo>′</mo>
    </msubsup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <min></min>
      <apply>
       <eq></eq>
       <apply>
        <ci>tr</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>K</ci>
          <ci>normal-′</ci>
         </apply>
         <apply>
          <times></times>
          <ci>t</ci>
          <ci>r</ci>
          <ci>a</ci>
         </apply>
        </apply>
       </apply>
       <ci>c</ci>
      </apply>
     </apply>
     <ci>ω</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>K</ci>
      <ci>normal-′</ci>
     </apply>
     <apply>
      <times></times>
      <ci>t</ci>
      <ci>r</ci>
      <ci>a</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{\operatorname{tr}(K^{\prime}_{tra})=c}\omega(K^{\prime}_{tra})
  </annotation>
 </semantics>
</math>

 to be the value of the objective function after solving a canonical SVM problem. We can then solve the following minimization problem:</p>

<p>

<math display="inline" id="Multiple_kernel_learning:22">
 <semantics>
  <mi>c</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>c</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c
  </annotation>
 </semantics>
</math>

 where 

<math display="block" id="Multiple_kernel_learning:23">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>0</mn>
     </mrow>
     <mi>n</mi>
    </munderover>
    <mrow>
     <msub>
      <mi>α</mi>
      <mi>i</mi>
     </msub>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>m</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>p</mi>
      </munderover>
      <mrow>
       <msub>
        <mi>η</mi>
        <mi>m</mi>
       </msub>
       <msub>
        <mi>K</mi>
        <mi>m</mi>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <msubsup>
         <mi>x</mi>
         <mi>i</mi>
         <mi>m</mi>
        </msubsup>
        <mo>,</mo>
        <msup>
         <mi>x</mi>
         <mi>m</mi>
        </msup>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <sum></sum>
       <ci>n</ci>
      </apply>
      <apply>
       <eq></eq>
       <ci>i</ci>
       <cn type="integer">0</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>α</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <sum></sum>
         <ci>p</ci>
        </apply>
        <apply>
         <eq></eq>
         <ci>m</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>η</ci>
         <ci>m</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>K</ci>
         <ci>m</ci>
        </apply>
        <interval closure="open">
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>i</ci>
          </apply>
          <ci>m</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>x</ci>
          <ci>m</ci>
         </apply>
        </interval>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=\sum^{n}_{i=0}\alpha_{i}\sum^{p}_{m=1}\eta_{m}K_{m}(x_{i}^{m},x^{m})
  </annotation>
 </semantics>
</math>

 is a positive constant. Many other variations exist on the same idea, with different methods of refining and solving the problem, e.g. with nonnegative weights for individual kernels and using non-linear combinations of kernels.</p>
<h4 id="bayesian-approaches">Bayesian approaches</h4>

<p>Bayesian approaches put priors on the kernel parameters and learn the parameter values from the priors and the base algorithm. For example, the decision function can be written as</p>

<p>

<math display="inline" id="Multiple_kernel_learning:24">
 <semantics>
  <mi>η</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>η</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \eta
  </annotation>
 </semantics>
</math>

 

<math display="inline" id="Multiple_kernel_learning:25">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 can be modeled with a Dirichlet prior and 

<math display="block" id="Multiple_kernel_learning:26">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>N</mi>
     </munderover>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>m</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>P</mi>
      </munderover>
      <mrow>
       <msubsup>
        <mi>α</mi>
        <mi>i</mi>
        <mi>m</mi>
       </msubsup>
       <msub>
        <mi>K</mi>
        <mi>m</mi>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <msubsup>
         <mi>x</mi>
         <mi>i</mi>
         <mi>m</mi>
        </msubsup>
        <mo>,</mo>
        <msup>
         <mi>x</mi>
         <mi>m</mi>
        </msup>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mi>b</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>N</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>m</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>P</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>α</ci>
          <ci>i</ci>
         </apply>
         <ci>m</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>K</ci>
         <ci>m</ci>
        </apply>
        <interval closure="open">
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>i</ci>
          </apply>
          <ci>m</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>x</ci>
          <ci>m</ci>
         </apply>
        </interval>
       </apply>
      </apply>
     </apply>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=\sum_{i=1}^{N}\sum_{m=1}^{P}\alpha_{i}^{m}K_{m}(x_{i}^{m},x^{m})+b
  </annotation>
 </semantics>
</math>

 can be modeled with a zero-mean Gaussian and an inverse gamma variance prior. This model is then optimized using a customized <a href="multinomial_probit" title="wikilink">multinomial probit</a> approach with a <a href="Gibbs_sampling" title="wikilink">Gibbs sampler</a>.</p>

<p><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> These methods have been used successfully in applications such as protein fold recognition and protein homology problems <a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>
<h4 id="boosting-approaches">Boosting approaches</h4>

<p>Boosting approaches add new kernels iteratively until some stopping criteria that is a function of performance is reached. An example of this is the MARK model developed by Bennett et al. (2002) <a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>

<p>

<math display="inline" id="Multiple_kernel_learning:27">
 <semantics>
  <msubsup>
   <mi>α</mi>
   <mi>i</mi>
   <mi>m</mi>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>α</ci>
     <ci>i</ci>
    </apply>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{i}^{m}
  </annotation>
 </semantics>
</math>

</p>

<p>The parameters 

<math display="inline" id="Multiple_kernel_learning:28">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Multiple_kernel_learning:29">
 <semantics>
  <msub>
   <mi>α</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>α</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{i}
  </annotation>
 </semantics>
</math>

 are learned by gradient descent on a coordinate basis. In this way, each iteration of the descent algorithm identifies the best kernel column to choose at each particular iteration and adds that to the combined kernel. The model is then rerun to generate the optimal weights 

<math display="inline" id="Multiple_kernel_learning:30">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Multiple_kernel_learning:31">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mi>i</mi>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mi>i</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>L</ci>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>i</ci>
     </apply>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L={(x_{i},y_{i})}
  </annotation>
 </semantics>
</math>

.</p>
<h3 id="semisupervised-learning">Semisupervised learning</h3>

<p><a href="Semisupervised_learning" title="wikilink">Semisupervised learning</a> approaches to multiple kernel learning are similar to other extensions of supervised learning approaches. An inductive procedure has been developed that uses a log-likelihood empirical loss and group LASSO regularization with conditional expectation consensus on unlabeled data for image categorization. We can define the problem as follows. Let 

<math display="inline" id="Multiple_kernel_learning:32">
 <semantics>
  <mrow>
   <mi>U</mi>
   <mo>=</mo>
   <msub>
    <mi>x</mi>
    <mi>i</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>U</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>i</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   U={x_{i}}
  </annotation>
 </semantics>
</math>

 be the labeled data, and let 

<math display="block" id="Multiple_kernel_learning:33">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>α</mi>
     <mn>0</mn>
    </msub>
    <mo>+</mo>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mrow>
       <mo stretchy="false">|</mo>
       <mi>L</mi>
       <mo stretchy="false">|</mo>
      </mrow>
     </munderover>
     <mrow>
      <msub>
       <mi>α</mi>
       <mi>i</mi>
      </msub>
      <msub>
       <mi>K</mi>
       <mi>i</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>α</ci>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <abs></abs>
        <ci>L</ci>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>α</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>K</ci>
        <ci>i</ci>
       </apply>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=\alpha_{0}+\sum_{i=1}^{|L|}\alpha_{i}K_{i}(x)
  </annotation>
 </semantics>
</math>

 be the set of unlabeled data. Then, we can write the decision function as follows.</p>

<p>

<math display="block" id="Multiple_kernel_learning:34">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <munder>
      <mi>min</mi>
      <mi>f</mi>
     </munder>
     <mi>L</mi>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>f</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mi>λ</mi>
    <mi>R</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>f</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mi>γ</mi>
    <mi mathvariant="normal">Θ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>f</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <ci>f</ci>
      </apply>
      <ci>L</ci>
     </apply>
     <ci>f</ci>
    </apply>
    <apply>
     <times></times>
     <ci>λ</ci>
     <ci>R</ci>
     <ci>f</ci>
    </apply>
    <apply>
     <times></times>
     <ci>γ</ci>
     <ci>normal-Θ</ci>
     <ci>f</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{f}L(f)+\lambda R(f)+\gamma\Theta(f)
  </annotation>
 </semantics>
</math>

</p>

<p>The problem can be written as</p>

<p>

<math display="inline" id="Multiple_kernel_learning:35">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Multiple_kernel_learning:36">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

 is the loss function (weighted negative log-likelihood in this case), 

<math display="inline" id="Multiple_kernel_learning:37">
 <semantics>
  <mi mathvariant="normal">Θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Theta
  </annotation>
 </semantics>
</math>

 is the regularization parameter (<a href="Proximal_gradient_methods_for_learning#Exploiting_group_structure" title="wikilink">Group LASSO</a> in this case), and 

<math display="block" id="Multiple_kernel_learning:38">
 <semantics>
  <mrow>
   <msubsup>
    <mi>g</mi>
    <mi>m</mi>
    <mi>π</mi>
   </msubsup>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mo><</mo>
   <msubsup>
    <mi>ϕ</mi>
    <mi>m</mi>
    <mi>π</mi>
   </msubsup>
   <mo>,</mo>
   <msub>
    <mi>ψ</mi>
    <mi>m</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>></mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>g</ci>
      <ci>π</ci>
     </apply>
     <ci>m</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <lt></lt>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>ϕ</ci>
      <ci>π</ci>
     </apply>
     <ci>m</ci>
    </apply>
    <ci>normal-,</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ψ</ci>
     <ci>m</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <gt></gt>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g^{\pi}_{m}(x)=<\phi^{\pi}_{m},\psi_{m}(x)>
  </annotation>
 </semantics>
</math>

 is the conditional expectation consensus (CEC) penalty on unlabeled data. The CEC penalty is defined as follows. Let the marginal kernel density for all the data be</p>

<p>

<math display="inline" id="Multiple_kernel_learning:39">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>ψ</mi>
     <mi>m</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <msub>
       <mi>K</mi>
       <mi>m</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>x</mi>
        <mn>1</mn>
       </msub>
       <mo>,</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>,</mo>
     <mi mathvariant="normal">…</mi>
     <mo>,</mo>
     <mrow>
      <msub>
       <mi>K</mi>
       <mi>m</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>x</mi>
        <mi>L</mi>
       </msub>
       <mo>,</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
    <mi>T</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ψ</ci>
      <ci>m</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <list>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>K</ci>
        <ci>m</ci>
       </apply>
       <interval closure="open">
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <cn type="integer">1</cn>
        </apply>
        <ci>x</ci>
       </interval>
      </apply>
      <ci>normal-…</ci>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>K</ci>
        <ci>m</ci>
       </apply>
       <interval closure="open">
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>L</ci>
        </apply>
        <ci>x</ci>
       </interval>
      </apply>
     </list>
     <ci>T</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \psi_{m}(x)=[K_{m}(x_{1},x),\ldots,K_{m}(x_{L},x)]^{T}
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Multiple_kernel_learning:40">
 <semantics>
  <msubsup>
   <mi>ϕ</mi>
   <mi>m</mi>
   <mi>π</mi>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>ϕ</ci>
     <ci>π</ci>
    </apply>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \phi^{\pi}_{m}
  </annotation>
 </semantics>
</math>

 (the kernel distance between the labeled data and all of the labeled and unlabeled data) and 

<math display="inline" id="Multiple_kernel_learning:41">
 <semantics>
  <mi mathvariant="normal">Π</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Π</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Pi
  </annotation>
 </semantics>
</math>

 is a non-negative random vector with a 2-norm of 1. The value of 

<math display="inline" id="Multiple_kernel_learning:42">
 <semantics>
  <mrow>
   <msubsup>
    <mi>q</mi>
    <mi>m</mi>
    <mrow>
     <mi>p</mi>
     <mi>i</mi>
    </mrow>
   </msubsup>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo stretchy="false">|</mo>
    <msubsup>
     <mi>g</mi>
     <mi>m</mi>
     <mi>π</mi>
    </msubsup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>q</ci>
      <apply>
       <times></times>
       <ci>p</ci>
       <ci>i</ci>
      </apply>
     </apply>
     <ci>m</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>g</ci>
       <ci>π</ci>
      </apply>
      <ci>m</ci>
     </apply>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">x</csymbol>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q^{pi}_{m}(y|g^{\pi}_{m}(x))
  </annotation>
 </semantics>
</math>

 is the number of times each kernel is projected. Expectation regularization is then performed on the MKD, resulting in a reference expectation 

<math display="inline" id="Multiple_kernel_learning:43">
 <semantics>
  <mrow>
   <msubsup>
    <mi>p</mi>
    <mi>m</mi>
    <mi>π</mi>
   </msubsup>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">|</mo>
    <msubsup>
     <mi>g</mi>
     <mi>m</mi>
     <mi>π</mi>
    </msubsup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>p</ci>
      <ci>π</ci>
     </apply>
     <ci>m</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">f</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">x</csymbol>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>g</ci>
       <ci>π</ci>
      </apply>
      <ci>m</ci>
     </apply>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">x</csymbol>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p^{\pi}_{m}(f(x)|g^{\pi}_{m}(x))
  </annotation>
 </semantics>
</math>

 and model expectation 

<math display="block" id="Multiple_kernel_learning:44">
 <semantics>
  <mrow>
   <mi mathvariant="normal">Θ</mi>
   <mo>=</mo>
   <mfrac>
    <mn>1</mn>
    <mi mathvariant="normal">Π</mi>
   </mfrac>
   <munderover>
    <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
    <mrow>
     <mi>π</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi mathvariant="normal">Π</mi>
   </munderover>
   <munderover>
    <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
    <mrow>
     <mi>m</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>M</mi>
   </munderover>
   <mi>D</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msubsup>
     <mi>q</mi>
     <mi>m</mi>
     <mrow>
      <mi>p</mi>
      <mi>i</mi>
     </mrow>
    </msubsup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>y</mi>
     <mo stretchy="false">|</mo>
     <msubsup>
      <mi>g</mi>
      <mi>m</mi>
      <mi>π</mi>
     </msubsup>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">|</mo>
    <mo stretchy="false">|</mo>
    <msubsup>
     <mi>p</mi>
     <mi>m</mi>
     <mi>π</mi>
    </msubsup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo stretchy="false">|</mo>
     <msubsup>
      <mi>g</mi>
      <mi>m</mi>
      <mi>π</mi>
     </msubsup>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">Θ</csymbol>
    <eq></eq>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <ci>normal-Π</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <sum></sum>
      <ci>normal-Π</ci>
     </apply>
     <apply>
      <eq></eq>
      <ci>π</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <sum></sum>
      <ci>M</ci>
     </apply>
     <apply>
      <eq></eq>
      <ci>m</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <csymbol cd="unknown">D</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>q</ci>
       <apply>
        <times></times>
        <ci>p</ci>
        <ci>i</ci>
       </apply>
      </apply>
      <ci>m</ci>
     </apply>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">y</csymbol>
      <ci>normal-|</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>g</ci>
        <ci>π</ci>
       </apply>
       <ci>m</ci>
      </apply>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">x</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-|</ci>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>p</ci>
       <ci>π</ci>
      </apply>
      <ci>m</ci>
     </apply>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">f</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">x</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <ci>normal-|</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>g</ci>
        <ci>π</ci>
       </apply>
       <ci>m</ci>
      </apply>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">x</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Theta=\frac{1}{\Pi}\sum^{\Pi}_{\pi=1}\sum^{M}_{m=1}D(q^{pi}_{m}(y|g^{\pi}_{m}%
(x))||p^{\pi}_{m}(f(x)|g^{\pi}_{m}(x)))
  </annotation>
 </semantics>
</math>

. Then, we define</p>

<p>

<math display="inline" id="Multiple_kernel_learning:45">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>Q</mi>
    <mo stretchy="false">|</mo>
    <mo stretchy="false">|</mo>
    <mi>P</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">∑</mo>
    <mi>i</mi>
   </msub>
   <mi>Q</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>i</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>ln</mi>
   <mfrac>
    <mrow>
     <mi>Q</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">D</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">Q</csymbol>
     <ci>normal-|</ci>
     <ci>normal-|</ci>
     <csymbol cd="unknown">P</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <ci>i</ci>
    </apply>
    <csymbol cd="unknown">Q</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">i</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ln></ln>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>Q</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <times></times>
      <ci>P</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D(Q||P)=\sum_{i}Q(i)\ln\frac{Q(i)}{P(i)}
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Multiple_kernel_learning:46">
 <semantics>
  <mrow>
   <mi>U</mi>
   <mo>=</mo>
   <msub>
    <mi>x</mi>
    <mi>i</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>U</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>i</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   U={x_{i}}
  </annotation>
 </semantics>
</math>

 is the <a href="Kullback-Leibler_divergence" title="wikilink">Kullback-Leibler divergence</a>. The combined minimization problem is optimized using a modified block gradient descent algorithm. For more information, see Wang et al.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<h3 id="unsupervised-learning">Unsupervised learning</h3>

<p><a href="Unsupervised_learning" title="wikilink">Unsupervised</a> multiple kernel learning algorithms have also been proposed by Zhuang et al. The problem is defined as follows. Let 

<math display="inline" id="Multiple_kernel_learning:47">
 <semantics>
  <mrow>
   <msup>
    <mi>K</mi>
    <mo>′</mo>
   </msup>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>M</mi>
    </msubsup>
    <mrow>
     <msub>
      <mi>β</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>K</mi>
      <mi>m</mi>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>K</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>M</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>β</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>K</ci>
       <ci>m</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K^{\prime}=\sum_{i=1}^{M}\beta_{i}K_{m}
  </annotation>
 </semantics>
</math>

 be a set of unlabeled data. The kernel definition is the linear combined kernel 

<math display="inline" id="Multiple_kernel_learning:48">
 <semantics>
  <msub>
   <mi>B</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>B</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B_{i}
  </annotation>
 </semantics>
</math>

. In this problem, the data needs to be "clustered" into groups based on the kernel distances. Let 

<math display="inline" id="Multiple_kernel_learning:49">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}
  </annotation>
 </semantics>
</math>

 be a the group or cluster of which 

<math display="inline" id="Multiple_kernel_learning:50">
 <semantics>
  <mrow>
   <msubsup>
    <mo largeop="true" symmetric="true">∑</mo>
    <mrow>
     <mi>i</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>n</mi>
   </msubsup>
   <msup>
    <mrow>
     <mo>∥</mo>
     <mrow>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo>-</mo>
      <mrow>
       <msub>
        <mo largeop="true" symmetric="true">∑</mo>
        <mrow>
         <msub>
          <mi>x</mi>
          <mi>j</mi>
         </msub>
         <mo>∈</mo>
         <msub>
          <mi>B</mi>
          <mi>i</mi>
         </msub>
        </mrow>
       </msub>
       <mrow>
        <mi>K</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <msub>
          <mi>x</mi>
          <mi>i</mi>
         </msub>
         <mo>,</mo>
         <msub>
          <mi>x</mi>
          <mi>j</mi>
         </msub>
         <mo stretchy="false">)</mo>
        </mrow>
        <msub>
         <mi>x</mi>
         <mi>j</mi>
        </msub>
       </mrow>
      </mrow>
     </mrow>
     <mo>∥</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <sum></sum>
      <ci>n</ci>
     </apply>
     <apply>
      <eq></eq>
      <ci>i</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <apply>
       <minus></minus>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <in></in>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>j</ci>
          </apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>B</ci>
           <ci>i</ci>
          </apply>
         </apply>
        </apply>
        <apply>
         <times></times>
         <ci>K</ci>
         <interval closure="open">
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>i</ci>
          </apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>j</ci>
          </apply>
         </interval>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>j</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum^{n}_{i=1}\left\|x_{i}-\sum_{x_{j}\in B_{i}}K(x_{i},x_{j})x_{j}\right\|^{2}
  </annotation>
 </semantics>
</math>

 is a member. We define the loss function as 

<math display="inline" id="Multiple_kernel_learning:51">
 <semantics>
  <mrow>
   <msubsup>
    <mo largeop="true" symmetric="true">∑</mo>
    <mrow>
     <mi>i</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>n</mi>
   </msubsup>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <msub>
       <mi>x</mi>
       <mi>j</mi>
      </msub>
      <mo>∈</mo>
      <msub>
       <mi>B</mi>
       <mi>i</mi>
      </msub>
     </mrow>
    </msub>
    <mrow>
     <mi>K</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>x</mi>
       <mi>j</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <msup>
      <mrow>
       <mo>∥</mo>
       <mrow>
        <msub>
         <mi>x</mi>
         <mi>i</mi>
        </msub>
        <mo>-</mo>
        <msub>
         <mi>x</mi>
         <mi>j</mi>
        </msub>
       </mrow>
       <mo>∥</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <eq></eq>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>n</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <in></in>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>j</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>B</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>K</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>j</ci>
       </apply>
      </interval>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <apply>
         <minus></minus>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>j</ci>
         </apply>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{i=1}^{n}\sum_{x_{j}\in B_{i}}K(x_{i},x_{j})\left\|x_{i}-x_{j}\right\|^{2}
  </annotation>
 </semantics>
</math>

. Furthermore, we minimize the distortion by minimizing 

<math display="block" id="Multiple_kernel_learning:52">
 <semantics>
  <mrow>
   <mrow>
    <munder>
     <mi>min</mi>
     <mrow>
      <mi>β</mi>
      <mo>,</mo>
      <mi>B</mi>
     </mrow>
    </munder>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </munderover>
     <msup>
      <mrow>
       <mo>∥</mo>
       <mrow>
        <msub>
         <mi>x</mi>
         <mi>i</mi>
        </msub>
        <mo>-</mo>
        <mrow>
         <munder>
          <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
          <mrow>
           <msub>
            <mi>x</mi>
            <mi>j</mi>
           </msub>
           <mo>∈</mo>
           <msub>
            <mi>B</mi>
            <mi>i</mi>
           </msub>
          </mrow>
         </munder>
         <mrow>
          <mi>K</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <msub>
            <mi>x</mi>
            <mi>i</mi>
           </msub>
           <mo>,</mo>
           <msub>
            <mi>x</mi>
            <mi>j</mi>
           </msub>
           <mo stretchy="false">)</mo>
          </mrow>
          <msub>
           <mi>x</mi>
           <mi>j</mi>
          </msub>
         </mrow>
        </mrow>
       </mrow>
       <mo>∥</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <msub>
     <mi>γ</mi>
     <mn>1</mn>
    </msub>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </munderover>
     <mrow>
      <munder>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <msub>
         <mi>x</mi>
         <mi>j</mi>
        </msub>
        <mo>∈</mo>
        <msub>
         <mi>B</mi>
         <mi>i</mi>
        </msub>
       </mrow>
      </munder>
      <mrow>
       <mi>K</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi>x</mi>
         <mi>i</mi>
        </msub>
        <mo>,</mo>
        <msub>
         <mi>x</mi>
         <mi>j</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
       <msup>
        <mrow>
         <mo>∥</mo>
         <mrow>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo>-</mo>
          <msub>
           <mi>x</mi>
           <mi>j</mi>
          </msub>
         </mrow>
         <mo>∥</mo>
        </mrow>
        <mn>2</mn>
       </msup>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <msub>
     <mi>γ</mi>
     <mn>2</mn>
    </msub>
    <mrow>
     <munder>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mi>i</mi>
     </munder>
     <mrow>
      <mo stretchy="false">|</mo>
      <msub>
       <mi>B</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">|</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <min></min>
      <list>
       <ci>β</ci>
       <ci>B</ci>
      </list>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <sum></sum>
        <ci>n</ci>
       </apply>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <apply>
         <minus></minus>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <sum></sum>
           <apply>
            <in></in>
            <apply>
             <csymbol cd="ambiguous">subscript</csymbol>
             <ci>x</ci>
             <ci>j</ci>
            </apply>
            <apply>
             <csymbol cd="ambiguous">subscript</csymbol>
             <ci>B</ci>
             <ci>i</ci>
            </apply>
           </apply>
          </apply>
          <apply>
           <times></times>
           <ci>K</ci>
           <interval closure="open">
            <apply>
             <csymbol cd="ambiguous">subscript</csymbol>
             <ci>x</ci>
             <ci>i</ci>
            </apply>
            <apply>
             <csymbol cd="ambiguous">subscript</csymbol>
             <ci>x</ci>
             <ci>j</ci>
            </apply>
           </interval>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>x</ci>
            <ci>j</ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>γ</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <in></in>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>j</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>B</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
       <apply>
        <times></times>
        <ci>K</ci>
        <interval closure="open">
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>j</ci>
         </apply>
        </interval>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="latexml">norm</csymbol>
          <apply>
           <minus></minus>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>x</ci>
            <ci>i</ci>
           </apply>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>x</ci>
            <ci>j</ci>
           </apply>
          </apply>
         </apply>
         <cn type="integer">2</cn>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>γ</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>i</ci>
      </apply>
      <apply>
       <abs></abs>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>B</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{\beta,B}\sum^{n}_{i=1}\left\|x_{i}-\sum_{x_{j}\in B_{i}}K(x_{i},x_{j})x_%
{j}\right\|^{2}+\gamma_{1}\sum_{i=1}^{n}\sum_{x_{j}\in B_{i}}K(x_{i},x_{j})%
\left\|x_{i}-x_{j}\right\|^{2}+\gamma_{2}\sum_{i}|B_{i}|
  </annotation>
 </semantics>
</math>

. Finally, we add a regularization term to avoid overfitting. Combining these terms, we can write the minimization problem as follows.</p>

<p>

<math display="inline" id="Multiple_kernel_learning:53">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>∈</mo>
   <mrow>
    <mn>0</mn>
    <mo>,</mo>
    <msup>
     <mn>1</mn>
     <mrow>
      <mi>n</mi>
      <mo>×</mo>
      <mi>n</mi>
     </mrow>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>D</ci>
    <list>
     <cn type="integer">0</cn>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <cn type="integer">1</cn>
      <apply>
       <times></times>
       <ci>n</ci>
       <ci>n</ci>
      </apply>
     </apply>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D\in{0,1}^{n\times n}
  </annotation>
 </semantics>
</math>

</p>

<p>where . One formulation of this is defined as follows. Let 

<math display="inline" id="Multiple_kernel_learning:54">
 <semantics>
  <mrow>
   <msub>
    <mi>D</mi>
    <mrow>
     <mi>i</mi>
     <mi>j</mi>
    </mrow>
   </msub>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>D</ci>
     <apply>
      <times></times>
      <ci>i</ci>
      <ci>j</ci>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{ij}=1
  </annotation>
 </semantics>
</math>

 be a matrix such that 

<math display="inline" id="Multiple_kernel_learning:55">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}
  </annotation>
 </semantics>
</math>

 means that 

<math display="inline" id="Multiple_kernel_learning:56">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{j}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Multiple_kernel_learning:57">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>B</mi>
     <mi>i</mi>
    </msub>
    <mo>=</mo>
    <msub>
     <mi>x</mi>
     <mi>j</mi>
    </msub>
   </mrow>
   <mo>:</mo>
   <mrow>
    <msub>
     <mi>D</mi>
     <mrow>
      <mi>i</mi>
      <mi>j</mi>
     </mrow>
    </msub>
    <mo>=</mo>
    <mn>1</mn>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>B</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>j</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>D</ci>
      <apply>
       <times></times>
       <ci>i</ci>
       <ci>j</ci>
      </apply>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B_{i}={x_{j}:D_{ij}=1}
  </annotation>
 </semantics>
</math>

 are neighbors. Then, 

<math display="inline" id="Multiple_kernel_learning:58">
 <semantics>
  <mi>K</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>K</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K
  </annotation>
 </semantics>
</math>

. Note that these groups must be learned as well. Zhuang et al. solve this problem by an alternating minimization method for 

<math display="inline" id="Multiple_kernel_learning:59">
 <semantics>
  <msub>
   <mi>B</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>B</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B_{i}
  </annotation>
 </semantics>
</math>

 and the groups 

<math display="inline" id="Multiple_kernel_learning:60">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{1}
  </annotation>
 </semantics>
</math>

. For more information, see Zhuang et al.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></p>
<h2 id="mkl-libraries">MKL Libraries</h2>

<p>Available MKL libraries include</p>
<ul>
<li><a href="http://www.cs.cornell.edu/~ashesh/pubs/code/SPG-GMKL/download.html">SPG-GMKL</a>: A scalable C++ MKL SVM library that can handle a million kernels.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></li>
<li><a href="http://research.microsoft.com/en-us/um/people/manik/code/GMKL/download.html">GMKL</a>: Generalized Multiple Kernel Learning code in <a class="uri" href="MATLAB" title="wikilink">MATLAB</a>, does 

<math display="inline" id="Multiple_kernel_learning:61">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{2}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Multiple_kernel_learning:62">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 regularization for supervised learning.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></li>
<li><a href="http://appsrv.cse.cuhk.edu.hk/~hqyang/doku.php?id=gmkl">(Another) GMKL</a>: A different MATLAB MKL code that can also perform elastic net regularization<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a></li>
<li><a href="http://research.microsoft.com/en-us/um/people/manik/code/smo-mkl/download.html">SMO-MKL</a>: C++ source code for a Sequential Minimal Optimization MKL algorithm. Does <span class="LaTeX">$p$</span>-n orm regularization.<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a></li>
<li><a href="http://asi.insa-rouen.fr/enseignants/~arakoto/code/mklindex.html">SimpleMKL</a>: A MATLAB code based on the SimpleMKL algorithm for MKL SVM.<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Machine_learning_algorithms" title="wikilink">Category:Machine learning algorithms</a> <a href="Category:Data_mining" title="wikilink">Category:Data mining</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Lin Chen, Lixin Duan, and Dong Xu, "Event Recognition in Videos by Learning From Heterogeneous Web Sources," in IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2013, pp. 2666-2673<a href="#fnref1">↩</a></li>
<li id="fn2">Serhat S. Bucak, Rong Jin, and Anil K. Jain, Multiple Kernel Learning for Visual Object Recognition: A Review. T-PAMI, 2013.<a href="#fnref2">↩</a></li>
<li id="fn3">Yu et al. <a href="http://www.biomedcentral.com/1471-2105/11/309">L2-norm multiple kernel learning and its application to biomedical data fusion</a>. BMC Bioinformatics 2010, 11:309<a href="#fnref3">↩</a></li>
<li id="fn4">Francis R. Bach, Gert R. G. Lanckriet, and Michael I. Jordan. 2004. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2004_BachLJ04.pdf">Multiple kernel learning, conic duality, and the SMO algorithm</a>. In Proceedings of the twenty-first international conference on Machine learning (ICML '04). ACM, New York, NY, USA<a href="#fnref4">↩</a></li>
<li id="fn5">Mehmet Gönen, Ethem Alpaydın. <a href="http://www.jmlr.org/papers/volume12/gonen11a/gonen11a.pdf">Multiple Kernel Learning Algorithms</a> Jour. Mach. Learn. Res. 12(Jul):2211−2268, 2011<a href="#fnref5">↩</a></li>
<li id="fn6">Ben-Hur, A. and Noble W.S. <a href="http://www.ncbi.nlm.nih.gov/pubmed/15961482?dopt=Abstract">Kernel methods for predicting protein-protein interactions.</a> Bioinformatics. 2005 Jun;21 Suppl 1:i38-46.<a href="#fnref6">↩</a></li>
<li id="fn7">Hiroaki Tanabe, Tu Bao Ho, Canh Hao Nguyen, and Saori Kawasaki. Simple but effective methods for combining kernels in computational biology. In Proceedings of IEEE International Conference on Research, Innovation and Vision for the Future, 2008.<a href="#fnref7">↩</a></li>
<li id="fn8">Shibin Qiu and Terran Lane. A framework for multiple kernel support vector regression and its applications to siRNA efficacy prediction. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 6(2):190–199, 2009<a href="#fnref8">↩</a></li>
<li id="fn9">Gert R. G. Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui, and Michael I. Jordan. Learning the kernel matrix with semidefinite programming. Journal of Machine Learning Research, 5:27–72, 2004a<a href="#fnref9">↩</a></li>
<li id="fn10">Gert R. G. Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui, and Michael I. Jordan. Learning the kernel matrix with semidefinite programming. In Proceedings of the 19th International Conference on Machine Learning, 2002<a href="#fnref10">↩</a></li>
<li id="fn11">Mark Girolami and Simon Rogers. Hierarchic Bayesian models for kernel learning. In Proceedings of the 22nd International Conference on Machine Learning, 2005<a href="#fnref11">↩</a></li>
<li id="fn12">Theodoros Damoulas and Mark A. Girolami. Combining feature spaces for classification. Pattern Recognition, 42(11):2671–2683, 2009<a href="#fnref12">↩</a></li>
<li id="fn13">Theodoros Damoulas and Mark A. Girolami. Probabilistic multi-class multi-kernel learning: On protein fold recognition and remote homology detection. Bioinformatics, 24(10):1264–1270, 2008<a href="#fnref13">↩</a></li>
<li id="fn14">Kristin P. Bennett, Michinari Momma, and Mark J. Embrechts. MARK: A boosting algorithm for heterogeneous kernel models. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2002<a href="#fnref14">↩</a></li>
<li id="fn15">Wang, Shuhui et al. <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6177671">S3MKL: Scalable Semi-Supervised Multiple Kernel Learning for Real-World Image Applications</a>. IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 14, NO. 4, AUGUST 2012<a href="#fnref15">↩</a></li>
<li id="fn16">J. Zhuang, J. Wang, S.C.H. Hoi &amp; X. Lan. <a href="http://jmlr.csail.mit.edu/proceedings/papers/v20/zhuang11/zhuang11.pdf">Unsupervised Multiple Kernel Learning</a>. Jour. Mach. Learn. Res. 20:129–144, 2011<a href="#fnref16">↩</a></li>
<li id="fn17">Ashesh Jain, S. V. N. Vishwanathan and Manik Varma. SPG-GMKL: Generalized multiple kernel learning with a million kernels. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Beijing, China, August 2012<a href="#fnref17">↩</a></li>
<li id="fn18">M. Varma and B. R. Babu. More generality in efficient multiple kernel learning. In Proceedings of the International Conference on Machine Learning, Montreal, Canada, June 2009<a href="#fnref18">↩</a></li>
<li id="fn19">Yang, H., Xu, Z., Ye, J., King, I., &amp; Lyu, M. R. (2011). Efficient Sparse Generalized Multiple Kernel Learning. IEEE Transactions on Neural Networks, 22(3), 433-446<a href="#fnref19">↩</a></li>
<li id="fn20">S. V. N. Vishwanathan, Z. Sun, N. Theera-Ampornpunt and M. Varma. Multiple kernel learning and the SMO algorithm. In Advances in Neural Information Processing Systems, Vancouver, B. C., Canada, December 2010.<a href="#fnref20">↩</a></li>
<li id="fn21">Alain Rakotomamonjy, Francis Bach, Stephane Canu, Yves Grandvalet. SimpleMKL. Journal of Machine Learning Research, Microtome Publishing, 2008, 9, pp.2491-2521.<a href="#fnref21">↩</a></li>
</ol>
</section>
</body>
</html>
