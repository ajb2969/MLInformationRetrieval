<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="587">Linear regression</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Linear regression</h1>
<hr/>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>linear regression</strong> is an approach for modeling the relationship between a scalar <a href="dependent_variable" title="wikilink">dependent variable</a> <em>y</em> and one or more <a href="explanatory_variable" title="wikilink">explanatory variables</a> (or independent variable) denoted <em>X</em>. The case of one explanatory variable is called <em><a href="simple_linear_regression" title="wikilink">simple linear regression</a></em>. For more than one explanatory variable, the process is called <em>multiple linear regression</em>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> (This term should be distinguished from <em><a href="multivariate_linear_regression" title="wikilink">multivariate linear regression</a></em>, where multiple correlated dependent variables are predicted, rather than a single scalar variable.)<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>In linear regression, <a class="uri" href="data" title="wikilink">data</a> are modeled using <a href="linear_predictor_function" title="wikilink">linear predictor functions</a>, and unknown model <a class="uri" href="parameters" title="wikilink">parameters</a> are <a href="estimation_theory" title="wikilink">estimated</a> from the data. Such models are called <em><a href="linear_model" title="wikilink">linear models</a></em>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Most commonly, linear regression refers to a model in which the <a href="conditional_expectation" title="wikilink">conditional mean</a> of <em>y</em> given the value of <em>X</em> is an <a href="affine_transformation" title="wikilink">affine function</a> of <em>X</em>. Less commonly, linear regression could refer to a model in which the <a class="uri" href="median" title="wikilink">median</a>, or some other <a class="uri" href="quantile" title="wikilink">quantile</a> of the conditional distribution of <em>y</em> given <em>X</em> is expressed as a linear function of <em>X</em>. Like all forms of <a href="regression_analysis" title="wikilink">regression analysis</a>, <em>linear regression</em> focuses on the <a href="conditional_probability_distribution" title="wikilink">conditional probability distribution</a> of <em>y</em> given <em>X</em>, rather than on the <a href="joint_probability_distribution" title="wikilink">joint probability distribution</a> of <em>y</em> and <em>X</em>, which is the domain of <a href="multivariate_analysis" title="wikilink">multivariate analysis</a>.</p>

<p>Linear regression was the first type of <a href="regression_analysis" title="wikilink">regression analysis</a> to be studied rigorously, and to be used extensively in practical applications.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.</p>

<p>Linear regression has many practical uses. Most applications fall into one of the following two broad categories:</p>
<ul>
<li>If the goal is prediction, or forecasting, or reduction, linear regression can be used to fit a predictive model to an observed data set of <em>y</em> and <em>X</em> values. After developing such a model, if an additional value of <em>X</em> is then given without its accompanying value of <em>y</em>, the fitted model can be used to make a prediction of the value of <em>y</em>.</li>
<li>Given a variable <em>y</em> and a number of variables <em>X</em><sub>1</sub>, ..., <em>X</em><sub><em>p</em></sub> that may be related to <em>y</em>, linear regression analysis can be applied to quantify the strength of the relationship between <em>y</em> and the <em>X</em><sub><em>j</em></sub>, to assess which <em>X</em><sub><em>j</em></sub> may have no relationship with <em>y</em> at all, and to identify which subsets of the <em>X</em><sub><em>j</em></sub> contain redundant information about <em>y</em>.</li>
</ul>

<p>Linear regression models are often fitted using the <a href="least_squares" title="wikilink">least squares</a> approach, but they may also be fitted in other ways, such as by minimizing the "lack of fit" in some other <a href="norm_(mathematics)" title="wikilink">norm</a> (as with <a href="least_absolute_deviations" title="wikilink">least absolute deviations</a> regression), or by minimizing a penalized version of the least squares <a href="loss_function" title="wikilink">loss function</a> as in <a href="ridge_regression" title="wikilink">ridge regression</a> (L2-norm penalty) and <a href="Least_squares#Lasso_method" title="wikilink">lasso</a> (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms "least squares" and "linear model" are closely linked, they are not synonymous.</p>
<h2 id="introduction-to-linear-regression">Introduction to linear regression</h2>

<p>  Given a <a class="uri" href="data" title="wikilink">data</a> set 

<math display="inline" id="Linear_regression:0">
 <semantics>
  <msubsup>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mi>y</mi>
     <mi>i</mi>
    </msub>
    <mo rspace="4.2pt">,</mo>
    <msub>
     <mi>x</mi>
     <mrow>
      <mi>i</mi>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">‚Ä¶</mi>
    <mo>,</mo>
    <msub>
     <mi>x</mi>
     <mrow>
      <mi>i</mi>
      <mi>p</mi>
     </mrow>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
   <mrow>
    <mi>i</mi>
    <mo>=</mo>
    <mn>1</mn>
   </mrow>
   <mi>n</mi>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <set>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <apply>
        <times></times>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>normal-‚Ä¶</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <apply>
        <times></times>
        <ci>i</ci>
        <ci>p</ci>
       </apply>
      </apply>
     </set>
     <apply>
      <eq></eq>
      <ci>i</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{y_{i},\,x_{i1},\ldots,x_{ip}\}_{i=1}^{n}
  </annotation>
 </semantics>
</math>

 of <em>n</em> <a href="statistical_unit" title="wikilink">statistical units</a>, a linear regression model assumes that the relationship between the dependent variable <em>y<sub>i</sub></em> and the <em>p</em>-vector of regressors <em>x<sub>i</sub></em> is <a href="linear_function" title="wikilink">linear</a>. This relationship is modeled through a <em>disturbance term</em> or <em>error variable</em> <em>Œµ<sub>i</sub></em> ‚Äî an unobserved <a href="random_variable" title="wikilink">random variable</a> that adds noise to the linear relationship between the dependent variable and regressors. Thus the model takes the form</p>

<p>

<math display="block" id="Linear_regression:1">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mi>y</mi>
      <mi>i</mi>
     </msub>
     <mo>=</mo>
     <mrow>
      <mrow>
       <msub>
        <mi>Œ≤</mi>
        <mn>1</mn>
       </msub>
       <msub>
        <mi>x</mi>
        <mrow>
         <mi>i</mi>
         <mn>1</mn>
        </mrow>
       </msub>
      </mrow>
      <mo>+</mo>
      <mi mathvariant="normal">‚ãØ</mi>
      <mo>+</mo>
      <mrow>
       <msub>
        <mi>Œ≤</mi>
        <mi>p</mi>
       </msub>
       <msub>
        <mi>x</mi>
        <mrow>
         <mi>i</mi>
         <mi>p</mi>
        </mrow>
       </msub>
      </mrow>
      <mo>+</mo>
      <msub>
       <mi>Œµ</mi>
       <mi>i</mi>
      </msub>
     </mrow>
     <mo>=</mo>
     <mrow>
      <mrow>
       <msubsup>
        <mi>ùê±</mi>
        <mi>i</mi>
        <mi mathvariant="normal">T</mi>
       </msubsup>
       <mi>ùú∑</mi>
      </mrow>
      <mo>+</mo>
      <msub>
       <mi>Œµ</mi>
       <mi>i</mi>
      </msub>
     </mrow>
    </mrow>
    <mo rspace="22.5pt">,</mo>
    <mrow>
     <mi>i</mi>
     <mo>=</mo>
     <mrow>
      <mn>1</mn>
      <mo>,</mo>
      <mi mathvariant="normal">‚Ä¶</mi>
      <mo>,</mo>
      <mi>n</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <and></and>
     <apply>
      <eq></eq>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <plus></plus>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>Œ≤</ci>
         <cn type="integer">1</cn>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <apply>
          <times></times>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </apply>
       <ci>normal-‚ãØ</ci>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>Œ≤</ci>
         <ci>p</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <apply>
          <times></times>
          <ci>i</ci>
          <ci>p</ci>
         </apply>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Œµ</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <eq></eq>
      <share href="#.cmml">
      </share>
      <apply>
       <plus></plus>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>ùê±</ci>
          <ci>normal-T</ci>
         </apply>
         <ci>i</ci>
        </apply>
        <ci>ùú∑</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Œµ</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <ci>i</ci>
     <list>
      <cn type="integer">1</cn>
      <ci>normal-‚Ä¶</ci>
      <ci>n</ci>
     </list>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}=\beta_{1}x_{i1}+\cdots+\beta_{p}x_{ip}+\varepsilon_{i}=\mathbf{x}^{\rm T%
}_{i}\boldsymbol{\beta}+\varepsilon_{i},\qquad i=1,\ldots,n,
  </annotation>
 </semantics>
</math>

 where <sup>T</sup> denotes the <a class="uri" href="transpose" title="wikilink">transpose</a>, so that <strong><em>x<strong><sub>i</sub><em><sup>T</sup></em></strong>Œ≤</em></strong> is the <a href="inner_product" title="wikilink">inner product</a> between <a href="coordinate_vector" title="wikilink">vectors</a> <em><strong>x</strong><sub>i</sub></em> and <strong><em>Œ≤</em></strong>.</p>

<p>Often these <em>n</em> equations are stacked together and written in vector form as</p>

<p>

<math display="block" id="Linear_regression:2">
 <semantics>
  <mrow>
   <mrow>
    <mi>ùê≤</mi>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mi>ùêó</mi>
      <mi>ùú∑</mi>
     </mrow>
     <mo>+</mo>
     <mi>ùú∫</mi>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>ùê≤</ci>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <ci>ùêó</ci>
      <ci>ùú∑</ci>
     </apply>
     <ci>ùú∫</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon},\,
  </annotation>
 </semantics>
</math>

 where</p>

<p>

<math display="block" id="Linear_regression:3">
 <semantics>
  <mrow>
   <mrow>
    <mi>ùê≤</mi>
    <mo>=</mo>
    <mrow>
     <mo>(</mo>
     <mtable displaystyle="true">
      <mtr>
       <mtd columnalign="center">
        <msub>
         <mi>y</mi>
         <mn>1</mn>
        </msub>
       </mtd>
      </mtr>
      <mtr>
       <mtd columnalign="center">
        <msub>
         <mi>y</mi>
         <mn>2</mn>
        </msub>
       </mtd>
      </mtr>
      <mtr>
       <mtd columnalign="center">
        <mi mathvariant="normal">‚ãÆ</mi>
       </mtd>
      </mtr>
      <mtr>
       <mtd columnalign="center">
        <msub>
         <mi>y</mi>
         <mi>n</mi>
        </msub>
       </mtd>
      </mtr>
     </mtable>
     <mo>)</mo>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>ùê≤</ci>
    <matrix>
     <matrixrow>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <cn type="integer">1</cn>
      </apply>
     </matrixrow>
     <matrixrow>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <cn type="integer">2</cn>
      </apply>
     </matrixrow>
     <matrixrow>
      <ci>normal-‚ãÆ</ci>
     </matrixrow>
     <matrixrow>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>n</ci>
      </apply>
     </matrixrow>
    </matrix>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{y}=\begin{pmatrix}y_{1}\\
y_{2}\\
\vdots\\
y_{n}\end{pmatrix},\quad
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Linear_regression:4">
 <semantics>
  <mrow>
   <mrow>
    <mi>ùêó</mi>
    <mo>=</mo>
    <mrow>
     <mo>(</mo>
     <mtable displaystyle="true">
      <mtr>
       <mtd columnalign="center">
        <msubsup>
         <mi>ùê±</mi>
         <mn>1</mn>
         <mi mathvariant="normal">T</mi>
        </msubsup>
       </mtd>
      </mtr>
      <mtr>
       <mtd columnalign="center">
        <msubsup>
         <mi>ùê±</mi>
         <mn>2</mn>
         <mi mathvariant="normal">T</mi>
        </msubsup>
       </mtd>
      </mtr>
      <mtr>
       <mtd columnalign="center">
        <mi mathvariant="normal">‚ãÆ</mi>
       </mtd>
      </mtr>
      <mtr>
       <mtd columnalign="center">
        <msubsup>
         <mi>ùê±</mi>
         <mi>n</mi>
         <mi mathvariant="normal">T</mi>
        </msubsup>
       </mtd>
      </mtr>
     </mtable>
     <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mo>(</mo>
     <mtable displaystyle="true">
      <mtr>
       <mtd columnalign="center">
        <msub>
         <mi>x</mi>
         <mn>11</mn>
        </msub>
       </mtd>
       <mtd columnalign="center">
        <mi mathvariant="normal">‚ãØ</mi>
       </mtd>
       <mtd columnalign="center">
        <msub>
         <mi>x</mi>
         <mrow>
          <mn>1</mn>
          <mi>p</mi>
         </mrow>
        </msub>
       </mtd>
      </mtr>
      <mtr>
       <mtd columnalign="center">
        <msub>
         <mi>x</mi>
         <mn>21</mn>
        </msub>
       </mtd>
       <mtd columnalign="center">
        <mi mathvariant="normal">‚ãØ</mi>
       </mtd>
       <mtd columnalign="center">
        <msub>
         <mi>x</mi>
         <mrow>
          <mn>2</mn>
          <mi>p</mi>
         </mrow>
        </msub>
       </mtd>
      </mtr>
      <mtr>
       <mtd columnalign="center">
        <mi mathvariant="normal">‚ãÆ</mi>
       </mtd>
       <mtd columnalign="center">
        <mi mathvariant="normal">‚ã±</mi>
       </mtd>
       <mtd columnalign="center">
        <mi mathvariant="normal">‚ãÆ</mi>
       </mtd>
      </mtr>
      <mtr>
       <mtd columnalign="center">
        <msub>
         <mi>x</mi>
         <mrow>
          <mi>n</mi>
          <mn>1</mn>
         </mrow>
        </msub>
       </mtd>
       <mtd columnalign="center">
        <mi mathvariant="normal">‚ãØ</mi>
       </mtd>
       <mtd columnalign="center">
        <msub>
         <mi>x</mi>
         <mrow>
          <mi>n</mi>
          <mi>p</mi>
         </mrow>
        </msub>
       </mtd>
      </mtr>
     </mtable>
     <mo>)</mo>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>ùêó</ci>
     <matrix>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>ùê±</ci>
         <ci>normal-T</ci>
        </apply>
        <cn type="integer">1</cn>
       </apply>
      </matrixrow>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>ùê±</ci>
         <ci>normal-T</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </matrixrow>
      <matrixrow>
       <ci>normal-‚ãÆ</ci>
      </matrixrow>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>ùê±</ci>
         <ci>normal-T</ci>
        </apply>
        <ci>n</ci>
       </apply>
      </matrixrow>
     </matrix>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <matrix>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <cn type="integer">11</cn>
       </apply>
       <ci>normal-‚ãØ</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">1</cn>
         <ci>p</ci>
        </apply>
       </apply>
      </matrixrow>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <cn type="integer">21</cn>
       </apply>
       <ci>normal-‚ãØ</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <cn type="integer">2</cn>
         <ci>p</ci>
        </apply>
       </apply>
      </matrixrow>
      <matrixrow>
       <ci>normal-‚ãÆ</ci>
       <ci>normal-‚ã±</ci>
       <ci>normal-‚ãÆ</ci>
      </matrixrow>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <ci>n</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>normal-‚ãØ</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <apply>
         <times></times>
         <ci>n</ci>
         <ci>p</ci>
        </apply>
       </apply>
      </matrixrow>
     </matrix>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{X}=\begin{pmatrix}\mathbf{x}^{\rm T}_{1}\\
\mathbf{x}^{\rm T}_{2}\\
\vdots\\
\mathbf{x}^{\rm T}_{n}\end{pmatrix}=\begin{pmatrix}x_{11}&\cdots&x_{1p}\\
x_{21}&\cdots&x_{2p}\\
\vdots&\ddots&\vdots\\
x_{n1}&\cdots&x_{np}\end{pmatrix},
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Linear_regression:5">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>ùú∑</mi>
     <mo>=</mo>
     <mrow>
      <mo>(</mo>
      <mtable displaystyle="true">
       <mtr>
        <mtd columnalign="center">
         <msub>
          <mi>Œ≤</mi>
          <mn>1</mn>
         </msub>
        </mtd>
       </mtr>
       <mtr>
        <mtd columnalign="center">
         <msub>
          <mi>Œ≤</mi>
          <mn>2</mn>
         </msub>
        </mtd>
       </mtr>
       <mtr>
        <mtd columnalign="center">
         <mi mathvariant="normal">‚ãÆ</mi>
        </mtd>
       </mtr>
       <mtr>
        <mtd columnalign="center">
         <msub>
          <mi>Œ≤</mi>
          <mi>p</mi>
         </msub>
        </mtd>
       </mtr>
      </mtable>
      <mo>)</mo>
     </mrow>
    </mrow>
    <mo rspace="12.5pt">,</mo>
    <mrow>
     <mi>ùú∫</mi>
     <mo>=</mo>
     <mrow>
      <mo>(</mo>
      <mtable displaystyle="true">
       <mtr>
        <mtd columnalign="center">
         <msub>
          <mi>Œµ</mi>
          <mn>1</mn>
         </msub>
        </mtd>
       </mtr>
       <mtr>
        <mtd columnalign="center">
         <msub>
          <mi>Œµ</mi>
          <mn>2</mn>
         </msub>
        </mtd>
       </mtr>
       <mtr>
        <mtd columnalign="center">
         <mi mathvariant="normal">‚ãÆ</mi>
        </mtd>
       </mtr>
       <mtr>
        <mtd columnalign="center">
         <msub>
          <mi>Œµ</mi>
          <mi>n</mi>
         </msub>
        </mtd>
       </mtr>
      </mtable>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <ci>ùú∑</ci>
     <matrix>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Œ≤</ci>
        <cn type="integer">1</cn>
       </apply>
      </matrixrow>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Œ≤</ci>
        <cn type="integer">2</cn>
       </apply>
      </matrixrow>
      <matrixrow>
       <ci>normal-‚ãÆ</ci>
      </matrixrow>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Œ≤</ci>
        <ci>p</ci>
       </apply>
      </matrixrow>
     </matrix>
    </apply>
    <apply>
     <eq></eq>
     <ci>ùú∫</ci>
     <matrix>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Œµ</ci>
        <cn type="integer">1</cn>
       </apply>
      </matrixrow>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Œµ</ci>
        <cn type="integer">2</cn>
       </apply>
      </matrixrow>
      <matrixrow>
       <ci>normal-‚ãÆ</ci>
      </matrixrow>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Œµ</ci>
        <ci>n</ci>
       </apply>
      </matrixrow>
     </matrix>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \boldsymbol{\beta}=\begin{pmatrix}\beta_{1}\\
\beta_{2}\\
\vdots\\
\beta_{p}\end{pmatrix},\quad\boldsymbol{\varepsilon}=\begin{pmatrix}%
\varepsilon_{1}\\
\varepsilon_{2}\\
\vdots\\
\varepsilon_{n}\end{pmatrix}.
  </annotation>
 </semantics>
</math>

 Some remarks on terminology and general use:</p>
<ul>
<li>

<math display="inline" id="Linear_regression:6">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>y</mi>
    <mi>i</mi>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}\,
  </annotation>
 </semantics>
</math>

 is called the <em>regressand</em>, <em>endogenous variable</em>, <em>response variable</em>, <em>measured variable</em>, <em>criterion variable</em>, or <em>dependent variable</em> (see <a href="dependent_and_independent_variables" title="wikilink">dependent and independent variables</a>.) The decision as to which variable in a data set is modeled as the dependent variable and which are modeled as the independent variables may be based on a presumption that the value of one of the variables is caused by, or directly influenced by the other variables. Alternatively, there may be an operational reason to model one of the variables in terms of the others, in which case there need be no presumption of causality.</li>
<li>

<math display="inline" id="Linear_regression:7">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>i</mi>
     <mn>1</mn>
    </mrow>
   </msub>
   <mo rspace="4.2pt">,</mo>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>i</mi>
     <mn>2</mn>
    </mrow>
   </msub>
   <mo rspace="4.2pt">,</mo>
   <mi mathvariant="normal">‚Ä¶</mi>
   <mo rspace="4.2pt">,</mo>
   <mpadded width="+1.7pt">
    <msub>
     <mi>x</mi>
     <mrow>
      <mi>i</mi>
      <mi>p</mi>
     </mrow>
    </msub>
   </mpadded>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <apply>
      <times></times>
      <ci>i</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <apply>
      <times></times>
      <ci>i</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
    <ci>normal-‚Ä¶</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <apply>
      <times></times>
      <ci>i</ci>
      <ci>p</ci>
     </apply>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i1},\,x_{i2},\,\ldots,\,x_{ip}\,
  </annotation>
 </semantics>
</math>

 are called <em>regressors</em>, <em>exogenous variables</em>, <em>explanatory variables</em>, <em>covariates</em>, <em>input variables</em>, <em>predictor variables</em>, or <em>independent variables</em> (see <a href="dependent_and_independent_variables" title="wikilink">dependent and independent variables</a>, but not to be confused with <a href="independent_random_variables" title="wikilink">independent random variables</a>). The matrix 

<math display="inline" id="Linear_regression:8">
 <semantics>
  <mi>ùêó</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùêó</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{X}
  </annotation>
 </semantics>
</math>

 is sometimes called the <a href="design_matrix" title="wikilink">design matrix</a>.
<ul>
<li>Usually a constant is included as one of the regressors. For example we can take <em>x</em><sub><em>i</em>1</sub>¬†=¬†1 for <em>i</em>¬†=¬†1,¬†...,¬†<em>n</em>. The corresponding element of <strong><em>Œ≤</em></strong> is called the <em>intercept</em>. Many statistical inference procedures for linear models require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero.</li>
<li>Sometimes one of the regressors can be a non-linear function of another regressor or of the data, as in <a href="polynomial_regression" title="wikilink">polynomial regression</a> and <a href="segmented_regression" title="wikilink">segmented regression</a>. The model remains linear as long as it is linear in the parameter vector <strong><em>Œ≤</em></strong>.</li>
<li>The regressors <em>x</em><sub><em>ij</em></sub> may be viewed either as <a href="random_variables" title="wikilink">random variables</a>, which we simply observe, or they can be considered as predetermined fixed values which we can choose. Both interpretations may be appropriate in different cases, and they generally lead to the same estimation procedures; however different approaches to asymptotic analysis are used in these two situations.</li>
</ul></li>
<li>

<math display="inline" id="Linear_regression:9">
 <semantics>
  <mpadded width="+1.7pt">
   <mi>ùú∑</mi>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <ci>ùú∑</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \boldsymbol{\beta}\,
  </annotation>
 </semantics>
</math>

 is a <em>p</em>-dimensional <em>parameter vector</em>. Its elements are also called <em>effects</em>, or <em>regression coefficients</em>. Statistical <a href="estimation_theory" title="wikilink">estimation</a> and <a href="statistical_inference" title="wikilink">inference</a> in linear regression focuses on <strong><em>Œ≤</em></strong>. The elements of this parameter vector are interpreted as the <a href="partial_derivative" title="wikilink">partial derivatives</a> of the dependent variable with respect to the various independent variables.</li>
<li>

<math display="inline" id="Linear_regression:10">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>Œµ</mi>
    <mi>i</mi>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Œµ</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varepsilon_{i}\,
  </annotation>
 </semantics>
</math>

 is called the <em>error term</em>, <em>disturbance term</em>, or <em>noise</em>. This variable captures all other factors which influence the dependent variable <em>y</em><sub><em>i</em></sub> other than the regressors <strong><em>x</em></strong><sub><em>i</em></sub>. The relationship between the error term and the regressors, for example whether they are <a href="correlation" title="wikilink">correlated</a>, is a crucial step in formulating a linear regression model, as it will determine the method to use for estimation.</li>
</ul>

<p><strong>Example</strong>. Consider a situation where a small ball is being tossed up in the air and then we measure its heights of ascent <em>h<sub>i</sub></em> at various moments in time <em>t<sub>i</sub></em>. Physics tells us that, ignoring the drag, the relationship can be modeled as</p>

<p>

<math display="block" id="Linear_regression:11">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>h</mi>
     <mi>i</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <mrow>
      <msub>
       <mi>Œ≤</mi>
       <mn>1</mn>
      </msub>
      <msub>
       <mi>t</mi>
       <mi>i</mi>
      </msub>
     </mrow>
     <mo>+</mo>
     <mrow>
      <msub>
       <mi>Œ≤</mi>
       <mn>2</mn>
      </msub>
      <msubsup>
       <mi>t</mi>
       <mi>i</mi>
       <mn>2</mn>
      </msubsup>
     </mrow>
     <mo>+</mo>
     <msub>
      <mi>Œµ</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>h</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Œ≤</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>t</ci>
       <ci>i</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Œ≤</ci>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>t</ci>
        <ci>i</ci>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Œµ</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}=\beta_{1}t_{i}+\beta_{2}t_{i}^{2}+\varepsilon_{i},
  </annotation>
 </semantics>
</math>

 where <em>Œ≤</em><sub>1</sub> determines the initial velocity of the ball, <em>Œ≤</em><sub>2</sub> is proportional to the <a href="standard_gravity" title="wikilink">standard gravity</a>, and <em>Œµ</em><sub><em>i</em></sub> is due to measurement errors. Linear regression can be used to estimate the values of <em>Œ≤</em><sub>1</sub> and <em>Œ≤</em><sub>2</sub> from the measured data. This model is non-linear in the time variable, but it is linear in the parameters <em>Œ≤</em><sub>1</sub> and <em>Œ≤</em><sub>2</sub>; if we take regressors <strong><em>x</em></strong><sub><em>i</em></sub>¬†=¬†(<em>x</em><sub><em>i</em>1</sub>, <em>x</em><sub><em>i</em>2</sub>) ¬†=¬†(<em>t</em><sub><em>i</em></sub>, <em>t</em><sub><em>i</em></sub><sup>2</sup>), the model takes on the standard form</p>

<p>

<math display="block" id="Linear_regression:12">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>h</mi>
     <mi>i</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <mrow>
      <msubsup>
       <mi>ùê±</mi>
       <mi>i</mi>
       <mi mathvariant="normal">T</mi>
      </msubsup>
      <mi>ùú∑</mi>
     </mrow>
     <mo>+</mo>
     <msub>
      <mi>Œµ</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>h</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>ùê±</ci>
        <ci>normal-T</ci>
       </apply>
       <ci>i</ci>
      </apply>
      <ci>ùú∑</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Œµ</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}=\mathbf{x}^{\rm T}_{i}\boldsymbol{\beta}+\varepsilon_{i}.
  </annotation>
 </semantics>
</math>

</p>
<h3 id="assumptions">Assumptions</h3>

<p>Standard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variables and their relationship. Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Some methods are general enough that they can relax multiple assumptions at once, and in other cases this can be achieved by combining different extensions. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.</p>

<p>The following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. <a href="ordinary_least_squares" title="wikilink">ordinary least squares</a>):</p>
<ul>
<li><strong>Weak exogeneity</strong>. This essentially means that the predictor variables <em>x</em> can be treated as fixed values, rather than <a href="random_variable" title="wikilink">random variables</a>. This means, for example, that the predictor variables are assumed to be error-free‚Äîthat is, not contaminated with measurement errors. Although this assumption is not realistic in many settings, dropping it leads to significantly more difficult <a href="errors-in-variables_model" title="wikilink">errors-in-variables models</a>.</li>
</ul>
<ul>
<li><strong>Linearity</strong>. This means that the mean of the response variable is a <a href="linear_combination" title="wikilink">linear combination</a> of the parameters (regression coefficients) and the predictor variables. Note that this assumption is much less restrictive than it may at first seem. Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters. The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently. This trick is used, for example, in <a href="polynomial_regression" title="wikilink">polynomial regression</a>, which uses linear regression to fit the response variable as an arbitrary <a class="uri" href="polynomial" title="wikilink">polynomial</a> function (up to a given rank) of a predictor variable. This makes linear regression an extremely powerful inference method. In fact, models such as polynomial regression are often "too powerful", in that they tend to <a class="uri" href="overfit" title="wikilink">overfit</a> the data. As a result, some kind of <a href="regularization_(mathematics)" title="wikilink">regularization</a> must typically be used to prevent unreasonable solutions coming out of the estimation process. Common examples are <a href="ridge_regression" title="wikilink">ridge regression</a> and <a href="lasso_regression" title="wikilink">lasso regression</a>. <a href="Bayesian_linear_regression" title="wikilink">Bayesian linear regression</a> can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, <a href="ridge_regression" title="wikilink">ridge regression</a> and <a href="lasso_regression" title="wikilink">lasso regression</a> can both be viewed as special cases of Bayesian linear regression, with particular types of <a href="prior_distribution" title="wikilink">prior distributions</a> placed on the regression coefficients.)</li>
</ul>
<ul>
<li><strong>Constant variance</strong> (a.k.a. <strong><a class="uri" href="homoscedasticity" title="wikilink">homoscedasticity</a></strong>). This means that different response variables have the same <a class="uri" href="variance" title="wikilink">variance</a> in their errors, regardless of the values of the predictor variables. In practice this assumption is invalid (i.e. the errors are <a href="heteroscedasticity" title="wikilink">heteroscedastic</a>) if the response variables can vary over a wide scale. In order to determine for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a "fanning effect" between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predicting outcome. Error will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a <a href="standard_deviation" title="wikilink">standard deviation</a> of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases‚Äîoften the same cases where the assumption of normally distributed errors fails‚Äîthe variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. <a href="weighted_least_squares" title="wikilink">weighted least squares</a> and <a href="heteroscedasticity-consistent_standard_errors" title="wikilink">heteroscedasticity-consistent standard errors</a>) can handle heteroscedasticity in a quite general way. <a href="Bayesian_linear_regression" title="wikilink">Bayesian linear regression</a> techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the <a class="uri" href="logarithm" title="wikilink">logarithm</a> of the response variable using a linear regression model, which implies that the response variable has a <a href="log-normal_distribution" title="wikilink">log-normal distribution</a> rather than a <a href="normal_distribution" title="wikilink">normal distribution</a>).</li>
</ul>
<ul>
<li><strong>Independence</strong> of errors. This assumes that the errors of the response variables are uncorrelated with each other. (Actual <a href="Independence_(probability_theory)" title="wikilink">statistical independence</a> is a stronger condition than mere lack of correlation and is often not needed, although it can be exploited if it is known to hold.) Some methods (e.g. <a href="generalized_least_squares" title="wikilink">generalized least squares</a>) are capable of handling correlated errors, although they typically require significantly more data unless some sort of <a href="regularization_(mathematics)" title="wikilink">regularization</a> is used to bias the model towards assuming uncorrelated errors. <a href="Bayesian_linear_regression" title="wikilink">Bayesian linear regression</a> is a general way of handling this issue.</li>
<li><strong>Lack of multicollinearity</strong> in the predictors. For standard <a href="least_squares" title="wikilink">least squares</a> estimation methods, the design matrix <em>X</em> must have full <a href="column_rank" title="wikilink">column rank</a> <em>p</em>,; otherwise, we have a condition known as <a class="uri" href="multicollinearity" title="wikilink">multicollinearity</a> in the predictor variables. This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same predictor variable is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared to the number of parameters to be estimated (e.g. fewer data points than regression coefficients). In the case of multicollinearity, the parameter vector <em>Œ≤</em> will be <a class="uri" href="non-identifiable" title="wikilink">non-identifiable</a>‚Äîit has no unique solution. At most we will be able to identify some of the parameters, i.e. narrow down its value to some linear subspace of <strong>R</strong><sup><em>p</em></sup>. See <a href="partial_least_squares_regression" title="wikilink">partial least squares regression</a>. Methods for fitting linear models with multicollinearity have been developed;<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> some require additional assumptions such as "effect sparsity"‚Äîthat a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in <a href="generalized_linear_model" title="wikilink">generalized linear models</a>, do not suffer from this problem‚Äîand in fact it's quite normal to when handling <a href="categorical_data" title="wikilink">categorically valued</a> predictors to introduce a separate <a href="indicator_variable" title="wikilink">indicator variable</a> predictor for each possible category, which inevitably introduces multicollinearity.</li>
</ul>

<p>Beyond these assumptions, several other statistical properties of the data strongly influence the performance of different estimation methods:</p>
<ul>
<li>The statistical relationship between the error terms and the regressors plays an important role in determining whether an estimation procedure has desirable sampling properties such as being unbiased and consistent.</li>
<li>The arrangement, or <a href="probability_distribution" title="wikilink">probability distribution</a> of the predictor variables <em>x</em> has a major influence on the precision of estimates of <em>Œ≤</em>. <a href="Sampling_(statistics)" title="wikilink">Sampling</a> and <a href="design_of_experiments" title="wikilink">design of experiments</a> are highly developed subfields of statistics that provide guidance for collecting data in such a way to achieve a precise estimate of <em>Œ≤</em>.</li>
</ul>
<h3 id="interpretation">Interpretation</h3>

<p> A fitted linear regression model can be used to identify the relationship between a single predictor variable <em>x</em><sub><em>j</em></sub> and the response variable <em>y</em> when all the other predictor variables in the model are "held fixed". Specifically, the interpretation of <em>Œ≤</em><sub><em>j</em></sub> is the <a href="expected_value" title="wikilink">expected</a> change in <em>y</em> for a one-unit change in <em>x</em><sub><em>j</em></sub> when the other covariates are held fixed‚Äîthat is, the expected value of the <a href="partial_derivative" title="wikilink">partial derivative</a> of <em>y</em> with respect to <em>x</em><sub><em>j</em></sub>. This is sometimes called the <em>unique effect</em> of <em>x</em><sub><em>j</em></sub> on <em>y</em>. In contrast, the <em>marginal effect</em> of <em>x</em><sub><em>j</em></sub> on <em>y</em> can be assessed using a <a href="Pearson_correlation" title="wikilink">correlation coefficient</a> or <a href="simple_linear_regression" title="wikilink">simple linear regression</a> model relating <em>x</em><sub><em>j</em></sub> to <em>y</em>; this effect is the <a href="total_derivative" title="wikilink">total derivative</a> of <em>y</em> with respect to <em>x</em><sub><em>j</em></sub>.</p>

<p>Care must be taken when interpreting regression results, as some of the regressors may not allow for marginal changes (such as dummy variables, or the intercept term), while others cannot be held fixed (recall the example from the introduction: it would be impossible to "hold <em>t<sub>i</sub></em> fixed" and at the same time change the value of <em>t<sub>i</sub></em><sup>2</sup>).</p>

<p>It is possible that the unique effect can be nearly zero even when the marginal effect is large. This may imply that some other covariate captures all the information in <em>x</em><sub><em>j</em></sub>, so that once that variable is in the model, there is no contribution of <em>x</em><sub><em>j</em></sub> to the variation in <em>y</em>. Conversely, the unique effect of <em>x</em><sub><em>j</em></sub> can be large while its marginal effect is nearly zero. This would happen if the other covariates explained a great deal of the variation of <em>y</em>, but they mainly explain variation in a way that is complementary to what is captured by <em>x</em><sub><em>j</em></sub>. In this case, including the other variables in the model reduces the part of the variability of <em>y</em> that is unrelated to <em>x</em><sub><em>j</em></sub>, thereby strengthening the apparent relationship with <em>x</em><sub><em>j</em></sub>.</p>

<p>The meaning of the expression "held fixed" may depend on how the values of the predictor variables arise. If the experimenter directly sets the values of the predictor variables according to a study design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been "held fixed" by the experimenter. Alternatively, the expression "held fixed" can refer to a selection that takes place in the context of data analysis. In this case, we "hold a variable fixed" by restricting our attention to the subsets of the data that happen to have a common value for the given predictor variable. This is the only interpretation of "held fixed" that can be used in an observational study.</p>

<p>The notion of a "unique effect" is appealing when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that is linked to the value of a predictor variable. However, it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following a study design.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> A commonality analysis may be helpful in disentangling the shared and unique impacts of correlated independent variables.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h2 id="extensions">Extensions</h2>

<p>Numerous extensions of linear regression have been developed, which allow some or all of the assumptions underlying the basic model to be relaxed.</p>
<h3 id="simple-and-multiple-regression">Simple and multiple regression</h3>

<p>The very simplest case of a single <a href="scalar_(mathematics)" title="wikilink">scalar</a> predictor variable <em>x</em> and a single scalar response variable <em>y</em> is known as <em>simple linear regression</em>. The extension to multiple and/or <a href="Euclidean_vector" title="wikilink">vector</a>-valued predictor variables (denoted with a capital <em>X</em>) is known as <em>multiple linear regression</em>, also known as <em>multivariable linear regression</em>. Nearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased in terms of the multiple regression model. Note, however, that in these cases the response variable <em>y</em> is still a scalar. Another term <em>multivariate linear regression</em> refers to cases where <em>y</em> is a vector, i.e., the same as <em>general linear regression</em>. The difference between <em>multivariate</em> linear regression and <em>multivariable</em> linear regression should be emphasized as it causes much confusion and misunderstanding in the literature.</p>
<h3 id="general-linear-models">General linear models</h3>

<p>The <a href="general_linear_model" title="wikilink">general linear model</a> considers the situation when the response variable <em>Y</em> is not a scalar but a vector. Conditional linearity of <em>E</em>(<em>y</em>|<em>x</em>)¬†=¬†<em>Bx</em> is still assumed, with a matrix <em>B</em> replacing the vector <em>Œ≤</em> of the classical linear regression model. Multivariate analogues of OLS and GLS have been developed. The term "general linear models" is equivalent to "multivariate linear models". It should be noted the difference of "multivariate linear models" and "multivariable linear models," where the former is the same as "general linear models" and the latter is the same as "multiple linear models."</p>
<h3 id="heteroscedastic-models">Heteroscedastic models</h3>

<p>Various models have been created that allow for <a class="uri" href="heteroscedasticity" title="wikilink">heteroscedasticity</a>, i.e. the errors for different response variables may have different <a href="variance" title="wikilink">variances</a>. For example, <a href="weighted_least_squares" title="wikilink">weighted least squares</a> is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also <a href="Linear_least_squares_(mathematics)#Weighted_linear_least_squares" title="wikilink">Weighted linear least squares</a>, and <a href="generalized_least_squares" title="wikilink">generalized least squares</a>.) <a href="Heteroscedasticity-consistent_standard_errors" title="wikilink">Heteroscedasticity-consistent standard errors</a> is an improved method for use with uncorrelated but potentially heteroscedastic errors.</p>
<h3 id="generalized-linear-models">Generalized linear models</h3>

<p><a href="Generalized_linear_model" title="wikilink">Generalized linear models</a> (GLMs) are a framework for modeling a response variable <em>y</em> that is bounded or discrete. This is used, for example:</p>
<ul>
<li>when modeling positive quantities (e.g. prices or populations) that vary over a large scale‚Äîwhich are better described using a <a href="skewed_distribution" title="wikilink">skewed distribution</a> such as the <a href="log-normal_distribution" title="wikilink">log-normal distribution</a> or <a href="Poisson_distribution" title="wikilink">Poisson distribution</a> (although GLMs are not used for log-normal data, instead the response variable is simply transformed using the logarithm function);</li>
<li>when modeling <a href="categorical_data" title="wikilink">categorical data</a>, such as the choice of a given candidate in an election (which is better described using a <a href="Bernoulli_distribution" title="wikilink">Bernoulli distribution</a>/<a href="binomial_distribution" title="wikilink">binomial distribution</a> for binary choices, or a <a href="categorical_distribution" title="wikilink">categorical distribution</a>/<a href="multinomial_distribution" title="wikilink">multinomial distribution</a> for multi-way choices), where there are a fixed number of choices that cannot be meaningfully ordered;</li>
<li>when modeling <a href="ordinal_data" title="wikilink">ordinal data</a>, e.g. ratings on a scale from 0 to 5, where the different outcomes can be ordered but where the quantity itself may not have any absolute meaning (e.g. a rating of 4 may not be "twice as good" in any objective sense as a rating of 2, but simply indicates that it is better than 2 or 3 but not as good as 5).</li>
</ul>

<p>Generalized linear models allow for an arbitrary <em>link function</em> <em>g</em> that relates the <a class="uri" href="mean" title="wikilink">mean</a> of the response variable to the predictors, i.e. <em>E</em>(<em>y</em>) = <em>g</em>(<em>Œ≤</em>‚Ä≤<em>x</em>). The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the 

<math display="inline" id="Linear_regression:13">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mrow>
    <mo>-</mo>
    <mi mathvariant="normal">‚àû</mi>
   </mrow>
   <mo>,</mo>
   <mi mathvariant="normal">‚àû</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <apply>
     <minus></minus>
     <infinity></infinity>
    </apply>
    <infinity></infinity>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (-\infty,\infty)
  </annotation>
 </semantics>
</math>

 range of the linear predictor and the range of the response variable.</p>

<p>Some common examples of GLMs are:</p>
<ul>
<li><a href="Poisson_regression" title="wikilink">Poisson regression</a> for count data.</li>
<li><a href="Logistic_regression" title="wikilink">Logistic regression</a> and <a href="probit_regression" title="wikilink">probit regression</a> for binary data.</li>
<li><a href="Multinomial_logistic_regression" title="wikilink">Multinomial logistic regression</a> and <a href="multinomial_probit" title="wikilink">multinomial probit</a> regression for categorical data.</li>
<li><a href="Ordered_probit" title="wikilink">Ordered probit</a> regression for ordinal data.</li>
</ul>

<p>Single index models allow some degree of nonlinearity in the relationship between <em>x</em> and <em>y</em>, while preserving the central role of the linear predictor <em>Œ≤</em>‚Ä≤<em>x</em> as in the classical linear regression model. Under certain conditions, simply applying OLS to data from a single-index model will consistently estimate <em>Œ≤</em> up to a proportionality constant.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<h3 id="hierarchical-linear-models">Hierarchical linear models</h3>

<p><a href="Hierarchical_linear_models" title="wikilink">Hierarchical linear models</a> (or <em>multilevel regression</em>) organizes the data into a hierarchy of regressions, for example where <em>A</em> is regressed on <em>B</em>, and <em>B</em> is regressed on <em>C</em>. It is often used where the data have a natural hierarchical structure such as in educational statistics, where students are nested in classrooms, classrooms are nested in schools, and schools are nested in some administrative grouping, such as a school district. The response variable might be a measure of student achievement such as a test score, and different covariates would be collected at the classroom, school, and school district levels.</p>
<h3 id="errors-in-variables">Errors-in-variables</h3>

<p><a href="Errors-in-variables_model" title="wikilink">Errors-in-variables models</a> (or "measurement error models") extend the traditional linear regression model to allow the predictor variables <em>X</em> to be observed with error. This error causes standard estimators of <em>Œ≤</em> to become biased. Generally, the form of bias is an attenuation, meaning that the effects are biased toward zero.</p>
<h3 id="others">Others</h3>
<ul>
<li>In <a href="Dempster‚ÄìShafer_theory" title="wikilink">Dempster‚ÄìShafer theory</a>, or a <a href="linear_belief_function" title="wikilink">linear belief function</a> in particular, a linear regression model may be represented as a partially swept matrix, which can be combined with similar matrices representing observations and other assumed normal distributions and state equations. The combination of swept or unswept matrices provides an alternative method for estimating linear regression models.</li>
</ul>
<h2 id="estimation-methods">Estimation methods</h2>

<p> A large number of procedures have been developed for <a class="uri" href="parameter" title="wikilink">parameter</a> estimation and inference in linear regression. These methods differ in computational simplicity of algorithms, presence of a closed-form solution, robustness with respect to heavy-tailed distributions, and theoretical assumptions needed to validate desirable statistical properties such as <a href="consistent_estimator" title="wikilink">consistency</a> and asymptotic <a href="efficiency_(statistics)" title="wikilink">efficiency</a>.</p>

<p>Some of the more common estimation techniques for linear regression are summarized below.</p>
<h3 id="least-squares-estimation-and-related-techniques">Least-squares estimation and related techniques</h3>
<h3 id="maximum-likelihood-estimation-and-related-techniques">Maximum-likelihood estimation and related techniques</h3>
<ul>
<li><strong><a href="Maximum_likelihood_estimation" title="wikilink">Maximum likelihood estimation</a></strong> can be performed when the distribution of the error terms is known to belong to a certain parametric family <em>∆í<sub>Œ∏</sub></em> of <a href="probability_distribution" title="wikilink">probability distributions</a>.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> When <em>f</em><sub>Œ∏</sub> is a normal distribution with zero <a href="expected_value" title="wikilink">mean</a> and variance Œ∏, the resulting estimate is identical to the OLS estimate. GLS estimates are maximum likelihood estimates when Œµ follows a multivariate normal distribution with a known covariance matrix.</li>
<li><strong><a href="Ridge_regression" title="wikilink">Ridge regression</a></strong>,<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a><a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> and other forms of penalized estimation such as <strong><a href="Least_squares#Lasso_method" title="wikilink">Lasso regression</a></strong>,<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> deliberately introduce <a href="bias_of_an_estimator" title="wikilink">bias</a> into the estimation of <em>Œ≤</em> in order to reduce the <a href="variance" title="wikilink">variability</a> of the estimate. The resulting estimators generally have lower <a href="mean_squared_error" title="wikilink">mean squared error</a> than the OLS estimates, particularly when <a class="uri" href="multicollinearity" title="wikilink">multicollinearity</a> is present. They are generally used when the goal is to predict the value of the response variable <em>y</em> for values of the predictors <em>x</em> that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias.</li>
<li><strong><a href="Least_absolute_deviation" title="wikilink">Least absolute deviation</a></strong> (LAD) regression is a <a href="robust_regression" title="wikilink">robust estimation</a> technique in that it is less sensitive to the presence of outliers than OLS (but is less <a href="efficiency_(statistics)" title="wikilink">efficient</a> than OLS when no outliers are present). It is equivalent to maximum likelihood estimation under a <a href="Laplace_distribution" title="wikilink">Laplace distribution</a> model for <em>Œµ</em>.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></li>
<li><strong>Adaptive estimation</strong>. If we assume that error terms are <a href="Independence_(probability_theory)" title="wikilink">independent</a> from the regressors 

<math display="inline" id="Linear_regression:14">
 <semantics>
  <mrow>
   <msub>
    <mi>Œµ</mi>
    <mi>i</mi>
   </msub>
   <mo>‚üÇ</mo>
   <msub>
    <mi>ùê±</mi>
    <mi>i</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">perpendicular-to</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Œµ</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ùê±</ci>
     <ci>i</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varepsilon_{i}\perp\mathbf{x}_{i}
  </annotation>
 </semantics>
</math>

, the optimal estimator is the 2-step MLE, where the first step is used to non-parametrically estimate the distribution of the error term.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></li>
</ul>
<h3 id="other-estimation-techniques">Other estimation techniques</h3>
<ul>
<li><strong><a href="Bayesian_linear_regression" title="wikilink">Bayesian linear regression</a></strong> applies the framework of <a href="Bayesian_statistics" title="wikilink">Bayesian statistics</a> to linear regression. (See also <a href="Bayesian_multivariate_linear_regression" title="wikilink">Bayesian multivariate linear regression</a>.) In particular, the regression coefficients Œ≤ are assumed to be <a href="random_variable" title="wikilink">random variables</a> with a specified <a href="prior_distribution" title="wikilink">prior distribution</a>. The prior distribution can bias the solutions for the regression coefficients, in a way similar to (but more general than) <a href="ridge_regression" title="wikilink">ridge regression</a> or <a href="lasso_regression" title="wikilink">lasso regression</a>. In addition, the Bayesian estimation process produces not a single point estimate for the "best" values of the regression coefficients but an entire <a href="posterior_distribution" title="wikilink">posterior distribution</a>, completely describing the uncertainty surrounding the quantity. This can be used to estimate the "best" coefficients using the mean, mode, median, any quantile (see <a href="quantile_regression" title="wikilink">quantile regression</a>), or any other function of the posterior distribution.</li>
<li><strong><a href="Quantile_regression" title="wikilink">Quantile regression</a></strong> focuses on the conditional quantiles of <em>y</em> given <em>X</em> rather than the conditional mean of <em>y</em> given <em>X</em>. Linear quantile regression models a particular conditional quantile, for example the conditional median, as a linear function Œ≤<sup>T</sup><em>x</em> of the predictors.</li>
<li><strong><a href="Mixed_model" title="wikilink">Mixed models</a></strong> are widely used to analyze linear regression relationships involving dependent data when the dependencies have a known structure. Common applications of mixed models include analysis of data involving repeated measurements, such as longitudinal data, or data obtained from cluster sampling. They are generally fit as <a href="parametric_statistics" title="wikilink">parametric</a> models, using maximum likelihood or Bayesian estimation. In the case where the errors are modeled as <a href="normal_distribution" title="wikilink">normal</a> random variables, there is a close connection between mixed models and generalized least squares.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> <a href="Fixed_effects_estimation" title="wikilink">Fixed effects estimation</a> is an alternative approach to analyzing this type of data.</li>
<li><strong><a href="Principal_component_regression" title="wikilink">Principal component regression</a></strong> (PCR)<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a><a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> is used when the number of predictor variables is large, or when strong correlations exist among the predictor variables. This two-stage procedure first reduces the predictor variables using <a href="principal_component_analysis" title="wikilink">principal component analysis</a> then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant principal components of the multivariate distribution of the predictor variables. The <a href="partial_least_squares_regression" title="wikilink">partial least squares regression</a> is the extension of the PCR method which does not suffer from the mentioned deficiency.</li>
<li><strong><a href="Least-angle_regression" title="wikilink">Least-angle regression</a></strong><a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> is an estimation procedure for linear regression models that was developed to handle high-dimensional covariate vectors, potentially with more covariates than observations.</li>
<li>The <strong><a href="Theil‚ÄìSen_estimator" title="wikilink">Theil‚ÄìSen estimator</a></strong> is a simple <a href="robust_regression" title="wikilink">robust estimation</a> technique that chooses the slope of the fit line to be the median of the slopes of the lines through pairs of sample points. It has similar statistical efficiency properties to simple linear regression but is much less sensitive to <a href="outlier" title="wikilink">outliers</a>.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a></li>
<li>Other robust estimation techniques, including the <strong>Œ±-trimmed mean</strong> approach, and <strong>L-, M-, S-, and R-estimators</strong> have been introduced.</li>
</ul>
<h3 id="further-discussion">Further discussion</h3>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a> and <a href="numerical_analysis" title="wikilink">numerical analysis</a>, the problem of <strong>numerical methods for linear least squares</strong> is an important one because linear regression models are one of the most important types of model, both as formal <a href="statistical_model" title="wikilink">statistical models</a> and for exploration of data sets. The majority of <a href="Comparison_of_statistical_packages" title="wikilink">statistical computer packages</a> contain facilities for regression analysis that make use of linear least squares computations. Hence it is appropriate that considerable effort has been devoted to the task of ensuring that these computations are undertaken efficiently and with due regard to <a href="Precision_(computer_science)" title="wikilink">numerical precision</a>.</p>

<p>Individual statistical analyses are seldom undertaken in isolation, but rather are part of a sequence of investigatory steps. Some of the topics involved in considering numerical methods for linear least squares relate to this point. Thus important topics can be</p>
<ul>
<li>Computations where a number of similar, and often nested, models are considered for the same data set. That is, where models with the same <a href="dependent_variable" title="wikilink">dependent variable</a> but different sets of <a href="independent_variables" title="wikilink">independent variables</a> are to be considered, for essentially the same set of data points.</li>
<li>Computations for analyses that occur in a sequence, as the number of data points increases.</li>
<li>Special considerations for very extensive data sets.</li>
</ul>

<p>Fitting of linear models by least squares often, but not always, arises in the context of <a href="statistical_analysis" title="wikilink">statistical analysis</a>. It can therefore be important that considerations of computational efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the <a href="linear_least_squares_(mathematics)" title="wikilink">linear least squares</a> problem.</p>

<p>Matrix calculations, like any others, are affected by <a href="rounding_error" title="wikilink">rounding errors</a>. An early summary of these effects, regarding the choice of computational methods for matrix inversion, was provided by Wilkinson.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a></p>
<h3 id="using-linear-algebra">Using Linear Algebra</h3>

<p>It follows that one can find a "best" approximation of another function by minimizing the area between two functions, a continuous function 

<math display="inline" id="Linear_regression:15">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 on 

<math display="inline" id="Linear_regression:16">
 <semantics>
  <mrow>
   <mo stretchy="false">[</mo>
   <mi>a</mi>
   <mo>,</mo>
   <mi>b</mi>
   <mo stretchy="false">]</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="closed">
    <ci>a</ci>
    <ci>b</ci>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   [a,b]
  </annotation>
 </semantics>
</math>

 and a function 

<math display="inline" id="Linear_regression:17">
 <semantics>
  <mrow>
   <mi>g</mi>
   <mo>‚àà</mo>
   <mi>W</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>g</ci>
    <ci>W</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g\in W
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Linear_regression:18">
 <semantics>
  <mi>W</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>W</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W
  </annotation>
 </semantics>
</math>

 is a subspace of 

<math display="inline" id="Linear_regression:19">
 <semantics>
  <mrow>
   <mi>C</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>a</mi>
    <mo>,</mo>
    <mi>b</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>C</ci>
    <interval closure="closed">
     <ci>a</ci>
     <ci>b</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   C[a,b]
  </annotation>
 </semantics>
</math>

:</p>

<p>

<math display="block" id="Linear_regression:20">
 <semantics>
  <mrow>
   <mtext>Area</mtext>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">‚à´</mo>
     <mi>a</mi>
     <mi>b</mi>
    </msubsup>
    <mrow>
     <mrow>
      <mo>|</mo>
      <mrow>
       <mrow>
        <mi>f</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>-</mo>
       <mrow>
        <mi>g</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo rspace="4.2pt">|</mo>
     </mrow>
     <mi>d</mi>
     <mi>x</mi>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <mtext>Area</mtext>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <int></int>
       <ci>a</ci>
      </apply>
      <ci>b</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <abs></abs>
       <apply>
        <minus></minus>
        <apply>
         <times></times>
         <ci>f</ci>
         <ci>x</ci>
        </apply>
        <apply>
         <times></times>
         <ci>g</ci>
         <ci>x</ci>
        </apply>
       </apply>
      </apply>
      <ci>d</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \text{Area }=\int_{a}^{b}\left|f(x)-g(x)\right|\,dx
  </annotation>
 </semantics>
</math>

, all within the subspace 

<math display="inline" id="Linear_regression:21">
 <semantics>
  <mi>W</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>W</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W
  </annotation>
 </semantics>
</math>

. Due to the frequent difficulty of evaluating integrands involving absolute value, one can instead define</p>

<p>

<math display="block" id="Linear_regression:22">
 <semantics>
  <mrow>
   <msubsup>
    <mo largeop="true" symmetric="true">‚à´</mo>
    <mi>a</mi>
    <mi>b</mi>
   </msubsup>
   <mrow>
    <mpadded width="+1.7pt">
     <msup>
      <mrow>
       <mo stretchy="false">[</mo>
       <mrow>
        <mrow>
         <mi>f</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>-</mo>
        <mrow>
         <mi>g</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo stretchy="false">]</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mpadded>
    <mi>d</mi>
    <mi>x</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <int></int>
      <ci>a</ci>
     </apply>
     <ci>b</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <times></times>
         <ci>f</ci>
         <ci>x</ci>
        </apply>
        <apply>
         <times></times>
         <ci>g</ci>
         <ci>x</ci>
        </apply>
       </apply>
      </apply>
      <cn type="integer">2</cn>
     </apply>
     <ci>d</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \int_{a}^{b}[f(x)-g(x)]^{2}\,dx
  </annotation>
 </semantics>
</math>

 as an adequate criterion for obtaining the least squares approximation, function 

<math display="inline" id="Linear_regression:23">
 <semantics>
  <mi>g</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>g</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g
  </annotation>
 </semantics>
</math>

, of 

<math display="inline" id="Linear_regression:24">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 with respect to the inner product space 

<math display="inline" id="Linear_regression:25">
 <semantics>
  <mi>W</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>W</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W
  </annotation>
 </semantics>
</math>

.</p>

<p>As such, 

<math display="inline" id="Linear_regression:26">
 <semantics>
  <msup>
   <mrow>
    <mo fence="true">‚à•</mo>
    <mrow>
     <mi>f</mi>
     <mo>-</mo>
     <mi>g</mi>
    </mrow>
    <mo fence="true">‚à•</mo>
   </mrow>
   <mn>2</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <apply>
      <minus></minus>
      <ci>f</ci>
      <ci>g</ci>
     </apply>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lVert f-g\rVert^{2}
  </annotation>
 </semantics>
</math>

 or, equivalently, 

<math display="inline" id="Linear_regression:27">
 <semantics>
  <mrow>
   <mo fence="true">‚à•</mo>
   <mrow>
    <mi>f</mi>
    <mo>-</mo>
    <mi>g</mi>
   </mrow>
   <mo fence="true">‚à•</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">norm</csymbol>
    <apply>
     <minus></minus>
     <ci>f</ci>
     <ci>g</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lVert f-g\rVert
  </annotation>
 </semantics>
</math>

, can thus be written in vector form:</p>

<p>

<math display="block" id="Linear_regression:28">
 <semantics>
  <mrow>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">‚à´</mo>
     <mi>a</mi>
     <mi>b</mi>
    </msubsup>
    <mrow>
     <mpadded width="+1.7pt">
      <msup>
       <mrow>
        <mo stretchy="false">[</mo>
        <mrow>
         <mrow>
          <mi>f</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <mi>x</mi>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
         <mo>-</mo>
         <mrow>
          <mi>g</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <mi>x</mi>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
        </mrow>
        <mo stretchy="false">]</mo>
       </mrow>
       <mn>2</mn>
      </msup>
     </mpadded>
     <mi>d</mi>
     <mi>x</mi>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>‚ü®</mo>
    <mrow>
     <mi>f</mi>
     <mo>-</mo>
     <mi>g</mi>
    </mrow>
    <mo>,</mo>
    <mrow>
     <mi>f</mi>
     <mo>-</mo>
     <mi>g</mi>
    </mrow>
    <mo>‚ü©</mo>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo fence="true">‚à•</mo>
     <mrow>
      <mi>f</mi>
      <mo>-</mo>
      <mi>g</mi>
     </mrow>
     <mo fence="true">‚à•</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <int></int>
        <ci>a</ci>
       </apply>
       <ci>b</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="latexml">delimited-[]</csymbol>
         <apply>
          <minus></minus>
          <apply>
           <times></times>
           <ci>f</ci>
           <ci>x</ci>
          </apply>
          <apply>
           <times></times>
           <ci>g</ci>
           <ci>x</ci>
          </apply>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <ci>d</ci>
       <ci>x</ci>
      </apply>
     </apply>
     <list>
      <apply>
       <minus></minus>
       <ci>f</ci>
       <ci>g</ci>
      </apply>
      <apply>
       <minus></minus>
       <ci>f</ci>
       <ci>g</ci>
      </apply>
     </list>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="latexml">norm</csymbol>
       <apply>
        <minus></minus>
        <ci>f</ci>
        <ci>g</ci>
       </apply>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \int_{a}^{b}[f(x)-g(x)]^{2}\,dx=\left\langle f-g,f-g\right\rangle=\lVert f-g%
\rVert^{2}
  </annotation>
 </semantics>
</math>

. In other words, the least squares approximation of 

<math display="inline" id="Linear_regression:29">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 is the function 

<math display="inline" id="Linear_regression:30">
 <semantics>
  <mrow>
   <mi>g</mi>
   <mo>‚àà</mo>
   <mrow>
    <mtext>subspace</mtext>
    <mi>W</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>g</ci>
    <apply>
     <times></times>
     <mtext>subspace</mtext>
     <ci>W</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g\in\text{ subspace }W
  </annotation>
 </semantics>
</math>

 closest to 

<math display="inline" id="Linear_regression:31">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 in terms of the inner product 

<math display="inline" id="Linear_regression:32">
 <semantics>
  <mrow>
   <mo>‚ü®</mo>
   <mi>f</mi>
   <mo>,</mo>
   <mi>g</mi>
   <mo>‚ü©</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <ci>f</ci>
    <ci>g</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \left\langle f,g\right\rangle
  </annotation>
 </semantics>
</math>

. Furthermore, this can be applied with a theorem:</p>
<dl>
<dd>Let 

<math display="inline" id="Linear_regression:33">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 be continuous on 

<math display="inline" id="Linear_regression:34">
 <semantics>
  <mrow>
   <mo stretchy="false">[</mo>
   <mi>a</mi>
   <mo>,</mo>
   <mi>b</mi>
   <mo stretchy="false">]</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="closed">
    <ci>a</ci>
    <ci>b</ci>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   [a,b]
  </annotation>
 </semantics>
</math>

, and let 

<math display="inline" id="Linear_regression:35">
 <semantics>
  <mi>W</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>W</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W
  </annotation>
 </semantics>
</math>

 be a finite-dimensional subspace of 

<math display="inline" id="Linear_regression:36">
 <semantics>
  <mrow>
   <mi>C</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>a</mi>
    <mo>,</mo>
    <mi>b</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>C</ci>
    <interval closure="closed">
     <ci>a</ci>
     <ci>b</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   C[a,b]
  </annotation>
 </semantics>
</math>

. The least squares approximating function of 

<math display="inline" id="Linear_regression:37">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 with respect to 

<math display="inline" id="Linear_regression:38">
 <semantics>
  <mi>W</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>W</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W
  </annotation>
 </semantics>
</math>

 is given by
</dd>
</dl>
<dl>
<dd><dl>
<dd>

<math display="inline" id="Linear_regression:39">
 <semantics>
  <mrow>
   <mi>g</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mrow>
      <mo>‚ü®</mo>
      <mi>f</mi>
      <mo>,</mo>
      <msub>
       <mover accent="true">
        <mi>w</mi>
        <mo stretchy="false">‚Üí</mo>
       </mover>
       <mn>1</mn>
      </msub>
      <mo>‚ü©</mo>
     </mrow>
     <msub>
      <mover accent="true">
       <mi>w</mi>
       <mo stretchy="false">‚Üí</mo>
      </mover>
      <mn>1</mn>
     </msub>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mrow>
      <mo>‚ü®</mo>
      <mi>f</mi>
      <mo>,</mo>
      <msub>
       <mover accent="true">
        <mi>w</mi>
        <mo stretchy="false">‚Üí</mo>
       </mover>
       <mn>2</mn>
      </msub>
      <mo>‚ü©</mo>
     </mrow>
     <msub>
      <mover accent="true">
       <mi>w</mi>
       <mo stretchy="false">‚Üí</mo>
      </mover>
      <mn>2</mn>
     </msub>
    </mrow>
    <mo>+</mo>
    <mi mathvariant="normal">‚Ä¶</mi>
    <mo>+</mo>
    <mrow>
     <mrow>
      <mo>‚ü®</mo>
      <mi>f</mi>
      <mo>,</mo>
      <msub>
       <mover accent="true">
        <mi>w</mi>
        <mo stretchy="false">‚Üí</mo>
       </mover>
       <mi>n</mi>
      </msub>
      <mo>‚ü©</mo>
     </mrow>
     <msub>
      <mover accent="true">
       <mi>w</mi>
       <mo stretchy="false">‚Üí</mo>
      </mover>
      <mi>n</mi>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>g</ci>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <list>
       <ci>f</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <ci>normal-‚Üí</ci>
         <ci>w</ci>
        </apply>
        <cn type="integer">1</cn>
       </apply>
      </list>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-‚Üí</ci>
        <ci>w</ci>
       </apply>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <list>
       <ci>f</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <ci>normal-‚Üí</ci>
         <ci>w</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </list>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-‚Üí</ci>
        <ci>w</ci>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <ci>normal-‚Ä¶</ci>
     <apply>
      <times></times>
      <list>
       <ci>f</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <ci>normal-‚Üí</ci>
         <ci>w</ci>
        </apply>
        <ci>n</ci>
       </apply>
      </list>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-‚Üí</ci>
        <ci>w</ci>
       </apply>
       <ci>n</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g=\left\langle f,\vec{w}_{1}\right\rangle\vec{w}_{1}+\left\langle f,\vec{w}_{2%
}\right\rangle\vec{w}_{2}+\dots+\left\langle f,\vec{w}_{n}\right\rangle\vec{w}%
_{n}
  </annotation>
 </semantics>
</math>

,
</dd>
</dl>
</dd>
</dl>
<dl>
<dd>where 

<math display="inline" id="Linear_regression:40">
 <semantics>
  <mrow>
   <mi>B</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mover accent="true">
      <mi>w</mi>
      <mo stretchy="false">‚Üí</mo>
     </mover>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mover accent="true">
      <mi>w</mi>
      <mo stretchy="false">‚Üí</mo>
     </mover>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">‚Ä¶</mi>
    <mo>,</mo>
    <msub>
     <mover accent="true">
      <mi>w</mi>
      <mo stretchy="false">‚Üí</mo>
     </mover>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>B</ci>
    <set>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <ci>normal-‚Üí</ci>
       <ci>w</ci>
      </apply>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <ci>normal-‚Üí</ci>
       <ci>w</ci>
      </apply>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-‚Ä¶</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <ci>normal-‚Üí</ci>
       <ci>w</ci>
      </apply>
      <ci>n</ci>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B=\{\vec{w}_{1},\vec{w}_{2},\dots,\vec{w}_{n}\}
  </annotation>
 </semantics>
</math>

 is an orthonormal basis for 

<math display="inline" id="Linear_regression:41">
 <semantics>
  <mi>W</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>W</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W
  </annotation>
 </semantics>
</math>

.
</dd>
</dl>
<h2 id="applications-of-linear-regression">Applications of linear regression</h2>

<p>Linear regression is widely used in biological, behavioral and social sciences to describe possible relationships between variables. It ranks as one of the most important tools used in these disciplines.</p>
<h3 id="trend-line">Trend line</h3>

<p>A <strong>trend line</strong> represents a trend, the long-term movement in <a href="time_series" title="wikilink">time series</a> data after other components have been accounted for. It tells whether a particular data set (say GDP, oil prices or stock prices) have increased or decreased over the period of time. A trend line could simply be drawn by eye through a set of data points, but more properly their position and slope is calculated using statistical techniques like linear regression. Trend lines typically are straight lines, although some variations use higher degree polynomials depending on the degree of curvature desired in the line.</p>

<p>Trend lines are sometimes used in business analytics to show changes in data over time. This has the advantage of being simple. Trend lines are often used to argue that a particular action or event (such as training, or an advertising campaign) caused observed changes at a point in time. This is a simple technique, and does not require a control group, experimental design, or a sophisticated analysis technique. However, it suffers from a lack of scientific validity in cases where other potential changes can affect the data.</p>
<h3 id="epidemiology">Epidemiology</h3>

<p>Early evidence relating <a href="tobacco_smoking" title="wikilink">tobacco smoking</a> to mortality and <a class="uri" href="morbidity" title="wikilink">morbidity</a> came from <a href="observational_studies" title="wikilink">observational studies</a> employing regression analysis. In order to reduce <a href="spurious_correlation" title="wikilink">spurious correlations</a> when analyzing observational data, researchers usually include several variables in their regression models in addition to the variable of primary interest. For example, suppose we have a regression model in which cigarette smoking is the independent variable of interest, and the dependent variable is lifespan measured in years. Researchers might include socio-economic status as an additional independent variable, to ensure that any observed effect of smoking on lifespan is not due to some effect of education or income. However, it is never possible to include all possible confounding variables in an empirical analysis. For example, a hypothetical gene might increase mortality and also cause people to smoke more. For this reason, <a href="randomized_controlled_trial" title="wikilink">randomized controlled trials</a> are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data. When controlled experiments are not feasible, variants of regression analysis such as <a href="instrumental_variables" title="wikilink">instrumental variables</a> regression may be used to attempt to estimate causal relationships from observational data.</p>
<h3 id="finance">Finance</h3>

<p>The <a href="capital_asset_pricing_model" title="wikilink">capital asset pricing model</a> uses linear regression as well as the concept of <a href="Beta_(finance)" title="wikilink">beta</a> for analyzing and quantifying the systematic risk of an investment. This comes directly from the beta coefficient of the linear regression model that relates the return on the investment to the return on all risky assets.</p>
<h3 id="economics">Economics</h3>

<p>Linear regression is the predominant empirical tool in <a class="uri" href="economics" title="wikilink">economics</a>. For example, it is used to predict <a href="consumption_(economics)" title="wikilink">consumption spending</a>,<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a> <a href="fixed_investment" title="wikilink">fixed investment</a> spending, <a href="inventory_investment" title="wikilink">inventory investment</a>, purchases of a country's <a class="uri" href="exports" title="wikilink">exports</a>,<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> spending on <a class="uri" href="imports" title="wikilink">imports</a>,<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> the <a href="money_demand" title="wikilink">demand to hold liquid assets</a>,<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a> <a href="Labour_economics" title="wikilink">labor demand</a>,<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a> and <a href="labor_supply" title="wikilink">labor supply</a>.<a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a></p>
<h3 id="environmental-science">Environmental science</h3>

<p>Linear regression finds application in a wide range of environmental science applications. In Canada, the Environmental Effects Monitoring Program uses statistical analyses on fish and <a href="Benthic_zone" title="wikilink">benthic</a> surveys to measure the effects of pulp mill or metal mine effluent on the aquatic ecosystem.<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Analysis_of_variance" title="wikilink">Analysis of variance</a></li>
<li><a href="Censored_regression_model" title="wikilink">Censored regression model</a></li>
<li><a href="Cross-sectional_regression" title="wikilink">Cross-sectional regression</a></li>
<li><a href="Curve_fitting" title="wikilink">Curve fitting</a></li>
<li><a href="Empirical_Bayes_methods" title="wikilink">Empirical Bayes methods</a></li>
<li><a href="Errors_and_residuals" title="wikilink">Errors and residuals</a></li>
<li><a href="Lack-of-fit_sum_of_squares" title="wikilink">Lack-of-fit sum of squares</a></li>
<li><a href="Linear_classifier" title="wikilink">Linear classifier</a></li>
<li><a href="Logistic_regression" title="wikilink">Logistic regression</a></li>
<li><a class="uri" href="M-estimator" title="wikilink">M-estimator</a></li>
<li><a href="MLPACK_(C++_library)" title="wikilink">MLPACK</a> contains a <a class="uri" href="C++" title="wikilink">C++</a> implementation of linear regression</li>
<li><a href="Multivariate_adaptive_regression_splines" title="wikilink">Multivariate adaptive regression splines</a></li>
<li><a href="Nonlinear_regression" title="wikilink">Nonlinear regression</a></li>
<li><a href="Nonparametric_regression" title="wikilink">Nonparametric regression</a></li>
<li><a href="Normal_equations" title="wikilink">Normal equations</a></li>
<li><a href="Projection_pursuit_regression" title="wikilink">Projection pursuit regression</a></li>
<li><a href="Segmented_regression" title="wikilink">Segmented linear regression</a></li>
<li><a href="Stepwise_regression" title="wikilink">Stepwise regression</a></li>
<li><a href="Support_vector_machine" title="wikilink">Support vector machine</a></li>
<li><a href="Truncated_regression_model" title="wikilink">Truncated regression model</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li>Cohen, J., Cohen P., West, S.G., &amp; Aiken, L.S. (2003). <em>Applied multiple regression/correlation analysis for the behavioral sciences.</em> (2nd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates</li>
<li><a href="Charles_Darwin" title="wikilink">Charles Darwin</a>. <em>The Variation of Animals and Plants under Domestication</em>. (1868) <em>(Chapter XIII describes what was known about reversion in Galton's time. Darwin uses the term "reversion".)</em></li>
<li></li>
<li>Francis Galton. "Regression Towards Mediocrity in Hereditary Stature," <em>Journal of the Anthropological Institute</em>, 15:246-263 (1886). <em>(Facsimile at: <a href="http://www.mugu.com/galton/essays/1880-1889/galton-1886-jaigi-regression-stature.pdf">1</a>)</em></li>
<li>Robert S. Pindyck and Daniel L. Rubinfeld (1998, 4h ed.). <em>Econometric Models and Economic Forecasts</em>, ch. 1 (Intro, incl. appendices on Œ£ operators &amp; derivation of parameter est.) &amp; Appendix 4.3 (mult. regression in matrix form).</li>
</ul>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://knowpapa.com/trend-line/">Online Linear Regression Calculator &amp; Trend Line Graphing Tool</a></li>
<li><a href="http://codingplayground.blogspot.it/2013/05/learning-linear-regression-with.html">Using gradient descent in C++, Boost, Ublas for linear regression</a></li>
<li><a href="http://people.duke.edu/~rnau/regintro.htm">Lecture notes on linear regression analysis (Robert Nau, Duke University)</a></li>
</ul>

<p><a href="tr:Regresyon_analizi" title="wikilink">tr:Regresyon analizi</a>"</p>

<p><a href="Category:Articles_with_inconsistent_citation_formats" title="wikilink">Category:Articles with inconsistent citation formats</a> <a href="Category:Regression_analysis" title="wikilink">Category:Regression analysis</a> <a href="Category:Estimation_theory" title="wikilink">Category:Estimation theory</a> <a href="Category:Parametric_statistics" title="wikilink">Category:Parametric statistics</a> <a class="uri" href="Category:Econometrics" title="wikilink">Category:Econometrics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
<li id="fn2">.<a href="#fnref2">‚Ü©</a></li>
<li id="fn3"><a href="#fnref3">‚Ü©</a></li>
<li id="fn4"><a href="#fnref4">‚Ü©</a></li>
<li id="fn5"></li>
<li id="fn6"></li>
<li id="fn7"></li>
<li id="fn8"></li>
<li id="fn9"><a href="#fnref9">‚Ü©</a></li>
<li id="fn10">Warne, R. T. (2011). Beyond multiple regression: Using commonality analysis to better understand R2 results. <em>Gifted Child Quarterly, 55</em>, 313-318. <a class="uri" href="doi:10.1177/0016986211422217">doi:10.1177/0016986211422217</a><a href="#fnref10">‚Ü©</a></li>
<li id="fn11"><a href="#fnref11">‚Ü©</a></li>
<li id="fn12"><a href="#fnref12">‚Ü©</a></li>
<li id="fn13"><a href="#fnref13">‚Ü©</a></li>
<li id="fn14"><a href="#fnref14">‚Ü©</a></li>
<li id="fn15"><a href="#fnref15">‚Ü©</a></li>
<li id="fn16"><a href="#fnref16">‚Ü©</a></li>
<li id="fn17"><a href="#fnref17">‚Ü©</a></li>
<li id="fn18"><a href="#fnref18">‚Ü©</a></li>
<li id="fn19"><a href="#fnref19">‚Ü©</a></li>
<li id="fn20"><a href="#fnref20">‚Ü©</a></li>
<li id="fn21"><a href="#fnref21">‚Ü©</a></li>
<li id="fn22"><a href="#fnref22">‚Ü©</a></li>
<li id="fn23">; .<a href="#fnref23">‚Ü©</a></li>
<li id="fn24">Wilkinson, J.H. (1963) "Chapter 3: Matrix Computations", <em>Rounding Errors in Algebraic Processes</em>, London: Her Majesty's Stationery Office (National Physical Laboratory, Notes in Applied Science, No.32)<a href="#fnref24">‚Ü©</a></li>
<li id="fn25"><a href="#fnref25">‚Ü©</a></li>
<li id="fn26"></li>
<li id="fn27"><a href="#fnref27">‚Ü©</a></li>
<li id="fn28"><a href="#fnref28">‚Ü©</a></li>
<li id="fn29"></li>
<li id="fn30"><a href="#fnref30">‚Ü©</a></li>
<li id="fn31">[<a class="uri" href="http://www.ec.gc.ca/esee-eem/default.asp?lang=En&amp;n">http://www.ec.gc.ca/esee-eem/default.asp?lang=En&amp;n;</a>;=453D78FC-1 EEMP webpage]<a href="#fnref31">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
