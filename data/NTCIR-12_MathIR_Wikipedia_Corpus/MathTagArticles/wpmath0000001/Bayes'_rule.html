<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1600">Bayes' rule</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Bayes' rule</h1>
<hr/>
<p>In <a href="probability_theory" title="wikilink">probability theory</a> and applications, <strong>Bayes' rule</strong> relates the <a class="uri" href="odds" title="wikilink">odds</a> of event <span class="LaTeX">$A_1$</span> to the odds of event <span class="LaTeX">$A_2$</span>, before (prior to) and after (posterior to) <a href="Conditional_probability" title="wikilink">conditioning</a> on another event <span class="LaTeX">$B$</span>. The odds on <span class="LaTeX">$A_1$</span> to event <span class="LaTeX">$A_2$</span> is simply the ratio of the probabilities of the two events. The prior odds is the ratio of the unconditional or prior probabilities, the posterior odds is the ratio of conditional or posterior probabilities given the event <span class="LaTeX">$B$</span>. The relationship is expressed in terms of the <strong>likelihood ratio</strong> or <strong>Bayes factor</strong>, <span class="LaTeX">$\Lambda$</span>. By definition, this is the ratio of the conditional probabilities of the event <span class="LaTeX">$B$</span> given that <span class="LaTeX">$A_1$</span> is the case or that <span class="LaTeX">$A_2$</span> is the case, respectively. The rule simply states: <strong>posterior odds equals prior odds times Bayes factor</strong> (Gelman et al., 2005, Chapter 1).</p>
<p>When arbitrarily many events <span class="LaTeX">$A$</span> are of interest, not just two, the rule can be rephrased as <strong>posterior is proportional to prior times likelihood</strong>, <span class="LaTeX">$P(A|B)\propto P(A) P(B|A)$</span> where the proportionality symbol means that the left hand side is proportional to (i.e., equals a constant times) the right hand side as <span class="LaTeX">$A$</span> varies, for fixed or given <span class="LaTeX">$B$</span> (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005).</p>
<p>Bayes' rule is an equivalent way to formulate <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>. If we know the odds for and against <span class="LaTeX">$A$</span> we also know the probabilities of <span class="LaTeX">$A$</span>. It may be preferred to Bayes' theorem in practice for a number of reasons.</p>
<p>Bayes' rule is widely used in <a class="uri" href="statistics" title="wikilink">statistics</a>, <a class="uri" href="science" title="wikilink">science</a> and <a class="uri" href="engineering" title="wikilink">engineering</a>, for instance in <a href="Bayesian_model_selection" title="wikilink">model selection</a>, probabilistic <a href="expert_systems" title="wikilink">expert systems</a> based on <a href="Bayes_networks" title="wikilink">Bayes networks</a>, <a href="statistical_proof" title="wikilink">statistical proof</a> in legal proceedings, email spam filters, and so on (Rosenthal, 2005; Bertsch McGrayne, 2012). As an elementary fact from the calculus of probability, Bayes' rule tells us how unconditional and conditional probabilities are related whether we work with a <a href="frequentist_interpretation_of_probability" title="wikilink">frequentist interpretation of probability</a> or a <a href="Bayesian_probability" title="wikilink">Bayesian interpretation of probability</a>. Under the Bayesian interpretation it is frequently applied in the situation where <span class="LaTeX">$A_1$</span> and <span class="LaTeX">$A_2$</span> are competing hypotheses, and <span class="LaTeX">$B$</span> is some observed evidence. The rule shows how one's judgement on whether <span class="LaTeX">$A_1$</span> or <span class="LaTeX">$A_2$</span> is true should be updated on observing the evidence <span class="LaTeX">$B$</span> (Gelman et al., 2003).</p>
<h2 id="the-rule">The rule</h2>
<h3 id="single-event">Single event</h3>
<p>Given events <span class="LaTeX">$A_1$</span>, <span class="LaTeX">$A_2$</span> and <span class="LaTeX">$B$</span>, Bayes' rule states that the conditional odds of <span class="LaTeX">$A_1:A_2$</span> given <span class="LaTeX">$B$</span> are equal to the marginal odds of <span class="LaTeX">$A_1:A_2$</span> multiplied by the <a href="Bayes_factor" title="wikilink">Bayes factor</a> or likelihood ratio <span class="LaTeX">$\Lambda$</span>:</p>
<p><span class="LaTeX">$$O(A_1:A_2|B) = \Lambda(A_1:A_2|B) \cdot O(A_1:A_2) ,$$</span></p>
<p>where</p>
<p><span class="LaTeX">$$\Lambda(A_1:A_2|B) = \frac{P(B|A_1)}{P(B|A_2)}.$$</span></p>
<p>Here, the odds and conditional odds, also known as prior odds and posterior odds, are defined by</p>
<p><span class="LaTeX">$$O(A_1:A_2) = \frac{P(A_1)}{P(A_2)},$$</span></p>
<p><span class="LaTeX">$$O(A_1:A_2|B) = \frac{P(A_1|B)}{P(A_2|B)}.$$</span></p>
<p>In the special case that <span class="LaTeX">$A_1 = A$</span> and <span class="LaTeX">$A_2 = \neg A$</span>, one writes <span class="LaTeX">$O(A)=O(A:\neg A)$</span>, and uses a similar abbreviation for the Bayes factor and for the conditional odds. The odds on <span class="LaTeX">$A$</span> is by definition the odds for and against <span class="LaTeX">$A$</span>. Bayes' rule can then be written in the abbreviated form</p>
<p><span class="LaTeX">$$O(A|B) = O(A)  \cdot \Lambda(A|B) ,$$</span></p>
<p>or in words: the posterior odds on <span class="LaTeX">$A$</span> equals the prior odds on <span class="LaTeX">$A$</span> times the likelihood ratio for <span class="LaTeX">$A$</span> given information <span class="LaTeX">$B$</span>. In short, <strong>posterior odds equals prior odds times likelihood ratio</strong>.</p>
<p>The rule is frequently applied when <span class="LaTeX">$A_1 = A$</span> and <span class="LaTeX">$A_2 = \neg A$</span> are two competing hypotheses concerning the cause of some event <span class="LaTeX">$B$</span>. The prior odds on <span class="LaTeX">$A$</span>, in other words, the odds between <span class="LaTeX">$A$</span> and <span class="LaTeX">$\neg A$</span>, expresses our initial beliefs concerning whether or not <span class="LaTeX">$A$</span> is true. The event <span class="LaTeX">$B$</span> represents some evidence, information, data, or observations. The likelihood ratio is the ratio of the chances of observing <span class="LaTeX">$B$</span> under the two hypotheses <span class="LaTeX">$A$</span> and <span class="LaTeX">$\neg A$</span>. The rule tells us how our prior beliefs concerning whether or not <span class="LaTeX">$A$</span> is true needs to be updated on receiving the information <span class="LaTeX">$B$</span>.</p>
<h3 id="many-events">Many events</h3>
<p>If we think of <span class="LaTeX">$A$</span> as arbitrary and <span class="LaTeX">$B$</span> as fixed then we can rewrite Bayes' theorem <span class="LaTeX">$P(A|B)=P(A)P(B|A)/P(B)$</span> in the form <span class="LaTeX">$P(A|B) \propto P(A)P(B|A)$</span> where the proportionality symbol means that, as <span class="LaTeX">$A$</span> varies but keeping <span class="LaTeX">$B$</span> fixed, the left hand side is equal to a constant times the right hand side.</p>
<p>In words <strong>posterior is proportional to prior times likelihood</strong>. This version of Bayes' theorem was first called "Bayes' rule" by Cournot (1843). Cournot popularized the earlier work of Laplace (1774) who had independently discovered Bayes' rule. The work of Bayes was published posthumously (1763) but remained more or less unknown till Cournot drew attention to it; see Fienberg (2006).</p>
<p>Bayes' rule may be preferred to the usual statement of Bayes' theorem for a number of reasons. One is that it is intuitively simpler to understand. Another reason is that normalizing probabilities is sometimes unnecessary: one sometimes only needs to know ratios of probabilities. Finally, doing the normalization is often easier to do after simplifying the product of prior and likelihood by deleting any factors which do not depend on <span class="LaTeX">$A$</span>, so we do not need to actually compute the denominator <span class="LaTeX">$P(B)$</span> in the usual statement of Bayes' theorem <span class="LaTeX">$P(A|B) = \frac{P(B | A)\, P(A)}{P(B)}\cdot \,$</span>.</p>
<p>In <a href="Bayesian_statistics" title="wikilink">Bayesian statistics</a>, Bayes' rule is often applied with a so-called <a href="improper_prior" title="wikilink">improper prior</a>, for instance, a uniform probability distribution over all real numbers. In that case, the prior distribution does not exist as a probability measure within conventional probability theory, and Bayes' theorem itself is not available.</p>
<h3 id="series-of-events">Series of events</h3>
<p>Bayes' rule may be applied a number of times. Each time we observe a new event, we update the odds between the events of interest, say <span class="LaTeX">$A_1$</span> and <span class="LaTeX">$A_2$</span> by taking account of the new information. For two events (information, evidence) <span class="LaTeX">$B$</span> and <span class="LaTeX">$C$</span>,</p>
<p><span class="LaTeX">$$O(A_1:A_2|B \cap C) = \Lambda(A_1:A_2|B \cap C) \cdot \Lambda(A_1:A_2|B) \cdot O(A_1:A_2) ,$$</span></p>
<p>where</p>
<p><span class="LaTeX">$$\Lambda(A_1:A_2|B) = \frac{P(B|A_1)}{P(B|A_2)} ,$$</span></p>
<p><span class="LaTeX">$$\Lambda(A_1:A_2|B \cap C) = \frac{P(C|A_1 \cap B)}{P(C|A_2 \cap B)} .$$</span></p>
<p>In the special case of two complementary events <span class="LaTeX">$A$</span> and <span class="LaTeX">$\neg A$</span>, the equivalent notation is</p>
<p><span class="LaTeX">$$O(A|B,C) = \Lambda(A|B \cap C) \cdot \Lambda(A|B) \cdot O(A).$$</span></p>
<h2 id="derivation">Derivation</h2>
<p>Consider two instances of <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>:</p>
<p><span class="LaTeX">$$P(A_1|B) = \frac{1}{P(B)} \cdot P(B|A_1) \cdot P(A_1),$$</span></p>
<p><span class="LaTeX">$$P(A_2|B) = \frac{1}{P(B)} \cdot P(B|A_2) \cdot P(A_2).$$</span></p>
<p>Combining these gives</p>
<p><span class="LaTeX">$$\frac{P(A_1|B)}{P(A_2|B)} = \frac{P(B|A_1)}{P(B|A_2)} \cdot \frac{P(A_1)}{P(A_2)}.$$</span></p>
<p>Now defining</p>
<p><span class="LaTeX">$$O(A_1:A_2|B)  \triangleq \frac{P(A_1|B)}{P(A_2|B)}$$</span></p>
<p><span class="LaTeX">$$O(A_1:A_2) \triangleq \frac{P(A_1)}{P(A_2)}$$</span></p>
<p><span class="LaTeX">$$\Lambda(A_1:A_2|B) \triangleq  \frac{P(B|A_1)}{P(B|A_2)},$$</span> this implies</p>
<p><span class="LaTeX">$$O(A_1:A_2|B) = \Lambda(A_1:A_2|B) \cdot O(A_1:A_2).$$</span></p>
<p>A similar derivation applies for conditioning on multiple events, using the appropriate <a href="Bayes'_theorem#Further_extensions" title="wikilink">extension of Bayes' theorem</a></p>
<h2 id="examples">Examples</h2>
<h3 id="frequentist-example">Frequentist example</h3>
<p>Consider the <a href="Bayes'_theorem#Drug_testing" title="wikilink">drug testing example</a> in the article on Bayes' theorem.</p>
<p>The same results may be obtained using Bayes' rule. The prior odds on an individual being a drug-user are 199 to 1 against, as <span class="LaTeX">$\textstyle 0.5\%=\frac{1}{200}$</span> and <span class="LaTeX">$\textstyle 99.5\%=\frac{199}{200}$</span>. The <a href="Bayes_factor" title="wikilink">Bayes factor</a> when an individual tests positive is <span class="LaTeX">$\textstyle \frac{0.99}{0.01} = 99:1$</span> in favour of being a drug-user: this is the ratio of the probability of a drug-user testing positive, to the probability of a non-drug user testing positive. The posterior odds on being a drug user are therefore <span class="LaTeX">$\textstyle 1 \times 99 : 199 \times 1 = 99:199$</span>, which is very close to <span class="LaTeX">$\textstyle 100:200 = 1:2$</span>. In round numbers, only one in three of those testing positive are actually drug-users.</p>
<h3 id="model-selection">Model selection</h3>
<h2 id="external-links">External links</h2>
<ul>
<li>Bessière, P, Mazer, E, Ahuactzin, JM and Mekhnacha, K (2013), "<a href="http://www.crcpress.com/product/isbn/9781439880326">Bayesian Programming</a>", CRC Press.</li>
</ul>
<ul>
<li>Fienberg, SE (2006), "When did Bayesian inference become "Bayesian"?"", <em>Bayesian analysis</em> vol. 1, nr. 1, pp. 1-40.</li>
</ul>
<ul>
<li>Gelman, A, Carlin, JB, Stern, HS and Rubin, DB (2003), "Bayesian Data Analysis", Second Edition, CRC Press.</li>
</ul>
<ul>
<li>Lee, PM (2012), "Bayesian Statistics: An Introduction", Wiley.</li>
</ul>
<ul>
<li>McGrayne, SB (2012), "The Theory That Would Not Die: How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy", Yale University Press.</li>
</ul>
<ul>
<li><a href="http://www.inference.phy.cam.ac.uk/mackay/itila/">The on-line textbook: Information Theory, Inference, and Learning Algorithms</a>, by <a href="MacKay,_DJC" title="wikilink">MacKay, DJC</a>, discusses Bayesian model comparison in Chapters 3 and 28.</li>
</ul>
<ul>
<li><span class="citation" id="refRosenthal2005b">Rosenthal, JS (2005): <em>Struck by Lightning: the Curious World of Probabilities</em>. Harper Collings 2005, ISBN 978-0-00-200791-7.</span></li>
</ul>
<ul>
<li>Stone, JV (2013), "Bayes’ Rule: A Tutorial Introduction to Bayesian Analysis", <a href="http://jim-stone.staff.shef.ac.uk/BookBayes2012/BayesRuleBookMain.html">Download chapter 1</a>, Sebtel Press, England.</li>
</ul>
<p><a href="ar:عامل_بايز" title="wikilink">ar:عامل بايز</a> <a class="uri" href="ja:ベイズ因子" title="wikilink">ja:ベイズ因子</a>"</p>
<p><a href="Category:Bayesian_inference" title="wikilink">Rule</a> <a href="Category:Model_selection" title="wikilink">Category:Model selection</a> <a href="Category:Statistical_ratios" title="wikilink">Category:Statistical ratios</a></p>
</body>
</html>
