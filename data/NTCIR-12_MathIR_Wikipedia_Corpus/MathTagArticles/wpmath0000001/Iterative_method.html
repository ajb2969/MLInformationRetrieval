<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="540">Iterative method</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Iterative method</h1>
<hr/>

<p>In <a href="computational_mathematics" title="wikilink">computational mathematics</a>, an <strong>iterative method</strong> is a mathematical procedure that generates a sequence of improving approximate solutions for a class of problems. A specific implementation of an iterative method, including the <a href="Algorithm#Termination" title="wikilink">termination</a> criteria, is an <a class="uri" href="algorithm" title="wikilink">algorithm</a> of the iterative method. An iterative method is called <strong>convergent</strong> if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, <a class="uri" href="heuristic" title="wikilink">heuristic</a>-based iterative methods are also common.</p>

<p>In the problems of <a href="root-finding_algorithm" title="wikilink">finding the root</a> of an equation (or a solution of a system of equations), an iterative method uses an initial guess to generate successive <a href="approximation" title="wikilink">approximations</a> to a solution. In contrast, <strong>direct methods</strong> attempt to solve the problem by a finite sequence of operations. In the absence of <a href="rounding_error" title="wikilink">rounding errors</a>, direct methods would deliver an exact solution (like solving a linear system of equations 

<math display="inline" id="Iterative_method:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>A</mi>
    <mi>ùï©</mi>
   </mrow>
   <mo>=</mo>
   <mi>ùïì</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>A</ci>
     <ci>ùï©</ci>
    </apply>
    <ci>ùïì</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A\mathbb{x}=\mathbb{b}
  </annotation>
 </semantics>
</math>

 by <a href="Gaussian_elimination" title="wikilink">Gaussian elimination</a>). Iterative methods are often the only choice for <a href="nonlinear_equation" title="wikilink">nonlinear equations</a>. However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.</p>
<h2 id="attractive-fixed-points">Attractive fixed points</h2>

<p>If an equation can be put into the form <em>f</em>(<em>x</em>) = <em>x</em>, and a solution <strong>x</strong> is an attractive <a href="fixed_point_(mathematics)" title="wikilink">fixed point</a> of the function <em>f</em>, then one may begin with a point <em>x</em><sub>1</sub> in the <a href="basin_of_attraction" title="wikilink">basin of attraction</a> of <strong>x</strong>, and let <em>x</em><sub><em>n</em>+1</sub> = <em>f</em>(<em>x</em><sub><em>n</em></sub>) for <em>n</em>¬†‚â•¬†1, and the sequence {<em>x</em><sub><em>n</em></sub>}<sub><em>n</em>¬†‚â•¬†1</sub> will converge to the solution <strong>x</strong>. Here <em>x</em><sub><em>n</em></sub> is the <em>n</em>th approximation or iteration of <em>x</em> and <em>x</em><sub><em>n</em>+1</sub> is the next or <em>n</em> + 1 iteration of <em>x</em>. Alternately, superscripts in parentheses are often used in numerical methods, so as not to interfere with subscripts with other meanings. (For example, <em>x</em><sup>(<em>n</em>+1)</sup> = <em>f</em>(<em>x</em><sup>(<em>n</em>)</sup>).) If the function <em>f</em> is continuously differentiable, a sufficient condition for convergence is that the spectral radius of the derivative is strictly bounded by one in a neighborhood of the fixed point. If this condition holds at the fixed point, then a sufficiently small neighborhood (basin of attraction) must exist.</p>
<h2 id="linear-systems">Linear systems</h2>

<p>In the case of a <a href="system_of_linear_equations" title="wikilink">system of linear equations</a>, the two main classes of iterative methods are the <strong>stationary iterative methods</strong>, and the more general <a href="Krylov_subspace" title="wikilink">Krylov subspace</a> methods.</p>
<h3 id="stationary-iterative-methods">Stationary iterative methods</h3>

<p>Stationary iterative methods solve a linear system with an <a href="Operator_(mathematics)" title="wikilink">operator</a> approximating the original one; and based on a measurement of the error in the result (<a href="Residual_(numerical_analysis)" title="wikilink">the residual</a>), form a "correction equation" for which this process is repeated. While these methods are simple to derive, implement, and analyze, convergence is only guaranteed for a limited class of matrices. Examples of stationary iterative methods are the <a href="Jacobi_method" title="wikilink">Jacobi method</a>, <a href="Gauss‚ÄìSeidel_method" title="wikilink">Gauss‚ÄìSeidel method</a> and the <a href="Successive_over-relaxation_method" title="wikilink">Successive over-relaxation method</a>. Linear stationary iterative methods are also called <a href="Relaxation_(iterative_method)" title="wikilink">relaxation methods</a>.</p>
<h3 id="krylov-subspace-methods">Krylov subspace methods</h3>

<p><a href="Krylov_subspace" title="wikilink">Krylov subspace</a> methods work by forming a <a href="basis_(linear_algebra)" title="wikilink">basis</a> of the sequence of successive matrix powers times the initial residual (the <strong>Krylov sequence</strong>). The approximations to the solution are then formed by minimizing the residual over the subspace formed. The prototypical method in this class is the <a href="conjugate_gradient_method" title="wikilink">conjugate gradient method</a> (CG). Other methods are the <a href="generalized_minimal_residual_method" title="wikilink">generalized minimal residual method</a> (GMRES) and the <a href="biconjugate_gradient_method" title="wikilink">biconjugate gradient method</a> (BiCG).</p>
<h3 id="convergence-of-krylov-subspace-methods">Convergence of Krylov subspace methods</h3>

<p>Since these methods form a basis, it is evident that the method converges in <em>N</em> iterations, where <em>N</em> is the system size. However, in the presence of rounding errors this statement does not hold; moreover, in practice <em>N</em> can be very large, and the iterative process reaches sufficient accuracy already far earlier. The analysis of these methods is hard, depending on a complicated function of the <a href="spectrum_of_an_operator" title="wikilink">spectrum</a> of the operator.</p>
<h3 id="preconditioners">Preconditioners</h3>

<p>The approximating operator that appears in stationary iterative methods can also be incorporated in <a href="Krylov_subspace_methods" title="wikilink">Krylov subspace methods</a> such as <a class="uri" href="GMRES" title="wikilink">GMRES</a> (alternatively, <a href="preconditioning" title="wikilink">preconditioned</a> Krylov methods can be considered as accelerations of stationary iterative methods), where they become transformations of the original operator to a presumably better conditioned one. The construction of preconditioners is a large research area.</p>
<h3 id="history">History</h3>

<p>Probably the first iterative method for solving a linear system appeared in a letter of <a href="Carl_Friedrich_Gauss" title="wikilink">Gauss</a> to a student of his. He proposed solving a 4-by-4 system of equations by repeatedly solving the component in which the residual was the largest.</p>

<p>The theory of stationary iterative methods was solidly established with the work of <a href="D.M._Young" title="wikilink">D.M. Young</a> starting in the 1950s. The <a href="Conjugate_Gradient_method" title="wikilink">Conjugate Gradient method</a> was also invented in the 1950s, with independent developments by <a href="Cornelius_Lanczos" title="wikilink">Cornelius Lanczos</a>, <a href="Magnus_Hestenes" title="wikilink">Magnus Hestenes</a> and <a href="Eduard_Stiefel" title="wikilink">Eduard Stiefel</a>, but its nature and applicability were misunderstood at the time. Only in the 1970s was it realized that conjugacy based methods work very well for <a href="partial_differential_equation" title="wikilink">partial differential equations</a>, especially the elliptic type.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Matrix_splitting" title="wikilink">Matrix splitting</a></li>
<li><a href="Root-finding_algorithm" title="wikilink">Root-finding algorithm</a></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.netlib.org/linalg/html_templates/Templates.html">Templates for the Solution of Linear Systems</a></li>
<li><a href="http://www-users.cs.umn.edu/~saad/books.html">Y. Saad: <em>Iterative Methods for Sparse Linear Systems</em>, 1st edition, PWS 1996</a></li>
</ul>

<p>"</p>

<p><a href="Category:Iterative_methods" title="wikilink">Category:Iterative methods</a> <a href="Category:Numerical_analysis" title="wikilink">Category:Numerical analysis</a></p>
</body>
</html>
