<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1595">Bayesian inference</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Bayesian inference</h1>
<hr/>

<p><strong>Bayesian inference</strong> is a method of <a href="statistical_inference" title="wikilink">statistical inference</a> in which <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a> is used to update the probability for a hypothesis as <a class="uri" href="evidence" title="wikilink">evidence</a> is acquired. Bayesian inference is an important technique in <a class="uri" href="statistics" title="wikilink">statistics</a>, and especially in <a href="mathematical_statistics" title="wikilink">mathematical statistics</a>. Bayesian updating is particularly important in the <a href="Sequential_analysis" title="wikilink">dynamic analysis of a sequence of data</a>. Bayesian inference has found application in a wide range of activities, including <a class="uri" href="science" title="wikilink">science</a>, <a class="uri" href="engineering" title="wikilink">engineering</a>, <a class="uri" href="philosophy" title="wikilink">philosophy</a>, <a class="uri" href="medicine" title="wikilink">medicine</a>, and <a class="uri" href="law" title="wikilink">law</a>. In the philosophy of <a href="decision_theory" title="wikilink">decision theory</a>, Bayesian inference is closely related to subjective probability, often called "<a href="Bayesian_probability" title="wikilink">Bayesian probability</a>". Bayesian probability provides a <a href="Rationality" title="wikilink">rational</a> method for updating beliefs.</p>
<h2 id="introduction-to-bayes-rule">Introduction to Bayes' rule</h2>

<p> </p>
<h3 id="formal">Formal</h3>

<p>Bayesian inference derives the <a href="posterior_probability" title="wikilink">posterior probability</a> as a <a href="consequence_relation" title="wikilink">consequence</a> of two <a href="Antecedent_(logic)" title="wikilink">antecedents</a>, a <a href="prior_probability" title="wikilink">prior probability</a> and a "<a href="likelihood_function" title="wikilink">likelihood function</a>" derived from a <a href="statistical_model" title="wikilink">statistical model</a> for the observed data. Bayesian inference computes the posterior probability according to <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>:</p>

<p>

<math display="block" id="Bayesian_inference:0">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo>∣</mo>
      <mi>H</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo>⋅</mo>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>H</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">H</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <ci>normal-⋅</ci>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">H</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <apply>
      <times></times>
      <ci>P</ci>
      <ci>E</ci>
     </apply>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(H\mid E)=\frac{P(E\mid H)\cdot P(H)}{P(E)}
  </annotation>
 </semantics>
</math>

 where</p>
<ul>
<li>

<math display="inline" id="Bayesian_inference:1">
 <semantics>
  <mo>∣</mo>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-∣</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\mid
  </annotation>
 </semantics>
</math>

 denotes a <a href="conditional_probability" title="wikilink">conditional probability</a>; more specifically, it means <em>given</em>.</li>
<li>

<math display="inline" id="Bayesian_inference:2">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H
  </annotation>
 </semantics>
</math>

 stands for any <em>hypothesis</em> whose probability may be affected by <a href="Experimental_data" title="wikilink">data</a> (called <em>evidence</em> below). Often there are competing hypotheses, from which one chooses the most probable.</li>
<li>the <em>evidence</em> 

<math display="inline" id="Bayesian_inference:3">
 <semantics>
  <mi>E</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>E</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E
  </annotation>
 </semantics>
</math>


 corresponds to new data that were not used in computing the prior probability.</li>
<li>

<math display="inline" id="Bayesian_inference:4">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <ci>H</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle P(H)
  </annotation>
 </semantics>
</math>

, the <em><a href="prior_probability" title="wikilink">prior probability</a></em>, is the probability of 

<math display="inline" id="Bayesian_inference:5">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H
  </annotation>
 </semantics>
</math>

 <em>before</em> 

<math display="inline" id="Bayesian_inference:6">
 <semantics>
  <mi>E</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>E</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E
  </annotation>
 </semantics>
</math>

 is observed. This indicates one's previous estimate of the probability that a hypothesis is true, before gaining the current evidence.</li>
<li>

<math display="inline" id="Bayesian_inference:7">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle P(H\mid E)
  </annotation>
 </semantics>
</math>

, the <em><a href="posterior_probability" title="wikilink">posterior probability</a></em>, is the probability of 

<math display="inline" id="Bayesian_inference:8">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H
  </annotation>
 </semantics>
</math>


 <em>given</em> 

<math display="inline" id="Bayesian_inference:9">
 <semantics>
  <mi>E</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>E</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E
  </annotation>
 </semantics>
</math>

, i.e., <em>after</em> 

<math display="inline" id="Bayesian_inference:10">
 <semantics>
  <mi>E</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>E</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E
  </annotation>
 </semantics>
</math>

 is observed. This tells us what we want to know: the probability of a hypothesis <em>given</em> the observed evidence.</li>
<li>

<math display="inline" id="Bayesian_inference:11">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo>∣</mo>
    <mi>H</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle P(E\mid H)
  </annotation>
 </semantics>
</math>

 is the probability of observing 

<math display="inline" id="Bayesian_inference:12">
 <semantics>
  <mi>E</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>E</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E
  </annotation>
 </semantics>
</math>

 <em>given</em> 

<math display="inline" id="Bayesian_inference:13">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H
  </annotation>
 </semantics>
</math>


. As a function of 

<math display="inline" id="Bayesian_inference:14">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H
  </annotation>
 </semantics>
</math>

 with 

<math display="inline" id="Bayesian_inference:15">
 <semantics>
  <mi>E</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>E</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E
  </annotation>
 </semantics>
</math>

 fixed, this is the <em><a href="Likelihood_function" title="wikilink">likelihood</a></em>. The likelihood function should <strong>not</strong> be confused with 

<math display="inline" id="Bayesian_inference:16">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle P(H\mid E)
  </annotation>
 </semantics>
</math>

 as a function of 

<math display="inline" id="Bayesian_inference:17">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H
  </annotation>
 </semantics>
</math>

 rather than of 

<math display="inline" id="Bayesian_inference:18">
 <semantics>
  <mi>E</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>E</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E
  </annotation>
 </semantics>
</math>


. It indicates the compatibility of the evidence with the given hypothesis.</li>
<li>

<math display="inline" id="Bayesian_inference:19">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <ci>E</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle P(E)
  </annotation>
 </semantics>
</math>

 is sometimes termed the <a href="marginal_likelihood" title="wikilink">marginal likelihood</a> or "model evidence". This factor is the same for all possible hypotheses being considered. (This can be seen by the fact that the hypothesis 

<math display="inline" id="Bayesian_inference:20">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H
  </annotation>
 </semantics>
</math>

 does not appear anywhere in the symbol, unlike for all the other factors.) This means that this factor does not enter into determining the relative probabilities of different hypotheses.</li>
</ul>

<p>Note that, for different values of 

<math display="inline" id="Bayesian_inference:21">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H
  </annotation>
 </semantics>
</math>

, only the factors 

<math display="inline" id="Bayesian_inference:22">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <ci>H</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle P(H)
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Bayesian_inference:23">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo>∣</mo>
    <mi>H</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle P(E\mid H)
  </annotation>
 </semantics>
</math>


 affect the value of 

<math display="inline" id="Bayesian_inference:24">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle P(H\mid E)
  </annotation>
 </semantics>
</math>

. As both of these factors appear in the numerator, the posterior probability is proportional to both. In words:</p>
<ul>
<li>(more precisely) <em>The posterior probability of a hypothesis is determined by a combination of the inherent likeliness of a hypothesis (the prior) and the compatibility of the observed evidence with the hypothesis (the likelihood).</em></li>
<li>(more concisely) <em>Posterior is proportional to likelihood times prior.</em></li>
</ul>

<p>Note that Bayes' rule can also be written as follows:</p>

<p>

<math display="block" id="Bayesian_inference:25">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo>∣</mo>
      <mi>H</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
   <mo>⋅</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">H</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <apply>
      <times></times>
      <ci>P</ci>
      <ci>E</ci>
     </apply>
    </apply>
    <ci>normal-⋅</ci>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(H\mid E)=\frac{P(E\mid H)}{P(E)}\cdot P(H)
  </annotation>
 </semantics>
</math>

 where the factor 

<math display="inline" id="Bayesian_inference:26">
 <semantics>
  <mfrac>
   <mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>E</mi>
     <mo>∣</mo>
     <mi>H</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>E</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <csymbol cd="unknown">P</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">E</csymbol>
      <ci>normal-∣</ci>
      <csymbol cd="unknown">H</csymbol>
      <ci>normal-)</ci>
     </cerror>
    </cerror>
    <apply>
     <times></times>
     <ci>P</ci>
     <ci>E</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\frac{P(E\mid H)}{P(E)}
  </annotation>
 </semantics>
</math>

 represents the impact of 

<math display="inline" id="Bayesian_inference:27">
 <semantics>
  <mi>E</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>E</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   E
  </annotation>
 </semantics>
</math>

 on the probability of 

<math display="inline" id="Bayesian_inference:28">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H
  </annotation>
 </semantics>
</math>


.</p>
<h3 id="informal">Informal</h3>

<p>If the evidence does not match up with a hypothesis, one should reject the hypothesis. But if a hypothesis is extremely unlikely <em>a priori</em>, one should also reject it, even if the evidence does appear to match up.</p>

<p>For example, imagine that I have various hypotheses about the nature of a newborn baby of a friend, including:</p>
<ul>
<li>

<math display="inline" id="Bayesian_inference:29">
 <semantics>
  <msub>
   <mi>H</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>H</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H_{1}
  </annotation>
 </semantics>
</math>

: the baby is a brown-haired boy.</li>
<li>

<math display="inline" id="Bayesian_inference:30">
 <semantics>
  <msub>
   <mi>H</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>H</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H_{2}
  </annotation>
 </semantics>
</math>

: the baby is a blond-haired girl.</li>
<li>

<math display="inline" id="Bayesian_inference:31">
 <semantics>
  <msub>
   <mi>H</mi>
   <mn>3</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>H</ci>
    <cn type="integer">3</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H_{3}
  </annotation>
 </semantics>
</math>

: the baby is a dog.</li>
</ul>

<p>Then consider two scenarios:</p>
<ol>
<li>I'm presented with evidence in the form of a picture of a blond-haired baby girl. I find this evidence supports 

<math display="inline" id="Bayesian_inference:32">
 <semantics>
  <msub>
   <mi>H</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>H</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H_{2}
  </annotation>
 </semantics>
</math>

 and opposes 

<math display="inline" id="Bayesian_inference:33">
 <semantics>
  <msub>
   <mi>H</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>H</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H_{1}
  </annotation>
 </semantics>
</math>


 and 

<math display="inline" id="Bayesian_inference:34">
 <semantics>
  <msub>
   <mi>H</mi>
   <mn>3</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>H</ci>
    <cn type="integer">3</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H_{3}
  </annotation>
 </semantics>
</math>

.</li>
<li>I'm presented with evidence in the form of a picture of a baby dog. Although this evidence, treated in isolation, supports 

<math display="inline" id="Bayesian_inference:35">
 <semantics>
  <msub>
   <mi>H</mi>
   <mn>3</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>H</ci>
    <cn type="integer">3</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle H_{3}
  </annotation>
 </semantics>
</math>

, my prior belief in this hypothesis (that a human can give birth to a dog) is extremely small, so the posterior probability is nevertheless small.</li>
</ol>

<p>The critical point about Bayesian inference, then, is that it provides a principled way of combining new evidence with prior beliefs, through the application of Bayes' rule. (Contrast this with frequentist inference, which relies only on the evidence as a whole, with no reference to prior beliefs.) Furthermore, Bayes' rule can be applied iteratively: after observing some evidence, the resulting posterior probability can then be treated as a prior probability, and a new posterior probability computed from new evidence. This allows for Bayesian principles to be applied to various kinds of evidence, whether viewed all at once or over time. This procedure is termed "Bayesian updating".</p>
<h3 id="bayesian-updating">Bayesian updating</h3>

<p>Bayesian updating is widely used and computationally convenient. However, it is not the only updating rule that might be considered "rational".</p>

<p><a href="Ian_Hacking" title="wikilink">Ian Hacking</a> noted that traditional "<a href="Dutch_book" title="wikilink">Dutch book</a>" arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. <a href="Ian_Hacking" title="wikilink">Hacking</a> wrote<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> "And neither the Dutch book argument, nor any other in the personalist arsenal of proofs of the probability axioms, entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour."</p>

<p>Indeed, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on "<a href="probability_kinematics" title="wikilink">probability kinematics</a>" following the publication of <a href="Richard_C._Jeffrey" title="wikilink">Richard C. Jeffrey</a>'s rule, which applies Bayes' rule to the case where the evidence itself is assigned a probability.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> The additional hypotheses needed to uniquely require Bayesian updating have been deemed to be substantial, complicated, and unsatisfactory.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="formal-description-of-bayesian-inference">Formal description of Bayesian inference</h2>
<h3 id="definitions">Definitions</h3>
<ul>
<li>

<math display="inline" id="Bayesian_inference:36">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

, a data point in general. This may in fact be a <a href="random_vector" title="wikilink">vector</a> of values.</li>
<li>

<math display="inline" id="Bayesian_inference:37">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta
  </annotation>
 </semantics>
</math>

, the <a class="uri" href="parameter" title="wikilink">parameter</a> of the data point's distribution, i.e., 

<math display="inline" id="Bayesian_inference:38">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>∼</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">x</csymbol>
    <csymbol cd="latexml">similar-to</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\sim p(x\mid\theta)
  </annotation>
 </semantics>
</math>


 . This may in fact be a <a href="random_vector" title="wikilink">vector</a> of parameters.</li>
<li>

<math display="inline" id="Bayesian_inference:39">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

, the <a class="uri" href="hyperparameter" title="wikilink">hyperparameter</a> of the parameter, i.e., 

<math display="inline" id="Bayesian_inference:40">
 <semantics>
  <mrow>
   <mi>θ</mi>
   <mo>∼</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">θ</csymbol>
    <csymbol cd="latexml">similar-to</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta\sim p(\theta\mid\alpha)
  </annotation>
 </semantics>
</math>

 . This may in fact be a <a href="random_vector" title="wikilink">vector</a> of hyperparameters.</li>
<li>

<math display="inline" id="Bayesian_inference:41">
 <semantics>
  <mi>𝐗</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐗</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{X}
  </annotation>
 </semantics>
</math>

, a set of 

<math display="inline" id="Bayesian_inference:42">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 observed data points, i.e., 

<math display="inline" id="Bayesian_inference:43">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1},\ldots,x_{n}
  </annotation>
 </semantics>
</math>


.</li>
<li>

<math display="inline" id="Bayesian_inference:44">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">~</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-~</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tilde{x}
  </annotation>
 </semantics>
</math>

, a new data point whose distribution is to be predicted.</li>
</ul>
<h3 id="bayesian-inference">Bayesian inference</h3>
<ul>
<li>The <a href="prior_distribution" title="wikilink">prior distribution</a> is the distribution of the parameter(s) before any data is observed, i.e. 

<math display="inline" id="Bayesian_inference:45">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\theta\mid\alpha)
  </annotation>
 </semantics>
</math>

 .</li>
<li>The prior distribution might not be easily determined. In this case, we can use the <a href="Jeffreys_prior" title="wikilink">Jeffreys prior</a> to obtain the posterior distribution before updating them with newer observations.</li>
</ul>
<ul>
<li>The <a href="sampling_distribution" title="wikilink">sampling distribution</a> is the distribution of the observed data conditional on its parameters, i.e. 

<math display="inline" id="Bayesian_inference:46">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐗</mi>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\mathbf{X}\mid\theta)
  </annotation>
 </semantics>
</math>

 . This is also termed the <a href="likelihood_function" title="wikilink">likelihood</a>, especially when viewed as a function of the parameter(s), sometimes written 

<math display="inline" id="Bayesian_inference:47">
 <semantics>
  <mrow>
   <mo>L</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>𝐗</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐗</mi>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <ci>normal-L</ci>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{L}(\theta\mid\mathbf{X})=p(\mathbf{X}\mid\theta)
  </annotation>
 </semantics>
</math>

 .</li>
<li>The <a href="marginal_likelihood" title="wikilink">marginal likelihood</a> (sometimes also termed the <em>evidence</em>) is the distribution of the observed data <a href="marginal_distribution" title="wikilink">marginalized</a> over the parameter(s), i.e. 

<math display="inline" id="Bayesian_inference:48">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐗</mi>
    <mo>∣</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">∫</mo>
    <mi>θ</mi>
   </msub>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐗</mi>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo rspace="0.8pt">d</mo>
   <mi>θ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <int></int>
     <ci>θ</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-d</ci>
    <csymbol cd="unknown">θ</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\mathbf{X}\mid\alpha)=\int_{\theta}p(\mathbf{X}\mid\theta)p(\theta\mid\alpha%
)\operatorname{d}\!\theta
  </annotation>
 </semantics>
</math>


 .</li>
<li>The <a href="posterior_distribution" title="wikilink">posterior distribution</a> is the distribution of the parameter(s) after taking into account the observed data. This is determined by <a href="Bayes'_rule" title="wikilink">Bayes' rule</a>, which forms the heart of Bayesian inference:</li>
</ul>

<p>

<math display="block" id="Bayesian_inference:49">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>𝐗</mi>
    <mo>,</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>𝐗</mi>
      <mo>∣</mo>
      <mi>θ</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>θ</mi>
      <mo>∣</mo>
      <mi>α</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>𝐗</mi>
      <mo>∣</mo>
      <mi>α</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
   <mo>∝</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐗</mi>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">X</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">θ</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">θ</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">α</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">X</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">α</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
    </apply>
    <csymbol cd="latexml">proportional-to</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\theta\mid\mathbf{X},\alpha)=\frac{p(\mathbf{X}\mid\theta)p(\theta\mid\alpha%
)}{p(\mathbf{X}\mid\alpha)}\propto p(\mathbf{X}\mid\theta)p(\theta\mid\alpha)
  </annotation>
 </semantics>
</math>

</p>

<p>Note that this is expressed in words as "posterior is proportional to likelihood times prior", or sometimes as "posterior = likelihood times prior, over evidence".</p>
<h3 id="bayesian-prediction">Bayesian prediction</h3>
<ul>
<li>The <a href="posterior_predictive_distribution" title="wikilink">posterior predictive distribution</a> is the distribution of a new data point, marginalized over the posterior:</li>
</ul>

<p>

<math display="block" id="Bayesian_inference:50">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">~</mo>
    </mover>
    <mo>∣</mo>
    <mi>𝐗</mi>
    <mo>,</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">∫</mo>
    <mi>θ</mi>
   </msub>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">~</mo>
    </mover>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>𝐗</mi>
    <mo>,</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo rspace="0.8pt">d</mo>
   <mi>θ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-~</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <int></int>
     <ci>θ</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-~</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-d</ci>
    <csymbol cd="unknown">θ</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\tilde{x}\mid\mathbf{X},\alpha)=\int_{\theta}p(\tilde{x}\mid\theta)p(\theta%
\mid\mathbf{X},\alpha)\operatorname{d}\!\theta
  </annotation>
 </semantics>
</math>

</p>
<ul>
<li>The <a href="prior_predictive_distribution" title="wikilink">prior predictive distribution</a> is the distribution of a new data point, marginalized over the prior:</li>
</ul>

<p>

<math display="block" id="Bayesian_inference:51">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">~</mo>
    </mover>
    <mo>∣</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">∫</mo>
    <mi>θ</mi>
   </msub>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">~</mo>
    </mover>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo rspace="0.8pt">d</mo>
   <mi>θ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-~</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <int></int>
     <ci>θ</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-~</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-d</ci>
    <csymbol cd="unknown">θ</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\tilde{x}\mid\alpha)=\int_{\theta}p(\tilde{x}\mid\theta)p(\theta\mid\alpha)%
\operatorname{d}\!\theta
  </annotation>
 </semantics>
</math>

</p>

<p>Bayesian theory calls for the use of the posterior predictive distribution to do <a href="predictive_inference" title="wikilink">predictive inference</a>, i.e., to <a href="prediction" title="wikilink">predict</a> the distribution of a new, unobserved data point. That is, instead of a fixed point as a prediction, a distribution over possible points is returned. Only this way is the entire posterior distribution of the parameter(s) used. By comparison, prediction in <a href="frequentist_statistics" title="wikilink">frequentist statistics</a> often involves finding an optimum point estimate of the parameter(s)—e.g., by <a href="maximum_likelihood" title="wikilink">maximum likelihood</a> or <a href="maximum_a_posteriori_estimation" title="wikilink">maximum a posteriori estimation</a> (MAP)—and then plugging this estimate into the formula for the distribution of a data point. This has the disadvantage that it does not account for any uncertainty in the value of the parameter, and hence will underestimate the <a class="uri" href="variance" title="wikilink">variance</a> of the predictive distribution.</p>

<p>(In some instances, frequentist statistics can work around this problem. For example, <a href="confidence_interval" title="wikilink">confidence intervals</a> and <a href="prediction_interval" title="wikilink">prediction intervals</a> in frequentist statistics when constructed from a <a href="normal_distribution" title="wikilink">normal distribution</a> with unknown <a class="uri" href="mean" title="wikilink">mean</a> and <a class="uri" href="variance" title="wikilink">variance</a> are constructed using a <a href="Student's_t-distribution" title="wikilink">Student's t-distribution</a>. This correctly estimates the variance, due to the fact that (1) the average of normally distributed random variables is also normally distributed; (2) the predictive distribution of a normally distributed data point with unknown mean and variance, using conjugate or uninformative priors, has a student's t-distribution. In Bayesian statistics, however, the posterior predictive distribution can always be determined exactly—or at least, to an arbitrary level of precision, when numerical methods are used.)</p>

<p>Note that both types of predictive distributions have the form of a <a href="compound_probability_distribution" title="wikilink">compound probability distribution</a> (as does the <a href="marginal_likelihood" title="wikilink">marginal likelihood</a>). In fact, if the prior distribution is a <a href="conjugate_prior" title="wikilink">conjugate prior</a>, and hence the prior and posterior distributions come from the same family, it can easily be seen that both prior and posterior predictive distributions also come from the same family of compound distributions. The only difference is that the posterior predictive distribution uses the updated values of the hyperparameters (applying the Bayesian update rules given in the <a href="conjugate_prior" title="wikilink">conjugate prior</a> article), while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution.</p>
<h2 id="inference-over-exclusive-and-exhaustive-possibilities">Inference over exclusive and exhaustive possibilities</h2>

<p>If evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions, Bayesian inference may be thought of as acting on this belief distribution as a whole.</p>
<h3 id="general-formulation">General formulation</h3>
<figure><b>(Figure)</b>
<figcaption>Diagram illustrating event space 

<math display="inline" id="Bayesian_inference:52">
 <semantics>
  <mi mathvariant="normal">Ω</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Ω</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Omega
  </annotation>
 </semantics>
</math>

 in general formulation of Bayesian inference. Although this diagram shows discrete models and events, the continuous case may be visualized similarly using probability densities.</figcaption>
</figure>

<p>Suppose a process is generating independent and identically distributed events 

<math display="inline" id="Bayesian_inference:53">
 <semantics>
  <msub>
   <mi>E</mi>
   <mi>n</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>E</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   E_{n}
  </annotation>
 </semantics>
</math>


, but the probability distribution is unknown. Let the event space 

<math display="inline" id="Bayesian_inference:54">
 <semantics>
  <mi mathvariant="normal">Ω</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Ω</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Omega
  </annotation>
 </semantics>
</math>

 represent the current state of belief for this process. Each model is represented by event 

<math display="inline" id="Bayesian_inference:55">
 <semantics>
  <msub>
   <mi>M</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>M</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M_{m}
  </annotation>
 </semantics>
</math>

. The conditional probabilities 

<math display="inline" id="Bayesian_inference:56">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>E</mi>
     <mi>n</mi>
    </msub>
    <mo>∣</mo>
    <msub>
     <mi>M</mi>
     <mi>m</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>E</ci>
      <ci>n</ci>
     </apply>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>M</ci>
      <ci>m</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(E_{n}\mid M_{m})
  </annotation>
 </semantics>
</math>

 are specified to define the models. 

<math display="inline" id="Bayesian_inference:57">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>M</mi>
     <mi>m</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>M</ci>
     <ci>m</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(M_{m})
  </annotation>
 </semantics>
</math>

 is the degree of belief in 

<math display="inline" id="Bayesian_inference:58">
 <semantics>
  <msub>
   <mi>M</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>M</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M_{m}
  </annotation>
 </semantics>
</math>


. Before the first inference step, 

<math display="inline" id="Bayesian_inference:59">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>M</mi>
      <mi>m</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <apply>
     <times></times>
     <ci>P</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>M</ci>
      <ci>m</ci>
     </apply>
    </apply>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{P(M_{m})\}
  </annotation>
 </semantics>
</math>

 is a set of <em>initial prior probabilities</em>. These must sum to 1, but are otherwise arbitrary.</p>

<p>Suppose that the process is observed to generate 

<math display="inline" id="Bayesian_inference:60">
 <semantics>
  <mrow>
   <mi>E</mi>
   <mo>∈</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mi>E</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>E</ci>
    <set>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>E</ci>
      <ci>n</ci>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E\in\{E_{n}\}
  </annotation>
 </semantics>
</math>

. For each 

<math display="inline" id="Bayesian_inference:61">
 <semantics>
  <mrow>
   <mi>M</mi>
   <mo>∈</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mi>M</mi>
     <mi>m</mi>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>M</ci>
    <set>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>M</ci>
      <ci>m</ci>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M\in\{M_{m}\}
  </annotation>
 </semantics>
</math>

, the prior 

<math display="inline" id="Bayesian_inference:62">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>M</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <ci>M</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(M)
  </annotation>
 </semantics>
</math>

 is updated to the posterior 

<math display="inline" id="Bayesian_inference:63">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>M</mi>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(M\mid E)
  </annotation>
 </semantics>
</math>


. From <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>:<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>

<math display="block" id="Bayesian_inference:64">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>M</mi>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo>∣</mo>
      <mi>M</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <msub>
      <mo largeop="true" symmetric="true">∑</mo>
      <mi>m</mi>
     </msub>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo>∣</mo>
      <msub>
       <mi>M</mi>
       <mi>m</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>M</mi>
       <mi>m</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
   <mo>⋅</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>M</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">M</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>m</ci>
      </apply>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <ci>normal-∣</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>M</ci>
        <ci>m</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>M</ci>
        <ci>m</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
    </apply>
    <ci>normal-⋅</ci>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(M\mid E)=\frac{P(E\mid M)}{\sum_{m}{P(E\mid M_{m})P(M_{m})}}\cdot P(M)
  </annotation>
 </semantics>
</math>

</p>

<p>Upon observation of further evidence, this procedure may be repeated.</p>
<h3 id="multiple-observations">Multiple observations</h3>

<p>For a set of <a href="independent_and_identically_distributed" title="wikilink">independent and identically distributed</a> observations 

<math display="inline" id="Bayesian_inference:65">
 <semantics>
  <mrow>
   <mi>𝐄</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mi>e</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>e</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>𝐄</ci>
    <set>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>e</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>e</ci>
      <ci>n</ci>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{E}=\{e_{1},\dots,e_{n}\}
  </annotation>
 </semantics>
</math>

, it may be shown that repeated application of the above is equivalent to</p>

<p>

<math display="block" id="Bayesian_inference:66">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>M</mi>
    <mo>∣</mo>
    <mi>𝐄</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>𝐄</mi>
      <mo>∣</mo>
      <mi>M</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <msub>
      <mo largeop="true" symmetric="true">∑</mo>
      <mi>m</mi>
     </msub>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>𝐄</mi>
      <mo>∣</mo>
      <msub>
       <mi>M</mi>
       <mi>m</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>M</mi>
       <mi>m</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
   <mo>⋅</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>M</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">M</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>m</ci>
      </apply>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <ci>normal-∣</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>M</ci>
        <ci>m</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>M</ci>
        <ci>m</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
    </apply>
    <ci>normal-⋅</ci>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(M\mid\mathbf{E})=\frac{P(\mathbf{E}\mid M)}{\sum_{m}{P(\mathbf{E}\mid M_{m})%
P(M_{m})}}\cdot P(M)
  </annotation>
 </semantics>
</math>

</p>

<p>Where</p>

<p>

<math display="block" id="Bayesian_inference:67">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐄</mi>
    <mo>∣</mo>
    <mi>M</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <munder>
    <mo largeop="true" movablelimits="false" symmetric="true">∏</mo>
    <mi>k</mi>
   </munder>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>e</mi>
     <mi>k</mi>
    </msub>
    <mo>∣</mo>
    <mi>M</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <csymbol cd="latexml">product</csymbol>
     <ci>k</ci>
    </apply>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>e</ci>
      <ci>k</ci>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-.</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(\mathbf{E}\mid M)=\prod_{k}{P(e_{k}\mid M)}.
  </annotation>
 </semantics>
</math>

</p>

<p>This may be used to optimize practical calculations.</p>
<h3 id="parametric-formulation">Parametric formulation</h3>

<p>By parameterizing the space of models, the belief in all models may be updated in a single step. The distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space. The distributions in this section are expressed as continuous, represented by probability densities, as this is the usual situation. The technique is however equally applicable to discrete distributions.</p>

<p>Let the vector 

<math display="inline" id="Bayesian_inference:68">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{\theta}
  </annotation>
 </semantics>
</math>


 span the parameter space. Let the initial prior distribution over 

<math display="inline" id="Bayesian_inference:69">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{\theta}
  </annotation>
 </semantics>
</math>

 be 

<math display="inline" id="Bayesian_inference:70">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\mathbf{\theta}\mid\mathbf{\alpha})
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Bayesian_inference:71">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{\alpha}
  </annotation>
 </semantics>
</math>

 is a set of parameters to the prior itself, or <em><a href="hyperparameter" title="wikilink">hyperparameters</a></em>. Let 

<math display="inline" id="Bayesian_inference:72">
 <semantics>
  <mrow>
   <mi>𝐄</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mi>e</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>e</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>𝐄</ci>
    <set>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>e</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>e</ci>
      <ci>n</ci>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{E}=\{e_{1},\dots,e_{n}\}
  </annotation>
 </semantics>
</math>

 be a set of <a href="Independent_and_identically_distributed_random_variables" title="wikilink">independent and identically distributed</a> event observations, where all 

<math display="inline" id="Bayesian_inference:73">
 <semantics>
  <msub>
   <mi>e</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>e</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   e_{i}
  </annotation>
 </semantics>
</math>


 are distributed as 

<math display="inline" id="Bayesian_inference:74">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>e</mi>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(e\mid\mathbf{\theta})
  </annotation>
 </semantics>
</math>

 for some 

<math display="inline" id="Bayesian_inference:75">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{\theta}
  </annotation>
 </semantics>
</math>

. <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a> is applied to find the <a href="posterior_distribution" title="wikilink">posterior distribution</a> over 

<math display="inline" id="Bayesian_inference:76">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{\theta}
  </annotation>
 </semantics>
</math>

:</p>

<p>

<math display="inline" id="Bayesian_inference:77">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>𝐄</mi>
    <mo>,</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle p(\mathbf{\theta}\mid\mathbf{E},\mathbf{\alpha})
  </annotation>
 </semantics>
</math>


</p>

<p>Where</p>

<p>

<math display="block" id="Bayesian_inference:78">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐄</mi>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo>,</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <munder>
    <mo largeop="true" movablelimits="false" symmetric="true">∏</mo>
    <mi>k</mi>
   </munder>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>e</mi>
     <mi>k</mi>
    </msub>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <csymbol cd="latexml">product</csymbol>
     <ci>k</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>e</ci>
      <ci>k</ci>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\mathbf{E}\mid\mathbf{\theta},\mathbf{\alpha})=\prod_{k}p(e_{k}\mid\mathbf{%
\theta})
  </annotation>
 </semantics>
</math>

</p>
<h2 id="mathematical-properties">Mathematical properties</h2>
<h3 id="interpretation-of-factor">Interpretation of factor</h3>

<p>

<math display="inline" id="Bayesian_inference:79">
 <semantics>
  <mrow>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo>∣</mo>
      <mi>M</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
   <mo>></mo>
   <mn>1</mn>
   <mo>⇒</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo>∣</mo>
    <mi>M</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>></mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">M</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <apply>
      <times></times>
      <ci>P</ci>
      <ci>E</ci>
     </apply>
    </apply>
    <gt></gt>
    <cn type="integer">1</cn>
    <ci>normal-⇒</ci>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <gt></gt>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\frac{P(E\mid M)}{P(E)}>1\Rightarrow\textstyle P(E\mid M)>P(E)
  </annotation>
 </semantics>
</math>

. That is, if the model were true, the evidence would be more likely than is predicted by the current state of belief. The reverse applies for a decrease in belief. If the belief does not change, 

<math display="inline" id="Bayesian_inference:80">
 <semantics>
  <mrow>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo>∣</mo>
      <mi>M</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
   <mo>=</mo>
   <mn>1</mn>
   <mo>⇒</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo>∣</mo>
    <mi>M</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">M</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <apply>
      <times></times>
      <ci>P</ci>
      <ci>E</ci>
     </apply>
    </apply>
    <eq></eq>
    <cn type="integer">1</cn>
    <ci>normal-⇒</ci>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\frac{P(E\mid M)}{P(E)}=1\Rightarrow\textstyle P(E\mid M)=P(E)
  </annotation>
 </semantics>
</math>

. That is, the evidence is independent of the model. If the model were true, the evidence would be exactly as likely as predicted by the current state of belief.</p>
<h3 id="cromwells-rule">Cromwell's rule</h3>

<p>If 

<math display="inline" id="Bayesian_inference:81">
 <semantics>
  <mrow>
   <mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>M</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>P</ci>
     <ci>M</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(M)=0
  </annotation>
 </semantics>
</math>

 then 

<math display="inline" id="Bayesian_inference:82">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>M</mi>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">0</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(M\mid E)=0
  </annotation>
 </semantics>
</math>

. If 

<math display="inline" id="Bayesian_inference:83">
 <semantics>
  <mrow>
   <mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>M</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>P</ci>
     <ci>M</ci>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(M)=1
  </annotation>
 </semantics>
</math>

, then 

<math display="inline" id="Bayesian_inference:84">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>M</mi>
    <mo stretchy="false">|</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">1</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(M|E)=1
  </annotation>
 </semantics>
</math>

. This can be interpreted to mean that hard convictions are insensitive to counter-evidence.</p>

<p>The former follows directly from Bayes' theorem. The latter can be derived by applying the first rule to the event "not 

<math display="inline" id="Bayesian_inference:85">
 <semantics>
  <mi>M</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>M</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M
  </annotation>
 </semantics>
</math>

" in place of "

<math display="inline" id="Bayesian_inference:86">
 <semantics>
  <mi>M</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>M</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M
  </annotation>
 </semantics>
</math>

", yielding "if 

<math display="inline" id="Bayesian_inference:87">
 <semantics>
  <mrow>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>M</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <times></times>
      <ci>P</ci>
      <ci>M</ci>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1-P(M)=0
  </annotation>
 </semantics>
</math>

, then 

<math display="inline" id="Bayesian_inference:88">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>-</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>M</mi>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <cn type="integer">1</cn>
    <minus></minus>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">M</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">0</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1-P(M\mid E)=0
  </annotation>
 </semantics>
</math>

", from which the result immediately follows.</p>
<h3 id="asymptotic-behaviour-of-posterior">Asymptotic behaviour of posterior</h3>

<p>Consider the behaviour of a belief distribution as it is updated a large number of times with <a href="independent_and_identically_distributed" title="wikilink">independent and identically distributed</a> trials. For sufficiently nice prior probabilities, the <a href="Bernstein–von_Mises_theorem" title="wikilink">Bernstein-von Mises theorem</a> gives that in the limit of infinite trials, the posterior converges to a <a href="Gaussian_distribution" title="wikilink">Gaussian distribution</a> independent of the initial prior under some conditions firstly outlined and rigorously proven by <a href="Joseph_L._Doob" title="wikilink">Joseph L. Doob</a> in 1948, namely if the random variable in consideration has a finite <a href="probability_space" title="wikilink">probability space</a>. The more general results were obtained later by the statistician <a href="David_A._Freedman_(statistician)" title="wikilink">David A. Freedman</a> who published in two seminal research papers in 1963 and 1965 when and under what circumstances the asymptotic behaviour of posterior is guaranteed. His 1963 paper treats, like Doob (1949), the finite case and comes to a satisfactory conclusion. However, if the random variable has an infinite but countable <a href="probability_space" title="wikilink">probability space</a> (i.e., corresponding to a die with infinite many faces) the 1965 paper demonstrates that for a dense subset of priors the <a href="Bernstein–von_Mises_theorem" title="wikilink">Bernstein-von Mises theorem</a> is not applicable. In this case there is <a href="almost_surely" title="wikilink">almost surely</a> no asymptotic convergence. Later in the 1980s and 1990s <a href="David_A._Freedman_(statistician)" title="wikilink">Freedman</a> and <a href="Persi_Diaconis" title="wikilink">Persi Diaconis</a> continued to work on the case of infinite countable probability spaces.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> To summarise, there may be insufficient trials to suppress the effects of the initial choice, and especially for large (but finite) systems the convergence might be very slow.</p>
<h3 id="conjugate-priors">Conjugate priors</h3>

<p>In parameterized form, the prior distribution is often assumed to come from a family of distributions called <a href="conjugate_prior" title="wikilink">conjugate priors</a>. The usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family, and the calculation may be expressed in <a href="Closed-form_expression" title="wikilink">closed form</a>.</p>
<h3 id="estimates-of-parameters-and-predictions">Estimates of parameters and predictions</h3>

<p>It is often desired to use a posterior distribution to estimate a parameter or variable. Several methods of Bayesian estimation select <a href="central_tendency" title="wikilink">measurements of central tendency</a> from the posterior distribution.</p>

<p>For one-dimensional problems, a unique median exists for practical continuous problems. The posterior median is attractive as a <a href="robust_statistics" title="wikilink">robust estimator</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>If there exists a finite mean for the posterior distribution, then the posterior mean is a method of estimation.</p>

<p>

<math display="block" id="Bayesian_inference:89">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>θ</mi>
    <mo stretchy="false">~</mo>
   </mover>
   <mo>=</mo>
   <mo>E</mo>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>θ</mi>
    <mo stretchy="false">]</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">∫</mo>
    <mi>θ</mi>
   </msub>
   <mpadded width="+1.7pt">
    <mi>θ</mi>
   </mpadded>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>𝐗</mi>
    <mo>,</mo>
    <mi>α</mi>
    <mo rspace="4.2pt" stretchy="false">)</mo>
   </mrow>
   <mi>d</mi>
   <mi>θ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <ci>normal-~</ci>
     <ci>θ</ci>
    </apply>
    <eq></eq>
    <ci>normal-E</ci>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-[</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-]</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <int></int>
     <ci>θ</ci>
    </apply>
    <csymbol cd="unknown">θ</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">d</csymbol>
    <csymbol cd="unknown">θ</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tilde{\theta}=\operatorname{E}[\theta]=\int_{\theta}\theta\,p(\theta\mid%
\mathbf{X},\alpha)\,d\theta
  </annotation>
 </semantics>
</math>

</p>

<p>Taking a value with the greatest probability defines <a href="maximum_a_posteriori_estimation" title="wikilink">maximum <em>a posteriori</em> (MAP)</a> estimates:</p>

<p>

<math display="block" id="Bayesian_inference:90">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mi>θ</mi>
     <mtext>MAP</mtext>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
   <mo>⊂</mo>
   <mi>arg</mi>
   <munder>
    <mi>max</mi>
    <mi>θ</mi>
   </munder>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>𝐗</mi>
    <mo>,</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-{</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>θ</ci>
      <mtext>MAP</mtext>
     </apply>
     <ci>normal-}</ci>
    </cerror>
    <subset></subset>
    <arg></arg>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <max></max>
     <ci>θ</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-.</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{\theta_{\text{MAP}}\}\subset\arg\max_{\theta}p(\theta\mid\mathbf{X},\alpha).
  </annotation>
 </semantics>
</math>

</p>

<p>There are examples where no maximum is attained, in which case the set of MAP estimates is <a href="empty_set" title="wikilink">empty</a>.</p>

<p>There are other methods of estimation that minimize the posterior <em><a class="uri" href="risk" title="wikilink">risk</a></em> (expected-posterior loss) with respect to a <a href="loss_function" title="wikilink">loss function</a>, and these are of interest to <a href="statistical_decision_theory" title="wikilink">statistical decision theory</a> using the sampling distribution ("frequentist statistics").</p>

<p>The <a href="posterior_predictive_distribution" title="wikilink">posterior predictive distribution</a> of a new observation 

<math display="inline" id="Bayesian_inference:91">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">~</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-~</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tilde{x}
  </annotation>
 </semantics>
</math>

 (that is independent of previous observations) is determined by</p>

<p>

<math display="block" id="Bayesian_inference:92">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">~</mo>
    </mover>
    <mo stretchy="false">|</mo>
    <mi>𝐗</mi>
    <mo>,</mo>
    <mi>α</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">∫</mo>
    <mi>θ</mi>
   </msub>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">~</mo>
    </mover>
    <mo>,</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>𝐗</mi>
    <mo>,</mo>
    <mi>α</mi>
    <mo rspace="4.2pt" stretchy="false">)</mo>
   </mrow>
   <mi>d</mi>
   <mi>θ</mi>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">∫</mo>
    <mi>θ</mi>
   </msub>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">~</mo>
    </mover>
    <mo>∣</mo>
    <mi>θ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>θ</mi>
    <mo>∣</mo>
    <mi>𝐗</mi>
    <mo>,</mo>
    <mi>α</mi>
    <mo rspace="4.2pt" stretchy="false">)</mo>
   </mrow>
   <mi>d</mi>
   <mi>θ</mi>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-~</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-|</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <int></int>
     <ci>θ</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-~</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-,</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">d</csymbol>
    <csymbol cd="unknown">θ</csymbol>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <int></int>
     <ci>θ</ci>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-~</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">α</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">d</csymbol>
    <csymbol cd="unknown">θ</csymbol>
    <ci>normal-.</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\tilde{x}|\mathbf{X},\alpha)=\int_{\theta}p(\tilde{x},\theta\mid\mathbf{X},%
\alpha)\,d\theta=\int_{\theta}p(\tilde{x}\mid\theta)p(\theta\mid\mathbf{X},%
\alpha)\,d\theta.
  </annotation>
 </semantics>
</math>

</p>
<h2 id="examples">Examples</h2>
<h3 id="probability-of-a-hypothesis">Probability of a hypothesis</h3>

<p>Suppose there are two full bowls of cookies. Bowl #1 has 10 chocolate chip and 30 plain cookies, while bowl #2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?</p>

<p>Intuitively, it seems clear that the answer should be more than a half, since there are more plain cookies in bowl #1. The precise answer is given by Bayes' theorem. Let 

<math display="inline" id="Bayesian_inference:93">
 <semantics>
  <msub>
   <mi>H</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>H</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H_{1}
  </annotation>
 </semantics>
</math>

 correspond to bowl #1, and 

<math display="inline" id="Bayesian_inference:94">
 <semantics>
  <msub>
   <mi>H</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>H</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H_{2}
  </annotation>
 </semantics>
</math>

 to bowl #2. It is given that the bowls are identical from Fred's point of view, thus 

<math display="inline" id="Bayesian_inference:95">
 <semantics>
  <mrow>
   <mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>H</mi>
      <mn>1</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>H</mi>
      <mn>2</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>P</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>H</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>P</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>H</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(H_{1})=P(H_{2})
  </annotation>
 </semantics>
</math>

, and the two must add up to 1, so both are equal to 0.5. The event 

<math display="inline" id="Bayesian_inference:96">
 <semantics>
  <mi>E</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>E</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   E
  </annotation>
 </semantics>
</math>

 is the observation of a plain cookie. From the contents of the bowls, we know that 

<math display="inline" id="Bayesian_inference:97">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo>∣</mo>
    <msub>
     <mi>H</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>30</mn>
   <mo>/</mo>
   <mn>40</mn>
   <mo>=</mo>
   <mn>0.75</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>H</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">30</cn>
    <divide></divide>
    <cn type="integer">40</cn>
    <eq></eq>
    <cn type="float">0.75</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(E\mid H_{1})=30/40=0.75
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Bayesian_inference:98">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo>∣</mo>
    <msub>
     <mi>H</mi>
     <mn>2</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>20</mn>
   <mo>/</mo>
   <mn>40</mn>
   <mo>=</mo>
   <mn>0.5.</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>H</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">20</cn>
    <divide></divide>
    <cn type="integer">40</cn>
    <eq></eq>
    <cn type="float">0.5.</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(E\mid H_{2})=20/40=0.5.
  </annotation>
 </semantics>
</math>

 Bayes' formula then yields</p>

<p>

<math display="inline" id="Bayesian_inference:99">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>H</mi>
     <mn>1</mn>
    </msub>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>H</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle P(H_{1}\mid E)
  </annotation>
 </semantics>
</math>


</p>

<p>Before we observed the cookie, the probability we assigned for Fred having chosen bowl #1 was the prior probability, 

<math display="inline" id="Bayesian_inference:100">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>H</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>H</ci>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(H_{1})
  </annotation>
 </semantics>
</math>

, which was 0.5. After observing the cookie, we must revise the probability to 

<math display="inline" id="Bayesian_inference:101">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>H</mi>
     <mn>1</mn>
    </msub>
    <mo>∣</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>H</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(H_{1}\mid E)
  </annotation>
 </semantics>
</math>

, which is 0.6.</p>
<h3 id="making-a-prediction">Making a prediction</h3>
<figure><b>(Figure)</b>
<figcaption>Example results for archaeology example. This simulation was generated using c=15.2.</figcaption>
</figure>

<p>An archaeologist is working at a site thought to be from the medieval period, between the 11th century to the 16th century. However, it is uncertain exactly when in this period the site was inhabited. Fragments of pottery are found, some of which are glazed and some of which are decorated. It is expected that if the site were inhabited during the early medieval period, then 1% of the pottery would be glazed and 50% of its area decorated, whereas if it had been inhabited in the late medieval period then 81% would be glazed and 5% of its area decorated. How confident can the archaeologist be in the date of inhabitation as fragments are unearthed?</p>

<p>The degree of belief in the continuous variable 

<math display="inline" id="Bayesian_inference:102">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   C
  </annotation>
 </semantics>
</math>

 (century) is to be calculated, with the discrete set of events 

<math display="inline" id="Bayesian_inference:103">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <mrow>
    <mi>G</mi>
    <mi>D</mi>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mi>G</mi>
    <mover accent="true">
     <mi>D</mi>
     <mo stretchy="false">¯</mo>
    </mover>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mover accent="true">
     <mi>G</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mi>D</mi>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mover accent="true">
     <mi>G</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mover accent="true">
     <mi>D</mi>
     <mo stretchy="false">¯</mo>
    </mover>
   </mrow>
   <mo stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <apply>
     <times></times>
     <ci>G</ci>
     <ci>D</ci>
    </apply>
    <apply>
     <times></times>
     <ci>G</ci>
     <apply>
      <ci>normal-¯</ci>
      <ci>D</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <ci>normal-¯</ci>
      <ci>G</ci>
     </apply>
     <ci>D</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <ci>normal-¯</ci>
      <ci>G</ci>
     </apply>
     <apply>
      <ci>normal-¯</ci>
      <ci>D</ci>
     </apply>
    </apply>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{GD,G\bar{D},\bar{G}D,\bar{G}\bar{D}\}
  </annotation>
 </semantics>
</math>

 as evidence. Assuming linear variation of glaze and decoration with time, and that these variables are independent,</p>

<p>

<math display="block" id="Bayesian_inference:104">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo>=</mo>
    <mi>G</mi>
    <mi>D</mi>
    <mo>∣</mo>
    <mi>C</mi>
    <mo>=</mo>
    <mi>c</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>0.01</mn>
    <mo>+</mo>
    <mn>0.16</mn>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>c</mi>
     <mo>-</mo>
     <mn>11</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>0.5</mn>
    <mo>-</mo>
    <mn>0.09</mn>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>c</mi>
     <mo>-</mo>
     <mn>11</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <eq></eq>
     <csymbol cd="unknown">G</csymbol>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">C</csymbol>
     <eq></eq>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="float">0.01</cn>
     <plus></plus>
     <cn type="float">0.16</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">c</csymbol>
      <minus></minus>
      <cn type="integer">11</cn>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="float">0.5</cn>
     <minus></minus>
     <cn type="float">0.09</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">c</csymbol>
      <minus></minus>
      <cn type="integer">11</cn>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(E=GD\mid C=c)=(0.01+0.16(c-11))(0.5-0.09(c-11))
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Bayesian_inference:105">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo>=</mo>
    <mi>G</mi>
    <mover accent="true">
     <mi>D</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mo>∣</mo>
    <mi>C</mi>
    <mo>=</mo>
    <mi>c</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>0.01</mn>
    <mo>+</mo>
    <mn>0.16</mn>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>c</mi>
     <mo>-</mo>
     <mn>11</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>0.5</mn>
    <mo>+</mo>
    <mn>0.09</mn>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>c</mi>
     <mo>-</mo>
     <mn>11</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <eq></eq>
     <csymbol cd="unknown">G</csymbol>
     <apply>
      <ci>normal-¯</ci>
      <ci>D</ci>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">C</csymbol>
     <eq></eq>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="float">0.01</cn>
     <plus></plus>
     <cn type="float">0.16</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">c</csymbol>
      <minus></minus>
      <cn type="integer">11</cn>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="float">0.5</cn>
     <plus></plus>
     <cn type="float">0.09</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">c</csymbol>
      <minus></minus>
      <cn type="integer">11</cn>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(E=G\bar{D}\mid C=c)=(0.01+0.16(c-11))(0.5+0.09(c-11))
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Bayesian_inference:106">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo>=</mo>
    <mover accent="true">
     <mi>G</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mi>D</mi>
    <mo>∣</mo>
    <mi>C</mi>
    <mo>=</mo>
    <mi>c</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>0.99</mn>
    <mo>-</mo>
    <mn>0.16</mn>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>c</mi>
     <mo>-</mo>
     <mn>11</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>0.5</mn>
    <mo>-</mo>
    <mn>0.09</mn>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>c</mi>
     <mo>-</mo>
     <mn>11</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <eq></eq>
     <apply>
      <ci>normal-¯</ci>
      <ci>G</ci>
     </apply>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">C</csymbol>
     <eq></eq>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="float">0.99</cn>
     <minus></minus>
     <cn type="float">0.16</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">c</csymbol>
      <minus></minus>
      <cn type="integer">11</cn>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="float">0.5</cn>
     <minus></minus>
     <cn type="float">0.09</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">c</csymbol>
      <minus></minus>
      <cn type="integer">11</cn>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(E=\bar{G}D\mid C=c)=(0.99-0.16(c-11))(0.5-0.09(c-11))
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Bayesian_inference:107">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>E</mi>
    <mo>=</mo>
    <mover accent="true">
     <mi>G</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mover accent="true">
     <mi>D</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mo>∣</mo>
    <mi>C</mi>
    <mo>=</mo>
    <mi>c</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>0.99</mn>
    <mo>-</mo>
    <mn>0.16</mn>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>c</mi>
     <mo>-</mo>
     <mn>11</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>0.5</mn>
    <mo>+</mo>
    <mn>0.09</mn>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>c</mi>
     <mo>-</mo>
     <mn>11</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">E</csymbol>
     <eq></eq>
     <apply>
      <ci>normal-¯</ci>
      <ci>G</ci>
     </apply>
     <apply>
      <ci>normal-¯</ci>
      <ci>D</ci>
     </apply>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">C</csymbol>
     <eq></eq>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="float">0.99</cn>
     <minus></minus>
     <cn type="float">0.16</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">c</csymbol>
      <minus></minus>
      <cn type="integer">11</cn>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="float">0.5</cn>
     <plus></plus>
     <cn type="float">0.09</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">c</csymbol>
      <minus></minus>
      <cn type="integer">11</cn>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(E=\bar{G}\bar{D}\mid C=c)=(0.99-0.16(c-11))(0.5+0.09(c-11))
  </annotation>
 </semantics>
</math>

</p>

<p>Assume a uniform prior of 

<math display="inline" id="Bayesian_inference:108">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>f</mi>
     <mi>C</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>c</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0.2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>f</ci>
      <ci>C</ci>
     </apply>
     <ci>c</ci>
    </apply>
    <cn type="float">0.2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle f_{C}(c)=0.2
  </annotation>
 </semantics>
</math>

, and that trials are <a href="independent_and_identically_distributed" title="wikilink">independent and identically distributed</a>. When a new fragment of type 

<math display="inline" id="Bayesian_inference:109">
 <semantics>
  <mi>e</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>e</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   e
  </annotation>
 </semantics>
</math>

 is discovered, Bayes' theorem is applied to update the degree of belief for each 

<math display="inline" id="Bayesian_inference:110">
 <semantics>
  <mi>c</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>c</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c
  </annotation>
 </semantics>
</math>

:</p>

<p>

<math display="inline" id="Bayesian_inference:111">
 <semantics>
  <mrow>
   <msub>
    <mi>f</mi>
    <mi>C</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>c</mi>
    <mo>∣</mo>
    <mi>E</mi>
    <mo>=</mo>
    <mi>e</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo>=</mo>
      <mi>e</mi>
      <mo>∣</mo>
      <mi>C</mi>
      <mo>=</mo>
      <mi>c</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo>=</mo>
      <mi>e</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
   <msub>
    <mi>f</mi>
    <mi>C</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>c</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo>=</mo>
      <mi>e</mi>
      <mo>∣</mo>
      <mi>C</mi>
      <mo>=</mo>
      <mi>c</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mstyle displaystyle="false">
      <msubsup>
       <mo largeop="true" symmetric="true">∫</mo>
       <mn>11</mn>
       <mn>16</mn>
      </msubsup>
     </mstyle>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>E</mi>
      <mo>=</mo>
      <mi>e</mi>
      <mo>∣</mo>
      <mi>C</mi>
      <mo>=</mo>
      <mi>c</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <msub>
      <mi>f</mi>
      <mi>C</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>c</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>d</mi>
     <mi>c</mi>
    </mrow>
   </mfrac>
   <msub>
    <mi>f</mi>
    <mi>C</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>c</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>C</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">E</csymbol>
     <eq></eq>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <eq></eq>
       <csymbol cd="unknown">e</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">C</csymbol>
       <eq></eq>
       <csymbol cd="unknown">c</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <eq></eq>
       <csymbol cd="unknown">e</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>C</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <eq></eq>
       <csymbol cd="unknown">e</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">C</csymbol>
       <eq></eq>
       <csymbol cd="unknown">c</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <int></int>
        <cn type="integer">11</cn>
       </apply>
       <cn type="integer">16</cn>
      </apply>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">E</csymbol>
       <eq></eq>
       <csymbol cd="unknown">e</csymbol>
       <ci>normal-∣</ci>
       <csymbol cd="unknown">C</csymbol>
       <eq></eq>
       <csymbol cd="unknown">c</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>f</ci>
       <ci>C</ci>
      </apply>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">c</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <csymbol cd="unknown">d</csymbol>
      <csymbol cd="unknown">c</csymbol>
     </cerror>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>C</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">c</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{C}(c\mid E=e)=\frac{P(E=e\mid C=c)}{P(E=e)}f_{C}(c)=\frac{P(E=e\mid C=c)}{%
\int_{11}^{16}{P(E=e\mid C=c)f_{C}(c)dc}}f_{C}(c)
  </annotation>
 </semantics>
</math>

</p>

<p>A computer simulation of the changing belief as 50 fragments are unearthed is shown on the graph. In the simulation, the site was inhabited around 1420, or 

<math display="inline" id="Bayesian_inference:112">
 <semantics>
  <mrow>
   <mi>c</mi>
   <mo>=</mo>
   <mn>15.2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>c</ci>
    <cn type="float">15.2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c=15.2
  </annotation>
 </semantics>
</math>

. By calculating the area under the relevant portion of the graph for 50 trials, the archaeologist can say that there is practically no chance the site was inhabited in the 11th and 12th centuries, about 1% chance that it was inhabited during the 13th century, 63% chance during the 14th century and 36% during the 15th century. Note that the <a href="Bernstein–von_Mises_theorem" title="wikilink">Bernstein-von Mises theorem</a> asserts here the asymptotic convergence to the "true" distribution because the <a href="probability_space" title="wikilink">probability space</a> corresponding to the discrete set of events 

<math display="inline" id="Bayesian_inference:113">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <mrow>
    <mi>G</mi>
    <mi>D</mi>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mi>G</mi>
    <mover accent="true">
     <mi>D</mi>
     <mo stretchy="false">¯</mo>
    </mover>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mover accent="true">
     <mi>G</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mi>D</mi>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mover accent="true">
     <mi>G</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mover accent="true">
     <mi>D</mi>
     <mo stretchy="false">¯</mo>
    </mover>
   </mrow>
   <mo stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <apply>
     <times></times>
     <ci>G</ci>
     <ci>D</ci>
    </apply>
    <apply>
     <times></times>
     <ci>G</ci>
     <apply>
      <ci>normal-¯</ci>
      <ci>D</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <ci>normal-¯</ci>
      <ci>G</ci>
     </apply>
     <ci>D</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <ci>normal-¯</ci>
      <ci>G</ci>
     </apply>
     <apply>
      <ci>normal-¯</ci>
      <ci>D</ci>
     </apply>
    </apply>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{GD,G\bar{D},\bar{G}D,\bar{G}\bar{D}\}
  </annotation>
 </semantics>
</math>

 is finite (see above section on asymptotic behaviour of the posterior).</p>
<h2 id="in-frequentist-statistics-and-decision-theory">In frequentist statistics and decision theory</h2>

<p>A <a href="statistical_decision_theory" title="wikilink">decision-theoretic</a> justification of the use of Bayesian inference was given by <a href="Abraham_Wald" title="wikilink">Abraham Wald</a>, who proved that every unique Bayesian procedure is <a href="admissible_decision_rule" title="wikilink">admissible</a>. Conversely, every <a href="admissible_decision_rule" title="wikilink">admissible</a> statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>

<p>Wald characterized admissible procedures as Bayesian procedures (and limits of Bayesian procedures), making the Bayesian formalism a central technique in such areas of <a href="frequentist_inference" title="wikilink">frequentist inference</a> as <a href="parameter_estimation" title="wikilink">parameter estimation</a>, <a href="hypothesis_testing" title="wikilink">hypothesis testing</a>, and computing <a href="confidence_intervals" title="wikilink">confidence intervals</a>.<ref>* </ref></p>
<ul>
<li></li>
<li>

<p> For example:</p></li>
<li>"Under some conditions, all admissible procedures are either Bayes procedures or limits of Bayes procedures (in various senses). These remarkable results, at least in their original form, are due essentially to Wald. They are useful because the property of being Bayes is easier to analyze than admissibility."<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></li>
</ul>
<ul>
<li>"In decision theory, a quite general method for proving admissibility consists in exhibiting a procedure as a unique Bayes solution."<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></li>
</ul>
<ul>
<li>"In the first chapters of this work, prior distributions with finite support and the corresponding Bayes procedures were used to establish some of the main theorems relating to the comparison of experiments. Bayes procedures with respect to more general prior distributions have played a very important role in the development of statistics, including its asymptotic theory." "There are many problems where a glance at posterior distributions, for suitable priors, yields immediately interesting information. Also, this technique can hardly be avoided in sequential analysis."<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></li>
</ul>
<ul>
<li>"A useful fact is that any Bayes decision rule obtained by taking a proper prior over the whole parameter space must be admissible"<ref> page 432</ref></li>
</ul>

<p></p>
<ul>
<li>"An important area of investigation in the development of admissibility ideas has been that of conventional sampling-theory procedures, and many interesting results have been obtained."<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></li>
</ul>
<h3 id="model-selection">Model selection</h3>
<h2 id="applications">Applications</h2>
<h3 id="computer-applications">Computer applications</h3>

<p>Bayesian inference has applications in <a href="artificial_intelligence" title="wikilink">artificial intelligence</a> and <a href="expert_system" title="wikilink">expert systems</a>. Bayesian inference techniques have been a fundamental part of computerized <a href="pattern_recognition" title="wikilink">pattern recognition</a> techniques since the late 1950s. There is also an ever growing connection between Bayesian methods and simulation-based <a href="Monte_Carlo_method" title="wikilink">Monte Carlo</a> techniques since complex models cannot be processed in closed form by a Bayesian analysis, while a <a href="graphical_model" title="wikilink">graphical model</a> structure <em>may</em> allow for efficient simulation algorithms like the <a href="Gibbs_sampling" title="wikilink">Gibbs sampling</a> and other <a href="Metropolis–Hastings_algorithm" title="wikilink">Metropolis–Hastings algorithm</a> schemes.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> Recently Bayesian inference has gained popularity amongst the <a class="uri" href="phylogenetics" title="wikilink">phylogenetics</a> community for these reasons; a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously.</p>

<p>As applied to <a href="statistical_classification" title="wikilink">statistical classification</a>, Bayesian inference has been used in recent years to develop algorithms for identifying <a href="e-mail_spam" title="wikilink">e-mail spam</a>. Applications which make use of Bayesian inference for spam filtering include <a href="CRM114_(program)" title="wikilink">CRM114</a>, <a class="uri" href="DSPAM" title="wikilink">DSPAM</a>, <a class="uri" href="Bogofilter" title="wikilink">Bogofilter</a>, <a class="uri" href="SpamAssassin" title="wikilink">SpamAssassin</a>, <a class="uri" href="SpamBayes" title="wikilink">SpamBayes</a>, <a class="uri" href="Mozilla" title="wikilink">Mozilla</a>, XEAMS, and others. Spam classification is treated in more detail in the article on the <a href="naive_Bayes_classifier" title="wikilink">naive Bayes classifier</a>.</p>

<p><a href="Solomonoff's_theory_of_inductive_inference" title="wikilink">Solomonoff's Inductive inference</a> is the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable probability distribution. It is a formal inductive framework that combines two well-studied principles of inductive inference: Bayesian statistics and <a href="Occam’s_Razor" title="wikilink">Occam’s Razor</a>.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> Solomonoff's universal prior probability of any prefix <em>p</em> of a computable sequence <em>x</em> is the sum of the probabilities of all programs (for a universal computer) that compute something starting with <em>p</em>. Given some <em>p</em> and any computable but unknown probability distribution from which <em>x</em> is sampled, the universal prior and Bayes' theorem can be used to predict the yet unseen parts of <em>x</em> in optimal fashion.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a><a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<h3 id="in-the-courtroom">In the courtroom</h3>

<p>Bayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant, and to see whether, in totality, it meets their personal threshold for '<a href="beyond_a_reasonable_doubt" title="wikilink">beyond a reasonable doubt</a>'.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a><a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a><a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> Bayes' theorem is applied successively to all evidence presented, with the posterior from one stage becoming the prior for the next. The benefit of a Bayesian approach is that it gives the juror an unbiased, rational mechanism for combining evidence. It may be appropriate to explain Bayes' theorem to jurors in <a href="Bayes'_rule" title="wikilink">odds form</a>, as <a href="betting_odds" title="wikilink">betting odds</a> are more widely understood than probabilities. Alternatively, a <a href="Gambling_and_information_theory" title="wikilink">logarithmic approach</a>, replacing multiplication with addition, might be easier for a jury to handle.</p>

<p>[[Image:Ebits2c.png|thumb|256px|right|</p>
<center>

<p>Adding up evidence.</p>
</center>

<p>]]</p>

<p>If the existence of the crime is not in doubt, only the identity of the culprit, it has been suggested that the prior should be uniform over the qualifying population.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> For example, if 1,000 people could have committed the crime, the prior probability of guilt would be 1/1000.</p>

<p>The use of Bayes' theorem by jurors is controversial. In the United Kingdom, a defence <a href="expert_witness" title="wikilink">expert witness</a> explained Bayes' theorem to the jury in <em><a href="Regina_versus_Denis_John_Adams" title="wikilink">R v Adams</a></em>. The jury convicted, but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use Bayes' theorem. The Court of Appeal upheld the conviction, but it also gave the opinion that "To introduce Bayes' Theorem, or any similar method, into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity, deflecting them from their proper task."</p>

<p>Gardner-Medwin<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> argues that the criterion on which a verdict in a criminal trial should be based is <em>not</em> the probability of guilt, but rather the <em>probability of the evidence, given that the defendant is innocent</em> (akin to a <a class="uri" href="frequentist" title="wikilink">frequentist</a> <a class="uri" href="p-value" title="wikilink">p-value</a>). He argues that if the posterior probability of guilt is to be computed by Bayes' theorem, the prior probability of guilt must be known. This will depend on the incidence of the crime, which is an unusual piece of evidence to consider in a criminal trial. Consider the following three propositions:</p>
<dl>
<dd><strong>A</strong> The known facts and testimony could have arisen if the defendant is guilty
</dd>
<dd><strong>B</strong> The known facts and testimony could have arisen if the defendant is innocent
</dd>
<dd><strong>C</strong> The defendant is guilty.
</dd>
</dl>

<p>Gardner-Medwin argues that the jury should believe both A and not-B in order to convict. A and not-B implies the truth of C, but the reverse is not true. It is possible that B and C are both true, but in this case he argues that a jury should acquit, even though they know that they will be letting some guilty people go free. See also <a href="Lindley's_paradox" title="wikilink">Lindley's paradox</a>.</p>
<h3 id="bayesian-epistemology">Bayesian epistemology</h3>

<p>Bayesian <a href="epistemological" title="wikilink">epistemology</a> is a movement that advocates for Bayesian inference as a means of justifying the rules of inductive logic.</p>

<p><a href="Karl_Popper" title="wikilink">Karl Popper</a> and <a href="David_Miller_(philosopher)" title="wikilink">David Miller</a> have rejected the alleged rationality of Bayesianism, i.e. using Bayes rule to make epistemological inferences:<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> It is prone to the same <a href="vicious_circle" title="wikilink">vicious circle</a> as any other <a href="justificationism" title="wikilink">justificationist</a> epistemology, because it presupposes what it attempts to justify. According to this view, a rational interpretation of Bayesian inference would see it merely as a probabilistic version of <a href="falsifiability" title="wikilink">falsification</a>, rejecting the belief, commonly held by Bayesians, that high likelihood achieved by a series of Bayesian updates would prove the hypothesis beyond any reasonable doubt, or even with likelihood greater than 0.</p>
<h3 id="other">Other</h3>
<ul>
<li>The <a href="scientific_method" title="wikilink">scientific method</a> is sometimes interpreted as an application of Bayesian inference. In this view, Bayes' rule guides (or should guide) the updating of probabilities about <a href="hypothesis" title="wikilink">hypotheses</a> conditional on new observations or <a href="experiment" title="wikilink">experiments</a>.<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a></li>
<li><a href="Bayesian_search_theory" title="wikilink">Bayesian search theory</a> is used to search for lost objects.</li>
<li><a href="Bayesian_inference_in_phylogeny" title="wikilink">Bayesian inference in phylogeny</a></li>
<li><a href="Bayesian_tool_for_methylation_analysis" title="wikilink">Bayesian tool for methylation analysis</a></li>
</ul>
<h2 id="bayes-and-bayesian-inference">Bayes and Bayesian inference</h2>

<p>The problem considered by Bayes in Proposition 9 of his essay, "<a href="An_Essay_towards_solving_a_Problem_in_the_Doctrine_of_Chances" title="wikilink">An Essay towards solving a Problem in the Doctrine of Chances</a>", is the posterior distribution for the parameter <em>a</em> (the success rate) of the <a href="binomial_distribution" title="wikilink">binomial distribution</a>.</p>
<h2 id="history">History</h2>

<p>The term <em>Bayesian</em> refers to <a href="Thomas_Bayes" title="wikilink">Thomas Bayes</a> (1702–1761), who proved a special case of what is now called <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>. However, it was <a href="Pierre-Simon_Laplace" title="wikilink">Pierre-Simon Laplace</a> (1749–1827) who introduced a general version of the theorem and used it to approach problems in <a href="celestial_mechanics" title="wikilink">celestial mechanics</a>, medical statistics, <a href="Reliability_(statistics)" title="wikilink">reliability</a>, and <a class="uri" href="jurisprudence" title="wikilink">jurisprudence</a>.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> Early Bayesian inference, which used uniform priors following Laplace's <a href="principle_of_insufficient_reason" title="wikilink">principle of insufficient reason</a>, was called "<a href="inverse_probability" title="wikilink">inverse probability</a>" (because it <a href="Inductive_reasoning" title="wikilink">infers</a> backwards from observations to parameters, or from effects to causes<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a>). After the 1920s, "inverse probability" was largely supplanted by a collection of methods that came to be called <a href="frequentist_statistics" title="wikilink">frequentist statistics</a>.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>

<p>In the 20th century, the ideas of Laplace were further developed in two different directions, giving rise to <em>objective</em> and <em>subjective</em> currents in Bayesian practice. In the objective or "non-informative" current, the statistical analysis depends on only the model assumed, the data analyzed,<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> and the method assigning the prior, which differs from one objective Bayesian to another objective Bayesian. In the subjective or "informative" current, the specification of the prior depends on the belief (that is, propositions on which the analysis is prepared to act), which can summarize information from experts, previous studies, etc.</p>

<p>In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of <a href="Markov_chain_Monte_Carlo" title="wikilink">Markov chain Monte Carlo</a> methods, which removed many of the computational problems, and an increasing interest in nonstandard, complex applications.<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> Despite growth of Bayesian research, most undergraduate teaching is still based on frequentist statistics.<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a> Nonetheless, Bayesian methods are widely accepted and used, such as for example in the field of <a href="machine_learning" title="wikilink">machine learning</a>.<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a></li>
<li><a href="Bayesian_hierarchical_modeling" title="wikilink">Bayesian hierarchical modeling</a></li>
<li><a href="Bayesian_Analysis_(journal)" title="wikilink">Bayesian Analysis</a>, the journal of the ISBA</li>
<li><a href="Inductive_probability" title="wikilink">Inductive probability</a></li>
<li><a href="International_Society_for_Bayesian_Analysis" title="wikilink">International Society for Bayesian Analysis</a> (ISBA)</li>
<li><a href="Jeffreys_prior" title="wikilink">Jeffreys prior</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li>Aster, Richard; Borchers, Brian, and Thurber, Clifford (2012). <em>Parameter Estimation and Inverse Problems</em>, Second Edition, Elsevier. ISBN 0123850487, ISBN 978-0123850485</li>
<li></li>
<li><a href="George_E._P._Box" title="wikilink">Box, G. E. P.</a> and Tiao, G. C. (1973) <em>Bayesian Inference in Statistical Analysis</em>, Wiley, ISBN 0-471-57428-7</li>
<li></li>
<li></li>
<li><a href="Edwin_Thompson_Jaynes" title="wikilink">Jaynes E. T.</a> (2003) <em>Probability Theory: The Logic of Science</em>, CUP. ISBN 978-0-521-59271-0 (<a href="http://www-biba.inrialpes.fr/Jaynes/prob.html">Link to Fragmentary Edition of March 1996</a>).</li>
<li></li>
<li></li>
</ul>
<h2 id="further-reading">Further reading</h2>
<h3 id="elementary">Elementary</h3>

<p>The following books are listed in ascending order of probabilistic sophistication:</p>
<ul>
<li>Stone, JV (2013), "Bayes’ Rule: A Tutorial Introduction to Bayesian Analysis", <a href="http://jim-stone.staff.shef.ac.uk/BookBayes2012/BayesRuleBookMain.html">Download first chapter here</a>, Sebtel Press, England.</li>
<li></li>
<li></li>
<li></li>
<li></li>
<li>Bolstad, William M. (2007) <em>Introduction to Bayesian Statistics</em>: Second Edition, John Wiley ISBN 0-471-27020-2</li>
<li>

<p>Updated classic textbook. Bayesian theory clearly presented.</p></li>
<li>Lee, Peter M. <em>Bayesian Statistics: An Introduction</em>. Fourth Edition (2012), John Wiley ISBN 978-1-1183-3257-3</li>
<li></li>
<li></li>
</ul>
<h3 id="intermediate-or-advanced">Intermediate or advanced</h3>
<ul>
<li></li>
<li></li>
<li><a href="Morris_H._DeGroot" title="wikilink">DeGroot, Morris H.</a>, <em>Optimal Statistical Decisions</em>. Wiley Classics Library. 2004. (Originally published (1970) by McGraw-Hill.) ISBN 0-471-68029-X.</li>
<li></li>
<li>Jaynes, E. T. (1998) <a href="http://www-biba.inrialpes.fr/Jaynes/prob.html"><em>Probability Theory: The Logic of Science</em></a>.</li>
<li>O'Hagan, A. and Forster, J. (2003) <em>Kendall's Advanced Theory of Statistics</em>, Volume 2B: <em>Bayesian Inference</em>. Arnold, New York. ISBN 0-340-52922-9.</li>
<li></li>
<li><a href="Glenn_Shafer" title="wikilink">Glenn Shafer</a> and <a href="Judea_Pearl" title="wikilink">Pearl, Judea</a>, eds. (1988) <em>Probabilistic Reasoning in Intelligent Systems</em>, San Mateo, CA: Morgan Kaufmann.</li>
<li>Pierre Bessière et al. (2013), "<a href="http://www.crcpress.com/product/isbn/9781439880326">Bayesian Programming</a>", CRC Press. ISBN 9781439880326</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li></li>
<li><a href="http://www.scholarpedia.org/article/Bayesian_statistics">Bayesian Statistics</a> from Scholarpedia.</li>
<li><a href="http://www.dcs.qmw.ac.uk/%7Enorman/BBNs/BBNs.htm">Introduction to Bayesian probability</a> from Queen Mary University of London</li>
<li><a href="http://webuser.bus.umich.edu/plenk/downloads.htm">Mathematical Notes on Bayesian Statistics and Markov Chain Monte Carlo</a></li>
<li><a href="http://cocosci.berkeley.edu/tom/bayes.html">Bayesian reading list</a>, categorized and annotated by <a href="http://psychology.berkeley.edu/faculty/profiles/tgriffiths.html">Tom Griffiths</a></li>
<li>A. Hajek and S. Hartmann: <a href="http://stephanhartmann.org/HajekHartmann_BayesEpist.pdf">Bayesian Epistemology</a>, in: J. Dancy et al. (eds.), A Companion to Epistemology. Oxford: Blackwell 2010, 93-106.</li>
<li>S. Hartmann and J. Sprenger: <a href="http://stephanhartmann.org/HartmannSprenger_BayesEpis.pdf">Bayesian Epistemology</a>, in: S. Bernecker and D. Pritchard (eds.), Routledge Companion to Epistemology. London: Routledge 2010, 609-620.</li>
<li><a href="http://plato.stanford.edu/entries/logic-inductive/"><em>Stanford Encyclopedia of Philosophy</em>: "Inductive Logic"</a></li>
<li><a href="http://faculty-staff.ou.edu/H/James.A.Hawthorne-1/Hawthorne--Bayesian_Confirmation_Theory.pdf">Bayesian Confirmation Theory</a></li>
<li><a href="http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-7.html">What Is Bayesian Learning?</a></li>
</ul>

<p>"</p>

<p><a href="Category:Bayesian_inference" title="wikilink"> </a> <a href="Category:Statistical_theory" title="wikilink">Category:Statistical theory</a> <a href="Category:Statistical_inference" title="wikilink">Category:Statistical inference</a> <a href="Category:Logic_and_statistics" title="wikilink">Category:Logic and statistics</a> <a href="Category:Statistical_forecasting" title="wikilink">Category:Statistical forecasting</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Hacking (1967, Section 3, p. 316), Hacking (1988, p. 124)<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="Bas_van_Fraassen" title="wikilink">van Fraassen, B.</a> (1989) <em>Laws and Symmetry</em>, Oxford University Press. ISBN 0-19-824860-1<a href="#fnref3">↩</a></li>
<li id="fn4">Gelman, Andrew; Carlin, John B.; Stern, Hal S.; Dunson, David B.;Vehtari, Aki; Rubin, Donald B. (2013). <em>Bayesian Data Analysis</em>, Third Edition. Chapman and Hall/CRC. ISBN 978-1-4398-4095-5.<a href="#fnref4">↩</a></li>
<li id="fn5">Larry Wasserman et alia, JASA 2000.<a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7">Bickel &amp; Doksum (2001, p. 32)<a href="#fnref7">↩</a></li>
<li id="fn8"></li>
<li id="fn9"> (see p. 309 of Chapter 6.7 "Admissibilty", and pp. 17–18 of Chapter 1.8 "Complete Classes"<a href="#fnref9">↩</a></li>
<li id="fn10"> (From "Chapter 12 Posterior Distributions and Bayes Solutions", p. 324)<a href="#fnref10">↩</a></li>
<li id="fn11"> p. 433)<a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13">Samuel Rathmanner and <a href="Marcus_Hutter" title="wikilink">Marcus Hutter</a>. "A Philosophical Treatise of Universal Induction". <em>Entropy</em>, 13(6):1076–1136, 2011.<a href="#fnref13">↩</a></li>
<li id="fn14"><a href="http://arxiv.org/pdf/0709.1516">"The Problem of Old Evidence"</a>, in §5 of "On Universal Prediction and Bayesian Confirmation", M. Hutter - Theoretical Computer Science, 2007 - Elsevier<a href="#fnref14">↩</a></li>
<li id="fn15">[<a class="uri" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.186.8268&amp;rep">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.186.8268&amp;rep;</a>;=rep1&amp;type;=pdf "Raymond J. Solomonoff"], Peter Gacs, Paul M. B. Vitanyi, 2011 cs.bu.edu<a href="#fnref15">↩</a></li>
<li id="fn16">Dawid, A. P. and Mortera, J. (1996) "Coherent Analysis of Forensic Identification Evidence". <em><a href="Journal_of_the_Royal_Statistical_Society" title="wikilink">Journal of the Royal Statistical Society</a></em>, Series B, 58, 425–443.<a href="#fnref16">↩</a></li>
<li id="fn17">Foreman, L. A.; Smith, A. F. M., and Evett, I. W. (1997). "Bayesian analysis of deoxyribonucleic acid profiling data in forensic identification applications (with discussion)". <em>Journal of the Royal Statistical Society</em>, Series A, 160, 429–469.<a href="#fnref17">↩</a></li>
<li id="fn18">Robertson, B. and Vignaux, G. A. (1995) <em>Interpreting Evidence: Evaluating Forensic Science in the Courtroom</em>. John Wiley and Sons. Chichester. ISBN 978-0-471-96026-3<a href="#fnref18">↩</a></li>
<li id="fn19">Dawid, A. P. (2001) "Bayes' Theorem and Weighing Evidence by Juries"; <a class="uri" href="http://128.40.111.250/evidence/content/dawid-paper.pdf">http://128.40.111.250/evidence/content/dawid-paper.pdf</a><a href="#fnref19">↩</a></li>
<li id="fn20">Gardner-Medwin, A. (2005) "What Probability Should the Jury Address?". <em><a href="Significance_(journal)" title="wikilink">Significance</a></em>, 2 (1), March 2005<a href="#fnref20">↩</a></li>
<li id="fn21">David Miller: <em>Critical Rationalism</em><a href="#fnref21">↩</a></li>
<li id="fn22">Howson &amp; Urbach (2005), Jaynes (2003)<a href="#fnref22">↩</a></li>
<li id="fn23"><a href="#fnref23">↩</a></li>
<li id="fn24"><a href="#fnref24">↩</a></li>
<li id="fn25"></li>
<li id="fn26"><a href="#fnref26">↩</a></li>
<li id="fn27"><a href="#fnref27">↩</a></li>
<li id="fn28"><a href="#fnref28">↩</a></li>
<li id="fn29"><a href="#fnref29">↩</a></li>
</ol>
</section>
</body>
</html>
