<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1771">Monte Carlo method</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Monte Carlo method</h1>
<hr/>

<p><strong>Monte Carlo methods</strong> (or <strong>Monte Carlo experiments</strong>) are a broad class of <a href="computation" title="wikilink">computational</a> <a href="algorithm" title="wikilink">algorithms</a> that rely on repeated <a class="uri" href="random" title="wikilink">random</a> sampling to obtain numerical results. They are often used in <a href="physics" title="wikilink">physical</a> and <a href="mathematics" title="wikilink">mathematical</a> problems and are most useful when it is difficult or impossible to use other mathematical methods. Monte Carlo methods are mainly used in three distinct problem classes: <a class="uri" href="optimization" title="wikilink">optimization</a>, <a href="numerical_integration" title="wikilink">numerical integration</a>, and generating draws from a <a href="probability_distribution" title="wikilink">probability distribution</a>.</p>

<p>In physics-related problems, Monte Carlo methods are quite useful for simulating systems with many <a href="coupling_(physics)" title="wikilink">coupled</a> degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see <a href="cellular_Potts_model" title="wikilink">cellular Potts model</a>, <a href="interacting_particle_systems" title="wikilink">interacting particle systems</a>, <a href="McKean–Vlasov_process" title="wikilink">McKean-Vlasov processes</a>, <a href="Kinetic_theory" title="wikilink">kinetic models of gases</a>). Other examples include modeling phenomena with significant <a class="uri" href="uncertainty" title="wikilink">uncertainty</a> in inputs such as the calculation of <a class="uri" href="risk" title="wikilink">risk</a> in business and, in math, evaluation of multidimensional <a href="Integral" title="wikilink">definite integrals</a> with complicated boundary conditions. In application to space and oil exploration problems, Monte Carlo–based predictions of failure, <a href="cost_overrun" title="wikilink">cost overruns</a> and schedule overruns are routinely better than human intuition or alternative "soft" methods.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>In principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By <a href="Law_of_large_numbers" title="wikilink">the law of large numbers</a>, integrals described by the <a href="expected_value" title="wikilink">expected value</a> of some random variable can be approximated by taking the <a href="Sample_mean_and_sample_covariance" title="wikilink">empirical mean</a> (a.k.a. the sample mean) of independent samples of the variable. When the <a href="probability_distribution" title="wikilink">probability distribution</a> of the variable is too complex, we often use a <a href="Markov_chain_Monte_Carlo" title="wikilink">Markov Chain Monte Carlo</a> (MCMC) sampler. The central idea is to design a judicious <a href="Markov_chain" title="wikilink">Markov chain</a> model with a prescribed <a href="stationary_probability_distribution" title="wikilink">stationary probability distribution</a><a href="Stationary_distribution" title="wikilink">.</a> By the <a href="ergodic_theorem" title="wikilink">ergodic theorem</a>, the <a href="stationary_probability_distribution" title="wikilink">stationary probability distribution</a> is approximated by the <a href="empirical_measure" title="wikilink">empirical measures</a> of the random states of the MCMC sampler.</p>

<p>In other important problems we are interested in generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depends on the distributions of the current random states (see <a href="McKean–Vlasov_process" title="wikilink">McKean-Vlasov processes</a>, <a href="particle_filter" title="wikilink">nonlinear filtering equation</a>).<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann-Bibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled <a href="empirical_measure" title="wikilink">empirical measures</a>. In contrast with traditional Monte Carlo and <a href="Markov_chain_Monte_Carlo" title="wikilink">Markov chain Monte Carlo</a> methodologies these <a href="Mean_field_particle_methods" title="wikilink">mean field particle</a> techniques rely on sequential interacting samples. The terminology mean field reflects the fact that each of the <em>samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes)</em> interacts with the <a href="empirical_measure" title="wikilink">empirical measures</a> of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.</p>
<h2 id="introduction">Introduction</h2>

<p> Monte Carlo methods vary, but tend to follow a particular pattern:</p>
<ol>
<li>Define a domain of possible inputs.</li>
<li>Generate inputs randomly from a <a href="probability_distribution" title="wikilink">probability distribution</a> over the domain.</li>
<li>Perform a <a href="Deterministic_algorithm" title="wikilink">deterministic</a> computation on the inputs.</li>
<li>Aggregate the results.</li>
</ol>

<p>For example, consider a circle inscribed in a unit square. Given that the circle and the square have a ratio of areas that is 

<math display="inline" id="Monte_Carlo_method:0">
 <semantics>
  <mi>π</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>π</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi
  </annotation>
 </semantics>
</math>

/4, the value of <a href="pi" title="wikilink">

<math display="inline" id="Monte_Carlo_method:1">
 <semantics>
  <mi>π</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>π</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi
  </annotation>
 </semantics>
</math>

</a> can be approximated using a Monte Carlo method:<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<ol>
<li>Draw a square on the ground, then <a href="inscribed_figure" title="wikilink">inscribe</a> a circle within it.</li>
<li><a href="uniform_distribution_(continuous)" title="wikilink">Uniformly</a> scatter some objects of uniform size (grains of rice or sand) over the square.</li>
<li>Count the number of objects inside the circle and the total number of objects.</li>
<li>The ratio of the two counts is an estimate of the ratio of the two areas, which is 

<math display="inline" id="Monte_Carlo_method:2">
 <semantics>
  <mi>π</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>π</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi
  </annotation>
 </semantics>
</math>

/4. Multiply the result by 4 to estimate 

<math display="inline" id="Monte_Carlo_method:3">
 <semantics>
  <mi>π</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>π</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi
  </annotation>
 </semantics>
</math>

.</li>
</ol>

<p>In this procedure the domain of inputs is the square that circumscribes our circle. We generate random inputs by scattering grains over the square then perform a computation on each input (test whether it falls within the circle). Finally, we aggregate the results to obtain our final result, the approximation of 

<math display="inline" id="Monte_Carlo_method:4">
 <semantics>
  <mi>π</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>π</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi
  </annotation>
 </semantics>
</math>

.</p>

<p>There are two important points to consider here: Firstly, if the grains are not uniformly distributed, then our approximation will be poor. Secondly, there should be a large number of inputs. The approximation is generally poor if only a few grains are randomly dropped into the whole square. On average, the approximation improves as more grains are dropped.</p>

<p>Uses of Monte Carlo methods require large amounts of random numbers, and it was their use that spurred the development of <a href="pseudorandom_number_generator" title="wikilink">pseudorandom number generators</a>, which were far quicker to use than the tables of random numbers that had been previously used for statistical sampling.</p>
<h2 id="history">History</h2>

<p>Before the Monte Carlo method was developed, simulations tested a previously understood deterministic problem and statistical sampling was used to estimate uncertainties in the simulations. Monte Carlo simulations invert this approach, solving deterministic problems using a <a class="uri" href="probabilistic" title="wikilink">probabilistic</a> <a href="meta-algorithm" title="wikilink">analog</a> (see <a href="Simulated_annealing" title="wikilink">Simulated annealing</a>).</p>

<p>An early variant of the Monte Carlo method can be seen in the <a href="Buffon's_needle" title="wikilink">Buffon's needle</a> experiment, in which 

<math display="inline" id="Monte_Carlo_method:5">
 <semantics>
  <mi>π</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>π</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi
  </annotation>
 </semantics>
</math>

 can be estimated by dropping needles on a floor made of parallel and equidistant strips. In the 1930s, <a href="Enrico_Fermi" title="wikilink">Enrico Fermi</a> first experimented with the Monte Carlo method while studying neutron diffusion, but did not publish anything on it.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>

<p>The modern version of the Markov Chain Monte Carlo method was invented in the late 1940s by <a href="Stanislaw_Ulam" title="wikilink">Stanislaw Ulam</a>, while he was working on nuclear weapons projects at the <a href="Los_Alamos_National_Laboratory" title="wikilink">Los Alamos National Laboratory</a>. Immediately after Ulam's breakthrough, <a href="John_von_Neumann" title="wikilink">John von Neumann</a> understood its importance and programmed the <a class="uri" href="ENIAC" title="wikilink">ENIAC</a> computer to carry out Monte Carlo calculations. In 1946, physicists at <a href="Los_Alamos_Scientific_Laboratory" title="wikilink">Los Alamos Scientific Laboratory</a> were investigating <a href="radiation_shielding" title="wikilink">radiation shielding</a> and the distance that <a class="uri" href="neutrons" title="wikilink">neutrons</a> would likely travel through various materials. Despite having most of the necessary data, such as the average distance a neutron would travel in a substance before it collided with an atomic nucleus, and how much energy the neutron was likely to give off following a collision, the Los Alamos physicists were unable to solve the problem using conventional, deterministic mathematical methods. <a href="Stanislaw_Ulam" title="wikilink">Stanislaw Ulam</a> had the idea of using random experiments. He recounts his inspiration as follows:</p>
<dl>
<dd><dl>
<dd>The first thoughts and attempts I made to practice [the Monte Carlo Method] were suggested by a question which occurred to me in 1946 as I was convalescing from an illness and playing solitaires. The question was what are the chances that a <a href="Canfield_(solitaire)" title="wikilink">Canfield solitaire</a> laid out with 52 cards will come out successfully? After spending a lot of time trying to estimate them by pure combinatorial calculations, I wondered whether a more practical method than "abstract thinking" might not be to lay it out say one hundred times and simply observe and count the number of successful plays. This was already possible to envisage with the beginning of the new era of fast computers, and I immediately thought of problems of neutron diffusion and other questions of mathematical physics, and more generally how to change processes described by certain differential equations into an equivalent form interpretable as a succession of random operations. Later [in 1946], I described the idea to <a href="John_von_Neumann" title="wikilink">John von Neumann</a>, and we began to plan actual calculations.
<dl>
<dd>–<a href="Stanislaw_Ulam" title="wikilink">Stanislaw Ulam</a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>

<p>Being secret, the work of von Neumann and Ulam required a code name. A colleague of von Neumann and Ulam, <a href="Nicholas_Metropolis" title="wikilink">Nicholas Metropolis</a>, suggested using the name <em>Monte Carlo</em>, which refers to the <a href="Monte_Carlo_Casino" title="wikilink">Monte Carlo Casino</a> in <a class="uri" href="Monaco" title="wikilink">Monaco</a> where Ulam's uncle would borrow money from relatives to gamble.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> Using <a href="A_Million_Random_Digits_with_100,000_Normal_Deviates" title="wikilink">lists of "truly random" random numbers</a> was extremely slow, but von Neumann developed a way to calculate <a href="pseudorandom_number" title="wikilink">pseudorandom numbers</a>, using the <a href="middle-square_method" title="wikilink">middle-square method</a>. Though this method has been criticized as crude, von Neumann was aware of this: he justified it as being faster than any other method at his disposal, and also noted that when it went awry it did so obviously, unlike methods that could be subtly incorrect.</p>

<p>Monte Carlo methods were central to the <a href="simulation" title="wikilink">simulations</a> required for the <a href="Manhattan_Project" title="wikilink">Manhattan Project</a>, though severely limited by the computational tools at the time. In the 1950s they were used at <a href="Los_Alamos_National_Laboratory" title="wikilink">Los Alamos</a> for early work relating to the development of the <a href="hydrogen_bomb" title="wikilink">hydrogen bomb</a>, and became popularized in the fields of <a class="uri" href="physics" title="wikilink">physics</a>, <a href="physical_chemistry" title="wikilink">physical chemistry</a>, and <a href="operations_research" title="wikilink">operations research</a>. The <a href="Rand_Corporation" title="wikilink">Rand Corporation</a> and the <a href="U.S._Air_Force" title="wikilink">U.S. Air Force</a> were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time, and they began to find a wide application in many different fields.</p>

<p>The theory of more sophisticated mean field type particle Monte Carlo methods had certainly started by the mid-1960s, with the work of <a href="Henry_McKean" title="wikilink">Henry P. McKean Jr.</a> on Markov interpretations of a class of nonlinear parabolic partial differential equations arising in fluid mechanics.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> We also quote an earlier pioneering article by <a href="Ted_Harris_(mathematician)" title="wikilink">Theodore E. Harris</a> and Herman Kahn, published in 1951, using mean field <a href="genetic_algorithm" title="wikilink">genetic</a>-type Monte Carlo methods for estimating particle transmission energies.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> Mean field genetic type Monte Carlo methodologies are also used as heuristic natural search algorithms (a.k.a. <a class="uri" href="Metaheuristic" title="wikilink">Metaheuristic</a>) in evolutionary computing. The origins of these mean field computational techniques can be traced to 1950 and 1954 with the work of <a href="Alan_Turing" title="wikilink">Alan Turing</a> on genetic type mutation-selection learning machines <a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> and the articles by <a href="Nils_Aall_Barricelli" title="wikilink">Nils Aall Barricelli</a> at the <a href="Institute_for_Advanced_Study" title="wikilink">Institute for Advanced Study</a> in <a href="Princeton,_New_Jersey" title="wikilink">Princeton, New Jersey</a>.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a><a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>

<p><a href="Quantum_Monte_Carlo" title="wikilink">Quantum Monte Carlo</a>, and more specifically <a href="Diffusion_Monte_Carlo" title="wikilink">Diffusion Monte Carlo methods</a> can also be interpreted as a mean field particle Monte Carlo approximation of Feynman-Kac path integrals.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a><a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a><a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a><a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a><a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a><a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a><a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions,<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> but the first heuristic-like and genetic type particle algorithm (a.k.a. Resampled or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> In molecular chemistry, the use of genetic heuristic-like particle methodologies (a.k.a. pruning and enrichment strategies) can be traced back to 1955 with the seminal work of Marshall. N. Rosenbluth and Arianna. W. Rosenbluth.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>

<p>The use of <a href="Sequential_Monte_Carlo_method" title="wikilink">Sequential Monte Carlo</a> in advanced <a href="Signal_processing" title="wikilink">Signal processing</a> and <a href="Bayesian_inference" title="wikilink">Bayesian inference</a> is more recent. It was in 1993, that Gordon et al., published in their seminal work<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> the first application of a Monte Carlo <a href="Resampling_(statistics)" title="wikilink">resampling</a> algorithm in Bayesian statistical inference. The authors named their algorithm 'the bootstrap filter', and demonstrated that compared to other filtering methods, their bootstrap algorithm does not require any assumption about that state-space or the noise of the system. We also quote another pioneering article in this field of Genshiro Kitagawa on a related "Monte Carlo filter",<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> and the ones by Pierre Del Moral<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a> and Himilcon Carvalho, Pierre Del Moral, André Monin and Gérard Salut<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a> on particle filters published in the mid-1990s. Particle filters were also developed in signal processing in the early 1989-1992 by P. Del Moral, J.C. Noyer, G. Rigal, and G. Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN (Service Technique des Constructions et Armes Navales), the IT company DIGILOG, and the <a href="https://www.laas.fr/public/en">LAAS-CNRS</a> (the Laboratory for Analysis and Architecture of Systems) on RADAR/SONAR and GPS signal processing problems.<a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a><a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a><a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a><a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a><a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a><a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a> These Sequential Monte Carlo methodologies can be interpreted as an acceptance-rejection sampler equipped with an interacting recycling mechanism.</p>

<p>From 1950 to 1996, all the publications on <a href="Sequential_Monte_Carlo_method" title="wikilink">Sequential Monte Carlo</a> methodologies including the pruning and resample Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like algorithms applied to different situations without a single proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms. The mathematical foundations and the first rigorous analysis of these particle algorithms are due to Pierre Del Moral<a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a><a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a> in 1996. Branching type particle methodologies with varying population sizes were also developed in the end of the 1990s by Dan Crisan, Jessica Gaines and Terry Lyons,<a class="footnoteRef" href="#fn38" id="fnref38"><sup>38</sup></a><a class="footnoteRef" href="#fn39" id="fnref39"><sup>39</sup></a><a class="footnoteRef" href="#fn40" id="fnref40"><sup>40</sup></a> and by Dan Crisan, Pierre Del Moral and Terry Lyons.<a class="footnoteRef" href="#fn41" id="fnref41"><sup>41</sup></a> Further developments in this field were developed in 2000 by P. Del Moral, A. Guionnet and L. Miclo.<a class="footnoteRef" href="#fn42" id="fnref42"><sup>42</sup></a><a class="footnoteRef" href="#fn43" id="fnref43"><sup>43</sup></a><a class="footnoteRef" href="#fn44" id="fnref44"><sup>44</sup></a></p>
<h2 id="definitions">Definitions</h2>

<p>There is no consensus on how <em>Monte Carlo</em> should be defined. For example, Ripley<a class="footnoteRef" href="#fn45" id="fnref45"><sup>45</sup></a> defines most probabilistic modeling as <em><a href="stochastic_simulation" title="wikilink">stochastic simulation</a></em>, with <em>Monte Carlo</em> being reserved for <a href="Monte_Carlo_integration" title="wikilink">Monte Carlo integration</a> and Monte Carlo statistical tests. <a href="Shlomo_Sawilowsky" title="wikilink">Sawilowsky</a><a class="footnoteRef" href="#fn46" id="fnref46"><sup>46</sup></a> distinguishes between a <a class="uri" href="simulation" title="wikilink">simulation</a>, a Monte Carlo method, and a Monte Carlo simulation: a simulation is a fictitious representation of reality, a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem, and a Monte Carlo simulation uses repeated sampling to determine the properties of some phenomenon (or behavior). Examples:</p>
<ul>
<li>Simulation: Drawing <strong>one</strong> pseudo-random uniform variable from the interval [0,1] can be used to simulate the tossing of a coin: If the value is less than or equal to 0.50 designate the outcome as heads, but if the value is greater than 0.50 designate the outcome as tails. This is a simulation, but not a Monte Carlo simulation.</li>
<li>Monte Carlo method: Pouring out a box of coins on a table, and then computing the ratio of coins that land heads versus tails is a Monte Carlo method of determining the behavior of repeated coin tosses, but it is not a simulation.</li>
<li>Monte Carlo simulation: Drawing <strong>a large number</strong> of pseudo-random uniform variables from the interval [0,1], and assigning values less than or equal to 0.50 as heads and greater than 0.50 as tails, is a <em>Monte Carlo simulation</em> of the behavior of repeatedly tossing a coin.</li>
</ul>

<p>Kalos and Whitlock<a class="footnoteRef" href="#fn47" id="fnref47"><sup>47</sup></a> point out that such distinctions are not always easy to maintain. For example, the emission of radiation from atoms is a natural stochastic process. It can be simulated directly, or its average behavior can be described by stochastic equations that can themselves be solved using Monte Carlo methods. "Indeed, the same computer code can be viewed simultaneously as a 'natural simulation' or as a solution of the equations by natural sampling."</p>
<h3 id="monte-carlo-and-random-numbers">Monte Carlo and random numbers</h3>

<p>Monte Carlo simulation methods do not always require <a href='Random_number_generation#"True"_random_numbers_vs._pseudo-random_numbers' title="wikilink">truly random numbers</a> to be useful — while for some applications, such as <a href="primality_testing" title="wikilink">primality testing</a>, unpredictability is vital.<a class="footnoteRef" href="#fn48" id="fnref48"><sup>48</sup></a> Many of the most useful techniques use deterministic, <a href="pseudorandom_number_generator" title="wikilink">pseudorandom</a> sequences, making it easy to test and re-run simulations. The only quality usually necessary to make good <a href="simulation" title="wikilink">simulations</a> is for the pseudo-random sequence to appear "random enough" in a certain sense.</p>

<p>What this means depends on the application, but typically they should pass a series of statistical tests. Testing that the numbers are <a href="Uniform_distribution_(continuous)" title="wikilink">uniformly distributed</a> or follow another desired distribution when a large enough number of elements of the sequence are considered is one of the simplest, and most common ones. Weak correlations between successive samples is also often desirable/necessary.</p>

<p>Sawilowsky lists the characteristics of a high quality Monte Carlo simulation:<a class="footnoteRef" href="#fn49" id="fnref49"><sup>49</sup></a></p>
<ul>
<li>the (pseudo-random) number generator has certain characteristics (<em>e.g.</em>, a long "period" before the sequence repeats)</li>
<li>the (pseudo-random) number generator produces values that pass tests for randomness</li>
<li>there are enough samples to ensure accurate results</li>
<li>the proper sampling technique is used</li>
<li>the algorithm used is valid for what is being modeled</li>
<li>it simulates the phenomenon in question.</li>
</ul>

<p><a href="Pseudo-random_number_sampling" title="wikilink">Pseudo-random number sampling</a> algorithms are used to transform uniformly distributed pseudo-random numbers into numbers that are distributed according to a given <a href="probability_distribution" title="wikilink">probability distribution</a>.</p>

<p><a href="Low-discrepancy_sequences" title="wikilink">Low-discrepancy sequences</a> are often used instead of random sampling from a space as they ensure even coverage and normally have a faster order of convergence than Monte Carlo simulations using random or pseudorandom sequences. Methods based on their use are called <a href="quasi-Monte_Carlo_method" title="wikilink">quasi-Monte Carlo methods</a>.</p>
<h3 id="monte-carlo-simulation-versus-what-if-scenarios">Monte Carlo simulation versus "what if" scenarios</h3>

<p>There are ways of using probabilities that are definitely not Monte Carlo simulations — for example, deterministic modeling using single-point estimates. Each uncertain variable within a model is assigned a “best guess” estimate. Scenarios (such as best, worst, or most likely case) for each input variable are chosen and the results recorded.<a class="footnoteRef" href="#fn50" id="fnref50"><sup>50</sup></a></p>

<p>By contrast, Monte Carlo simulations sample from a <a href="probability_distribution" title="wikilink">probability distribution</a> for each variable to produce hundreds or thousands of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring.<a class="footnoteRef" href="#fn51" id="fnref51"><sup>51</sup></a> For example, a comparison of a spreadsheet cost construction model run using traditional “what if” scenarios, and then running the comparison again with Monte Carlo simulation and <a href="triangular_distribution" title="wikilink">triangular probability distributions</a> shows that the Monte Carlo analysis has a narrower range than the “what if” analysis. This is because the “what if” analysis gives equal weight to all scenarios (see <a href="Corporate_finance#Quantifying_uncertainty" title="wikilink">quantifying uncertainty in corporate finance</a>), while the Monte Carlo method hardly samples in the very low probability regions. The samples in such regions are called "rare events".</p>
<h2 id="applications">Applications</h2>

<p>Monte Carlo methods are especially useful for simulating phenomena with significant <a class="uri" href="uncertainty" title="wikilink">uncertainty</a> in inputs and systems with a large number of <a href="coupling_(physics)" title="wikilink">coupled</a> degrees of freedom. Areas of application include:</p>
<h3 id="physical-sciences">Physical sciences</h3>

<p>Monte Carlo methods are very important in <a href="computational_physics" title="wikilink">computational physics</a>, <a href="physical_chemistry" title="wikilink">physical chemistry</a>, and related applied fields, and have diverse applications from complicated <a href="quantum_chromodynamics" title="wikilink">quantum chromodynamics</a> calculations to designing <a href="heat_shield" title="wikilink">heat shields</a> and <a href="aerodynamics" title="wikilink">aerodynamic</a> forms as well as in modeling radiation transport for radiation dosimetry calculations.<a class="footnoteRef" href="#fn52" id="fnref52"><sup>52</sup></a><a class="footnoteRef" href="#fn53" id="fnref53"><sup>53</sup></a><a class="footnoteRef" href="#fn54" id="fnref54"><sup>54</sup></a> In <a href="statistical_physics" title="wikilink">statistical physics</a> <a href="Monte_Carlo_molecular_modeling" title="wikilink">Monte Carlo molecular modeling</a> is an alternative to computational <a href="molecular_dynamics" title="wikilink">molecular dynamics</a>, and Monte Carlo methods are used to compute <a href="statistical_field_theory" title="wikilink">statistical field theories</a> of simple particle and polymer systems.<a class="footnoteRef" href="#fn55" id="fnref55"><sup>55</sup></a><a class="footnoteRef" href="#fn56" id="fnref56"><sup>56</sup></a> <a href="Quantum_Monte_Carlo" title="wikilink">Quantum Monte Carlo</a> methods solve the <a href="many-body_problem" title="wikilink">many-body problem</a> for quantum systems.<a class="footnoteRef" href="#fn57" id="fnref57"><sup>57</sup></a><a class="footnoteRef" href="#fn58" id="fnref58"><sup>58</sup></a><a class="footnoteRef" href="#fn59" id="fnref59"><sup>59</sup></a> In experimental <a href="particle_physics" title="wikilink">particle physics</a>, Monte Carlo methods are used for designing <a href="particle_detector" title="wikilink">detectors</a>, understanding their behavior and comparing experimental data to theory. In <a class="uri" href="astrophysics" title="wikilink">astrophysics</a>, they are used in such diverse manners as to model both <a class="uri" href="galaxy" title="wikilink">galaxy</a> evolution<a class="footnoteRef" href="#fn60" id="fnref60"><sup>60</sup></a> and microwave radiation transmission through a rough planetary surface.<a class="footnoteRef" href="#fn61" id="fnref61"><sup>61</sup></a> Monte Carlo methods are also used in the <a href="Ensemble_forecasting" title="wikilink">ensemble models</a> that form the basis of modern <a href="Numerical_weather_prediction" title="wikilink">weather forecasting</a>.</p>
<h3 id="engineering">Engineering</h3>

<p>Monte Carlo methods are widely used in engineering for <a href="sensitivity_analysis" title="wikilink">sensitivity analysis</a> and quantitative <a class="uri" href="probabilistic" title="wikilink">probabilistic</a> analysis in <a href="Process_design_(chemical_engineering)" title="wikilink">process design</a>. The need arises from the interactive, co-linear and non-linear behavior of typical process simulations. For example,</p>
<ul>
<li>In <a href="microelectronics" title="wikilink">microelectronics engineering</a>, Monte Carlo methods are applied to analyze correlated and uncorrelated variations in <a href="Analog_signal" title="wikilink">analog</a> and <a href="Digital_data" title="wikilink">digital</a> <a href="integrated_circuits" title="wikilink">integrated circuits</a>.</li>
<li>In <a class="uri" href="geostatistics" title="wikilink">geostatistics</a> and <a class="uri" href="geometallurgy" title="wikilink">geometallurgy</a>, Monte Carlo methods underpin the design of <a href="mineral_processing" title="wikilink">mineral processing</a> <a href="process_flow_diagram" title="wikilink">flowsheets</a> and contribute to quantitative <a href="Quantitative_risk_analysis" title="wikilink">risk analysis</a>.</li>
<li>In <a href="wind_energy" title="wikilink">wind energy</a> yield analysis, the predicted energy output of a wind farm during its lifetime is calculated giving different levels of uncertainty (<a href="Percentile" title="wikilink">P90</a>, P50, etc.)</li>
<li>impacts of pollution are simulated<a class="footnoteRef" href="#fn62" id="fnref62"><sup>62</sup></a> and diesel compared with petrol.<a class="footnoteRef" href="#fn63" id="fnref63"><sup>63</sup></a></li>
<li>In <a href="Fluid_Dynamics" title="wikilink">Fluid Dynamics</a>, in particular <a href="gas_dynamics" title="wikilink">Rarefied Gas Dynamics</a>, where the Boltzmann equation is solved for finite Knudsen number fluid flows using the <a href="Direct_Simulation_Monte_Carlo" title="wikilink">Direct Simulation Monte Carlo</a> <a class="footnoteRef" href="#fn64" id="fnref64"><sup>64</sup></a> method in combination with highly efficient computational algorithms.<a class="footnoteRef" href="#fn65" id="fnref65"><sup>65</sup></a></li>
<li>In <a href="autonomous_robotics" title="wikilink">autonomous robotics</a>, <a href="Monte_Carlo_localization" title="wikilink">Monte Carlo localization</a> can determine the position of a robot. It is often applied to stochastic filters such as the <a href="Kalman_filter" title="wikilink">Kalman filter</a> or <a href="Particle_filter" title="wikilink">Particle filter</a> that forms the heart of the <a href="Simultaneous_localization_and_mapping" title="wikilink">SLAM</a> (Simultaneous Localization and Mapping) algorithm.</li>
<li>In <a class="uri" href="telecommunications" title="wikilink">telecommunications</a>, when planning a wireless network, design must be proved to work for a wide variety of scenarios that depend mainly on the number of users, their locations and the services they want to use. Monte Carlo methods are typically used to generate these users and their states. The network performance is then evaluated and, if results are not satisfactory, the network design goes through an optimization process.</li>
<li>In <a href="reliability_engineering" title="wikilink">reliability engineering</a>, one can use Monte Carlo simulation to generate <a href="mean_time_between_failures" title="wikilink">mean time between failures</a> and <a href="mean_time_to_repair" title="wikilink">mean time to repair</a> for components.</li>
<li>In <a href="signal_processing" title="wikilink">signal processing</a> and <a href="Bayesian_inference" title="wikilink">Bayesian inference</a>, <a href="particle_filter" title="wikilink">particle filters</a> and <a href="Sequential_Monte_Carlo_method" title="wikilink">sequential Monte Carlo techniques</a> are a class of <a href="mean_field_particle_methods" title="wikilink">mean field particle methods</a> for sampling and computing the posterior distribution of a signal process given some noisy and partial observations using interacting <a href="empirical_measure" title="wikilink">empirical measure</a>s.</li>
</ul>
<h3 id="computational-biology">Computational biology</h3>

<p>Monte Carlo methods are used in various fields of computational biology, for example for <a href="Bayesian_inference_in_phylogeny" title="wikilink">Bayesian inference in phylogeny</a>, or for studying biological systems such as genomes, proteins,<a class="footnoteRef" href="#fn66" id="fnref66"><sup>66</sup></a> or membranes.<a class="footnoteRef" href="#fn67" id="fnref67"><sup>67</sup></a> The systems can be studied in the coarse-grained or <em>ab initio</em> frameworks depending on the desired accuracy. Computer simulations allow us to monitor the local environment of a particular molecule to see if some chemical reaction is happening for instance. In cases where it is not feasible to conduct a physical experiment, <a href="thought_experiment" title="wikilink">thought experiments</a> can be conducted (for instance: breaking bonds, introducing impurities at specific sites, changing the local/global structure, or introducing external fields).</p>
<h3 id="computer-graphics">Computer graphics</h3>

<p><a href="Path_Tracing" title="wikilink">Path Tracing</a>, occasionally referred to as Monte Carlo Ray Tracing, renders a 3D scene by randomly tracing samples of possible light paths. Repeated sampling of any given pixel will eventually cause the average of the samples to converge on the correct solution of the <a href="rendering_equation" title="wikilink">rendering equation</a>, making it one of the most physically accurate 3D graphics rendering methods in existence.</p>
<h3 id="applied-statistics">Applied statistics</h3>

<p>In applied statistics, Monte Carlo methods are generally used for two purposes:</p>
<ol>
<li>To compare competing statistics for small samples under realistic data conditions. Although <a href="Type_I_error" title="wikilink">Type I error</a> and power properties of statistics can be calculated for data drawn from classical theoretical distributions (<em>e.g.</em>, <a href="normal_curve" title="wikilink">normal curve</a>, <a href="Cauchy_distribution" title="wikilink">Cauchy distribution</a>) for <a class="uri" href="asymptotic" title="wikilink">asymptotic</a> conditions (<em>i. e</em>, infinite sample size and infinitesimally small treatment effect), real data often do not have such distributions.<a class="footnoteRef" href="#fn68" id="fnref68"><sup>68</sup></a></li>
<li>To provide implementations of <a href="Statistical_hypothesis_testing" title="wikilink">hypothesis tests</a> that are more efficient than exact tests such as <a href="permutation_tests" title="wikilink">permutation tests</a> (which are often impossible to compute) while being more accurate than critical values for <a href="asymptotic_distribution" title="wikilink">asymptotic distributions</a>.</li>
</ol>

<p>Monte Carlo methods are also a compromise between approximate randomization and permutation tests. An approximate <a href="randomization_test" title="wikilink">randomization test</a> is based on a specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice – or more frequently—for the efficiency of not having to track which permutations have already been selected).</p>
<h3 id="artificial-intelligence-for-games">Artificial intelligence for games</h3>

<p>Monte Carlo methods have been developed into a technique called <a href="Monte-Carlo_tree_search" title="wikilink">Monte-Carlo tree search</a> that is useful for searching for the best move in a game. Possible moves are organized in a <a href="search_tree" title="wikilink">search tree</a> and a large number of random simulations are used to estimate the long-term potential of each move. A black box simulator represents the opponent's moves.<a class="footnoteRef" href="#fn69" id="fnref69"><sup>69</sup></a></p>

<p>The Monte Carlo Tree Search (MCTS) method has four steps:<a class="footnoteRef" href="#fn70" id="fnref70"><sup>70</sup></a></p>
<ol>
<li>Starting at root node of the tree, select optimal child nodes until a leaf node is reached.</li>
<li>Expand the leaf node and choose one of its children.</li>
<li>Play a simulated game starting with that node.</li>
<li>Use the results of that simulated game to update the node and its ancestors.</li>
</ol>

<p>The net effect, over the course of many simulated games, is that the value of a node representing a move will go up or down, hopefully corresponding to whether or not that node represents a good move.</p>

<p>Monte Carlo Tree Search has been used successfully to play games such as <a href="Go_(game)" title="wikilink">Go</a>,<a class="footnoteRef" href="#fn71" id="fnref71"><sup>71</sup></a> <a class="uri" href="Tantrix" title="wikilink">Tantrix</a>,<a class="footnoteRef" href="#fn72" id="fnref72"><sup>72</sup></a> <a href="Battleship_(game)" title="wikilink">Battleship</a>,<a class="footnoteRef" href="#fn73" id="fnref73"><sup>73</sup></a> <a class="uri" href="Havannah" title="wikilink">Havannah</a>,<a class="footnoteRef" href="#fn74" id="fnref74"><sup>74</sup></a> and <a class="uri" href="Arimaa" title="wikilink">Arimaa</a>.<a class="footnoteRef" href="#fn75" id="fnref75"><sup>75</sup></a> </p>
<h3 id="design-and-visuals">Design and visuals</h3>

<p>Monte Carlo methods are also efficient in solving coupled integral differential equations of radiation fields and energy transport, and thus these methods have been used in <a href="global_illumination" title="wikilink">global illumination</a> computations that produce photo-realistic images of virtual 3D models, with applications in <a href="video_game" title="wikilink">video games</a>, <a class="uri" href="architecture" title="wikilink">architecture</a>, <a class="uri" href="design" title="wikilink">design</a>, computer generated <a href="film" title="wikilink">films</a>, and cinematic special effects.<a class="footnoteRef" href="#fn76" id="fnref76"><sup>76</sup></a></p>
<h3 id="finance-and-business">Finance and business</h3>

<p><a href="Monte_Carlo_methods_in_finance" title="wikilink">Monte Carlo methods in finance</a> are often used to <a href="Corporate_finance#Quantifying_uncertainty" title="wikilink">evaluate investments in projects</a> at a business unit or corporate level, or to evaluate <a href="derivative_(finance)" title="wikilink">financial derivatives</a>. They can be used to model <a href="project_management" title="wikilink">project schedules</a>, where simulations aggregate estimates for worst-case, best-case, and most likely durations for each task to determine outcomes for the overall project. Monte Carlo methods are also used in option pricing, default risk analysis.<a class="footnoteRef" href="#fn77" id="fnref77"><sup>77</sup></a><a class="footnoteRef" href="#fn78" id="fnref78"><sup>78</sup></a></p>
<h2 id="use-in-mathematics">Use in mathematics</h2>

<p>In general, Monte Carlo methods are used in mathematics to solve various problems by generating suitable random numbers (see also <a href="Random_number_generation" title="wikilink">Random number generation</a>) and observing that fraction of the numbers that obeys some property or properties. The method is useful for obtaining numerical solutions to problems too complicated to solve analytically. The most common application of the Monte Carlo method is Monte Carlo integration.</p>
<h3 id="integration">Integration</h3>

<p> </p>

<p>Deterministic <a href="numerical_integration" title="wikilink">numerical integration</a> algorithms work well in a small number of dimensions, but encounter two problems when the functions have many variables. First, the number of function evaluations needed increases rapidly with the number of dimensions. For example, if 10 evaluations provide adequate accuracy in one dimension, then <a href="googol" title="wikilink">10<sup>100</sup></a> points are needed for 100 dimensions—far too many to be computed. This is called the <a href="curse_of_dimensionality" title="wikilink">curse of dimensionality</a>. Second, the boundary of a multidimensional region may be very complicated, so it may not be feasible to reduce the problem to a series of nested one-dimensional integrals.<a class="footnoteRef" href="#fn79" id="fnref79"><sup>79</sup></a> 100 <a href="dimension" title="wikilink">dimensions</a> is by no means unusual, since in many physical problems, a "dimension" is equivalent to a <a href="degrees_of_freedom_(physics_and_chemistry)" title="wikilink">degree of freedom</a>.</p>

<p>Monte Carlo methods provide a way out of this exponential increase in computation time. As long as the function in question is reasonably <a class="uri" href="well-behaved" title="wikilink">well-behaved</a>, it can be estimated by randomly selecting points in 100-dimensional space, and taking some kind of average of the function values at these points. By the <a href="central_limit_theorem" title="wikilink">central limit theorem</a>, this method displays 

<math display="inline" id="Monte_Carlo_method:6">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>/</mo>
   <msqrt>
    <mi>N</mi>
   </msqrt>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <cn type="integer">1</cn>
    <apply>
     <root></root>
     <ci>N</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \scriptstyle 1/\sqrt{N}
  </annotation>
 </semantics>
</math>

 convergence—i.e., quadrupling the number of sampled points halves the error, regardless of the number of dimensions.<a class="footnoteRef" href="#fn80" id="fnref80"><sup>80</sup></a></p>

<p>A refinement of this method, known as <a href="importance_sampling" title="wikilink">importance sampling</a> in statistics, involves sampling the points randomly, but more frequently where the integrand is large. To do this precisely one would have to already know the integral, but one can approximate the integral by an integral of a similar function or use adaptive routines such as <a href="stratified_sampling" title="wikilink">stratified sampling</a>, <a href="Monte_Carlo_integration#Recursive_stratified_sampling" title="wikilink">recursive stratified sampling</a>, adaptive umbrella sampling<a class="footnoteRef" href="#fn81" id="fnref81"><sup>81</sup></a><a class="footnoteRef" href="#fn82" id="fnref82"><sup>82</sup></a> or the <a href="VEGAS_algorithm" title="wikilink">VEGAS algorithm</a>.</p>

<p>A similar approach, the <a href="quasi-Monte_Carlo_method" title="wikilink">quasi-Monte Carlo method</a>, uses <a href="low-discrepancy_sequence" title="wikilink">low-discrepancy sequences</a>. These sequences "fill" the area better and sample the most important points more frequently, so quasi-Monte Carlo methods can often converge on the integral more quickly.</p>

<p>Another class of methods for sampling points in a volume is to simulate random walks over it (<a href="Markov_chain_Monte_Carlo" title="wikilink">Markov chain Monte Carlo</a>). Such methods include the <a href="Metropolis-Hastings_algorithm" title="wikilink">Metropolis-Hastings algorithm</a>, <a href="Gibbs_sampling" title="wikilink">Gibbs sampling</a>, <a href="Wang_and_Landau_algorithm" title="wikilink">Wang and Landau algorithm</a>, and interacting type MCMC methodologies such as the <a href="Particle_filter" title="wikilink">Sequential Monte Carlo</a> samplers.<a class="footnoteRef" href="#fn83" id="fnref83"><sup>83</sup></a></p>
<h3 id="simulation-and-optimization">Simulation and optimization</h3>

<p>Another powerful and very popular application for random numbers in numerical simulation is in <a href="Optimization_(mathematics)" title="wikilink">numerical optimization</a>. The problem is to minimize (or maximize) functions of some vector that often has a large number of dimensions. Many problems can be phrased in this way: for example, a <a href="computer_chess" title="wikilink">computer chess</a> program could be seen as trying to find the set of, say, 10 moves that produces the best evaluation function at the end. In the <a href="traveling_salesman_problem" title="wikilink">traveling salesman problem</a> the goal is to minimize distance traveled. There are also applications to engineering design, such as <a href="multidisciplinary_design_optimization" title="wikilink">multidisciplinary design optimization</a>. It has been applied with <a href="quasi-one-dimensional_models" title="wikilink">quasi-one-dimensional models</a> to solve particle dynamics problems by efficiently exploring large configuration space.</p>

<p>The <a href="traveling_salesman_problem" title="wikilink">traveling salesman problem</a> is what is called a conventional optimization problem. That is, all the facts (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. However, let's assume that instead of wanting to minimize the total distance traveled to visit each desired destination, we wanted to minimize the total time needed to reach each destination. This goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account.</p>
<h3 id="inverse-problems">Inverse problems</h3>

<p>Probabilistic formulation of <a href="inverse_problem" title="wikilink">inverse problems</a> leads to the definition of a <a href="probability_distribution" title="wikilink">probability distribution</a> in the model space. This probability distribution combines <a href="prior_probability" title="wikilink">prior</a> information with new information obtained by measuring some observable parameters (data). As, in the general case, the theory linking data with model parameters is nonlinear, the posterior probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.).</p>

<p>When analyzing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the <em>a priori</em> distribution is available.</p>

<p>The best-known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex <em>a priori</em> information and data with an arbitrary noise distribution.<a class="footnoteRef" href="#fn84" id="fnref84"><sup>84</sup></a><a class="footnoteRef" href="#fn85" id="fnref85"><sup>85</sup></a></p>
<h3 id="petroleum-reservoir-management">Petroleum reservoir management</h3>

<p>Monte Carlo methods are very popular in hydrocarbon reservoir management in the context of nonlinear inverse problems. This includes generating computational models of oil and gas reservoirs for consistency with observed production data. For the goal of decision making and uncertainty assessment, Monte Carlo methods are used for generating multiple geological realizations.<a class="footnoteRef" href="#fn86" id="fnref86"><sup>86</sup></a></p>
<h2 id="in-popular-culture">In popular culture</h2>
<ul>
<li><em>The Monte Carlo Method</em>, the 1998 album by the southern California indie rock band <a href="Nothing_Painted_Blue." title="wikilink">Nothing Painted Blue.</a> (Scat. 1998).</li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Auxiliary_field_Monte_Carlo" title="wikilink">Auxiliary field Monte Carlo</a></li>
<li><a href="Biology_Monte_Carlo_method" title="wikilink">Biology Monte Carlo method</a></li>
<li><a href="Comparison_of_risk_analysis_Microsoft_Excel_add-ins" title="wikilink">Comparison of risk analysis Microsoft Excel add-ins</a></li>
<li><a href="Direct_simulation_Monte_Carlo" title="wikilink">Direct simulation Monte Carlo</a></li>
<li><a href="Dynamic_Monte_Carlo_method" title="wikilink">Dynamic Monte Carlo method</a></li>
<li><a href="Kinetic_Monte_Carlo" title="wikilink">Kinetic Monte Carlo</a></li>
<li><a href="Mean_field_particle_methods" title="wikilink">Mean field particle methods</a></li>
<li><a href="Particle_filter" title="wikilink">Particle filter</a></li>
<li><a href="List_of_software_for_Monte_Carlo_molecular_modeling" title="wikilink">List of software for Monte Carlo molecular modeling</a></li>
<li><a href="Monte_Carlo_method_for_photon_transport" title="wikilink">Monte Carlo method for photon transport</a></li>
<li><a href="Monte_Carlo_methods_for_electron_transport" title="wikilink">Monte Carlo methods for electron transport</a></li>
<li><a href="Morris_method" title="wikilink">Morris method</a></li>
<li><a href="Genetic_algorithms" title="wikilink">Genetic algorithms</a></li>
<li><a href="Quasi-Monte_Carlo_method" title="wikilink">Quasi-Monte Carlo method</a></li>
<li><a href="Sobol_sequence" title="wikilink">Sobol sequence</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Monte_Carlo_methods" title="wikilink"> </a> <a href="Category:Numerical_analysis" title="wikilink">Category:Numerical analysis</a> <a href="Category:Statistical_mechanics" title="wikilink">Category:Statistical mechanics</a> <a href="Category:Computational_physics" title="wikilink">Category:Computational physics</a> <a href="Category:Sampling_techniques" title="wikilink">Category:Sampling techniques</a> <a href="Category:Statistical_approximations" title="wikilink">Category:Statistical approximations</a> <a href="Category:Stochastic_simulation" title="wikilink">Category:Stochastic simulation</a> <a href="Category:Probabilistic_complexity_theory" title="wikilink">Category:Probabilistic complexity theory</a> <a href="Category:Risk_analysis" title="wikilink">Category:Risk analysis</a> <a href="Category:Risk_management" title="wikilink">Category:Risk management</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18"><a href="#fnref18">↩</a></li>
<li id="fn19"><a href="#fnref19">↩</a></li>
<li id="fn20"><a href="#fnref20">↩</a></li>
<li id="fn21"><a href="#fnref21">↩</a></li>
<li id="fn22"><a href="#fnref22">↩</a></li>
<li id="fn23"><a href="#fnref23">↩</a></li>
<li id="fn24"></li>
<li id="fn25"><a href="#fnref25">↩</a></li>
<li id="fn26"><a href="#fnref26">↩</a></li>
<li id="fn27"><a href="#fnref27">↩</a></li>
<li id="fn28"><a href="#fnref28">↩</a></li>
<li id="fn29"><a href="#fnref29">↩</a></li>
<li id="fn30">P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : An unified framework for particle solutions<br/>
LAAS-CNRS, Toulouse, Research Report no. 91137, DRET-DIGILOG- LAAS/CNRS contract, April (1991).<a href="#fnref30">↩</a></li>
<li id="fn31">P. Del Moral, G. Rigal, and G. Salut. Nonlinear and non Gaussian particle filters applied to inertial platform repositioning.<br/>
LAAS-CNRS, Toulouse, Research Report no. 92207, STCAN/DIGILOG-LAAS/CNRS Convention STCAN no. A.91.77.013, (94p.) September (1991).<a href="#fnref31">↩</a></li>
<li id="fn32">P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : Particle resolution in filtering and estimation. Experimental results.<br/>
Convention DRET no. 89.34.553.00.470.75.01, Research report no.2 (54p.), January (1992).<a href="#fnref32">↩</a></li>
<li id="fn33">P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : Particle resolution in filtering and estimation. Theoretical results<br/>
Convention DRET no. 89.34.553.00.470.75.01, Research report no.3 (123p.), October (1992).<a href="#fnref33">↩</a></li>
<li id="fn34">P. Del Moral, J.-Ch. Noyer, G. Rigal, and G. Salut. Particle filters in radar signal processing : detection, estimation and air targets recognition.<br/>
LAAS-CNRS, Toulouse, Research report no. 92495, December (1992).<a href="#fnref34">↩</a></li>
<li id="fn35">P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : Particle resolution in filtering and estimation.<br/>
Studies on: Filtering, optimal control, and maximum likelihood estimation. Convention DRET no. 89.34.553.00.470.75.01. Research report no.4 (210p.), January (1993).<a href="#fnref35">↩</a></li>
<li id="fn36"></li>
<li id="fn37"><a href="#fnref37">↩</a></li>
<li id="fn38"><a href="#fnref38">↩</a></li>
<li id="fn39"><a href="#fnref39">↩</a></li>
<li id="fn40"><a href="#fnref40">↩</a></li>
<li id="fn41"><a href="#fnref41">↩</a></li>
<li id="fn42"></li>
<li id="fn43"><a href="#fnref43">↩</a></li>
<li id="fn44"><a href="#fnref44">↩</a></li>
<li id="fn45"><a href="#fnref45">↩</a></li>
<li id="fn46"><a href="#fnref46">↩</a></li>
<li id="fn47"></li>
<li id="fn48"><a href="#fnref48">↩</a></li>
<li id="fn49"></li>
<li id="fn50"><a href="#fnref50">↩</a></li>
<li id="fn51"><a href="#fnref51">↩</a></li>
<li id="fn52"><a href="#fnref52">↩</a></li>
<li id="fn53"><a href="#fnref53">↩</a></li>
<li id="fn54"><a href="#fnref54">↩</a></li>
<li id="fn55"></li>
<li id="fn56"><a href="#fnref56">↩</a></li>
<li id="fn57"></li>
<li id="fn58"></li>
<li id="fn59"></li>
<li id="fn60"><a href="#fnref60">↩</a></li>
<li id="fn61"><a href="#fnref61">↩</a></li>
<li id="fn62"><a href="#fnref62">↩</a></li>
<li id="fn63"><a href="#fnref63">↩</a></li>
<li id="fn64">G. A. Bird, Molecular Gas Dynamics, Clarendon, Oxford (1976)<a href="#fnref64">↩</a></li>
<li id="fn65"><a href="#fnref65">↩</a></li>
<li id="fn66">,<a href="#fnref66">↩</a></li>
<li id="fn67"><a href="#fnref67">↩</a></li>
<li id="fn68"><a href="#fnref68">↩</a></li>
<li id="fn69"><a class="uri" href="http://sander.landofsand.com/publications/Monte-Carlo_Tree_Search_-_A_New_Framework_for_Game_AI.pdf">http://sander.landofsand.com/publications/Monte-Carlo_Tree_Search_-_A_New_Framework_for_Game_AI.pdf</a><a href="#fnref69">↩</a></li>
<li id="fn70"><a href="http://mcts.ai/about/index.html">Monte Carlo Tree Search - About</a><a href="#fnref70">↩</a></li>
<li id="fn71"><a href="http://link.springer.com/chapter/10.1007/978-3-540-87608-3_6">Parallel Monte-Carlo Tree Search - Springer</a><a href="#fnref71">↩</a></li>
<li id="fn72"><a href="http://www.tantrix.com:4321/Tantrix/TRobot/MCTS%20Final%20Report.pdf">http://www.tantrix.com:4321/Tantrix/TRobot/MCTS%20Final%20Report.pdf</a><a href="#fnref72">↩</a></li>
<li id="fn73"><a class="uri" href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Publications_files/pomcp.pdf">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Publications_files/pomcp.pdf</a><a href="#fnref73">↩</a></li>
<li id="fn74"><a href="http://link.springer.com/chapter/10.1007/978-3-642-17928-0_10">Improving Monte–Carlo Tree Search in Havannah - Springer</a><a href="#fnref74">↩</a></li>
<li id="fn75"><a class="uri" href="http://www.arimaa.com/arimaa/papers/ThomasJakl/bc-thesis.pdf">http://www.arimaa.com/arimaa/papers/ThomasJakl/bc-thesis.pdf</a><a href="#fnref75">↩</a></li>
<li id="fn76"><a href="#fnref76">↩</a></li>
<li id="fn77"><a href="#fnref77">↩</a></li>
<li id="fn78"><a href="#fnref78">↩</a></li>
<li id="fn79"><a href="#fnref79">↩</a></li>
<li id="fn80"></li>
<li id="fn81"><a href="#fnref81">↩</a></li>
<li id="fn82"><a href="#fnref82">↩</a></li>
<li id="fn83"><a href="#fnref83">↩</a></li>
<li id="fn84"><a href="#fnref84">↩</a></li>
<li id="fn85"><a href="#fnref85">↩</a></li>
<li id="fn86">Shirangi, M. G., History matching production data and uncertainty assessment with an efficient TSVD parameterization algorithm, Journal of Petroleum Science and Engineering, <a class="uri" href="http://www.sciencedirect.com/science/article/pii/S0920410513003227">http://www.sciencedirect.com/science/article/pii/S0920410513003227</a><a href="#fnref86">↩</a></li>
</ol>
</section>
</body>
</html>
