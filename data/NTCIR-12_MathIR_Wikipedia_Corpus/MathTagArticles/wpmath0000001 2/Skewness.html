<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="970">Skewness</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Skewness</h1>
<hr/>
<figure><b>(Figure)</b>
<figcaption>|Example distribution with non-zero (positive) skewness. These data are from experiments on wheat grass growth.</figcaption>
</figure>
<p>In <a href="probability_theory" title="wikilink">probability theory</a> and <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>skewness</strong> is a measure of the asymmetry of the <a href="probability_distribution" title="wikilink">probability distribution</a> of a <a href="real_number" title="wikilink">real</a>-valued <a href="random_variable" title="wikilink">random variable</a> about its mean. The skewness value can be positive or negative, or even undefined.</p>
<p>The qualitative interpretation of the skew is complicated. For a <a class="uri" href="unimodal" title="wikilink">unimodal</a> distribution, negative skew indicates that the <em>tail</em> on the left side of the probability density function is <a href="Long_tail" title="wikilink">longer</a> or <a href="Fat-tailed_distribution" title="wikilink">fatter</a> than the right side – it does not distinguish these shapes. Conversely, positive skew indicates that the tail on the right side is longer or fatter than the left side. In cases where one tail is long but the other tail is fat, skewness does not obey a simple rule. For example, a zero value indicates that the tails on both sides of the mean balance out, which is the case for a symmetric distribution, but is also true for an asymmetric distribution where the asymmetries even out, such as one tail being long but thin, and the other being short but fat. Further, in multimodal distributions and discrete distributions, skewness is also difficult to interpret. Importantly, the skewness does not determine the <a href="#Relationship_of_mean_and_median" title="wikilink">relationship of mean and median</a>.</p>
<h2 id="introduction">Introduction</h2>
<p>Consider the two distributions in the figure just below. Within each graph, the bars on the right side of the distribution taper differently than the bars on the left side. These tapering sides are called <em>tails</em>, and they provide a visual means for determining which of the two kinds of skewness a distribution has:</p>
<ol>
<li><em></em>: The left tail is longer; the mass of the distribution is concentrated on the right of the figure. The distribution is said to be <em>left-skewed</em>, <em>left-tailed</em>, or <em>skewed to the left</em>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></li>
<li><em></em>: The right tail is longer; the mass of the distribution is concentrated on the left of the figure. The distribution is said to be <em>right-skewed</em>, <em>right-tailed</em>, or <em>skewed to the right</em>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></li>
</ol>
<figure><b>(Figure)</b>
<figcaption>Negative and positive skew diagrams (English).svg</figcaption>
</figure>
<p>Skewness in a data series may be observed not only graphically but by simple inspection of the values. For instance, consider the numeric sequence (49, 50, 51), whose values are evenly distributed around a central value of (50). We can transform this sequence into a negatively skewed distribution by adding a value far below the mean, as in e.g. (40, 49, 50, 51). Similarly, we can make the sequence positively skewed by adding a value far above the mean, as in e.g. (49, 50, 51, 60).</p>
<h2 id="relationship-of-mean-and-median">Relationship of mean and median</h2>
<p>The skewness is not strictly connected with the relationship between the mean and median: a distribution with negative skew can have the mean greater than or less than the median, and likewise for positive skew.</p>
<p>In the older notion of <a href="nonparametric_skew" title="wikilink">nonparametric skew</a>, defined as <span class="LaTeX">$(\mu - \nu)/\sigma,$</span> where <em>µ</em> is the <a class="uri" href="mean" title="wikilink">mean</a>, <em>ν</em> is the <a class="uri" href="median" title="wikilink">median</a>, and <em>σ</em> is the <a href="standard_deviation" title="wikilink">standard deviation</a>, the skewness is defined in terms of this relationship: positive/right nonparametric skew means the mean is greater than (to the right of) the median, while negative/left nonparametric skew means the mean is less than (to the left of) the median. However, the modern definition of skewness and the traditional nonparametric definition do not in general have the same sign: while they agree for some families of distributions, they differ in general, and conflating them is misleading.</p>
<p>If the distribution is <a href="Symmetric_probability_distribution" title="wikilink">symmetric</a> then the mean is equal to the median and the distribution will have zero skewness.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> If, in addition, the distribution is <a href="Unimodal_distribution" title="wikilink">unimodal</a>, then the <a class="uri" href="mean" title="wikilink">mean</a> = <a class="uri" href="median" title="wikilink">median</a> = <a href="Mode_(statistics)" title="wikilink">mode</a>. This is the case of a coin toss or the series 1,2,3,4,... Note, however, that the converse is not true in general, i.e. zero skewness does not imply that the mean is equal to the median.</p>
<p>"<em>Many textbooks,</em>" a 2005 article points out, "<em>teach a rule of thumb stating that the mean is right of the median under right skew, and left of the median under left skew. This rule fails with surprising frequency. It can fail in <a href="multimodal_distribution" title="wikilink">multimodal distributions</a>, or in distributions where one tail is <a href="Long_tail" title="wikilink">long</a> but the other is <a href="Heavy-tailed_distribution" title="wikilink">heavy</a>. Most commonly, though, the rule fails in discrete distributions where the areas to the left and right of the median are not equal. Such distributions not only contradict the textbook relationship between mean, median, and skew, they also contradict the textbook interpretation of the median.</em>"<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h2 id="definition">Definition</h2>
<h3 id="pearsons-moment-coefficient-of-skewness">Pearson's moment coefficient of skewness</h3>
<p>The skewness of a random variable <em>X</em> is the <strong>moment coefficient of skewness</strong>.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> It is sometimes referred to as <strong>Pearson's moment coefficient of skewness</strong>,<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> not to be confused with Pearson's other skewness statistics (see below). It is the third <a href="standardized_moment" title="wikilink">standardized moment</a>.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> It is denoted <em>γ</em><sub>1</sub> and defined as</p>
<p><span class="LaTeX">$$\gamma_1 = \operatorname{E}\left[\left(\frac{X-\mu}{\sigma}\right)^3 \right] 
             = \frac{\mu_3}{\sigma^3} 
             = \frac{\operatorname{E}\left[(X-\mu)^3\right]}{\ \ \ ( \operatorname{E}\left[ (X-\mu)^2 \right] )^{3/2}}
             = \frac{\kappa_3}{\kappa_2^{3/2}},$$</span> where <em>μ</em><sub>3</sub> is the third <a href="central_moment" title="wikilink">central moment</a>, <em>μ</em> is the mean, <em>σ</em> is the <a href="standard_deviation" title="wikilink">standard deviation</a>, and <em>E</em> is the <a href="expected_value" title="wikilink">expectation operator</a>. The last equality expresses skewness in terms of the ratio of the third <a class="uri" href="cumulant" title="wikilink">cumulant</a> <em>κ</em><sub>3</sub> and the 1.5th power of the second cumulant <em>κ</em><sub>2</sub>. This is analogous to the definition of <a class="uri" href="kurtosis" title="wikilink">kurtosis</a> as the fourth cumulant normalized by the square of the second cumulant.</p>
<p>The skewness is also sometimes denoted Skew[<em>X</em>].</p>
<p>The formula expressing skewness in terms of the non-central moment E[<em>X</em><sup>3</sup>] can be expressed by expanding the previous formula,</p>
<p><span class="LaTeX">$$\begin{align}
    \gamma_1
     &= \operatorname{E}\left[\left(\frac{X-\mu}{\sigma}\right)^3 \right] \\
     & = \frac{\operatorname{E}[X^3] - 3\mu\operatorname E[X^2] + 3\mu^2\operatorname E[X] - \mu^3}{\sigma^3}\\
     &= \frac{\operatorname{E}[X^3] - 3\mu(\operatorname E[X^2] -\mu\operatorname E[X]) - \mu^3}{\sigma^3}\\
     &= \frac{\operatorname{E}[X^3] - 3\mu\sigma^2 - \mu^3}{\sigma^3}. 
\end{align}$$</span></p>
<h3 id="properties">Properties</h3>
<p>Skewness can be infinite, as when</p>
<p><span class="LaTeX">$$\Pr \left[ X > x \right]=x^{-3}\mbox{ for }x>1,\ \Pr[X<1]=0$$</span> or undefined, as when</p>
<p><span class="LaTeX">$$\Pr[X<x]=(1-x)^{-3} 2\mbox{="" and="" for="" negative="" }\pr[x="" }x\mbox{="">x]=(1+x)^{-3}/2\mbox{ for positive }x.</x]=(1-x)^{-3}>$$</span> In this latter example, the third cumulant is undefined. One can also have distributions such as</p>
<p><span class="LaTeX">$$\Pr \left[ X > x \right]=x^{-2}\mbox{ for }x>1,\ \Pr[X<1]=0$$</span> where both the second and third cumulants are infinite, so the skewness is again undefined.</p>
<p>If <em>Y</em> is the sum of <em>n</em> <a href="independent_and_identically_distributed_random_variables" title="wikilink">independent and identically distributed random variables</a>, all with the distribution of <em>X</em>, then the third cumulant of <em>Y</em> is <em>n</em> times that of <em>X</em> and the second cumulant of <em>Y</em> is <em>n</em> times that of <em>X</em>, so <span class="LaTeX">$\mbox{Skew}[Y] = \mbox{Skew}[X]/\sqrt{n}$</span>. This shows that the skewness of the sum is smaller, as it approaches a Gaussian distribution in accordance with the <a href="central_limit_theorem" title="wikilink">central limit theorem</a>.</p>
<h3 id="sample-skewness">Sample skewness</h3>
<p>For a sample of <em>n</em> values, a natural <a href="Method_of_moments_(statistics)" title="wikilink">method of moments</a> estimator of the population skewness is</p>
<p><span class="LaTeX">$$b_1 = \frac{m_3}{s^3} 
        = \frac{\tfrac{1}{n} \sum_{i=1}^n (x_i-\overline{x})^3}{\left[\tfrac{1}{n-1} \sum_{i=1}^n (x_i-\overline{x})^2\right]^{3/2}}\ ,$$</span> where <span class="LaTeX">$\scriptstyle\overline{x}$</span> is the <a href="sample_mean" title="wikilink">sample mean</a>, <em>s</em> is the <a href="Standard_deviation#Corrected_sample_standard_deviation" title="wikilink">sample standard deviation</a>, and the numerator <em>m</em><sub>3</sub> is the sample third central <a href="Moment_(mathematics)" title="wikilink">moment</a>.</p>
<p>Another common definition of the <em>sample skewness</em> is<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<p><span class="LaTeX">$$G_1 = \frac{k_3}{k_2^{3/2}} = \frac{n^2}{(n-1)(n-2)}\; \frac{m_3}{s^3} ,$$</span></p>
<p>where <span class="LaTeX">$k_3$</span> is the unique symmetric unbiased estimator of the third <a class="uri" href="cumulant" title="wikilink">cumulant</a> and <span class="LaTeX">$k_2 = s^2$</span> is the symmetric unbiased estimator of the second cumulant (i.e. the variance).</p>
<p>In general, the ratios <span class="LaTeX">$b_1$</span> and <span class="LaTeX">$G_1$</span> are both <a href="Bias_of_an_estimator" title="wikilink">biased estimators</a> of the population skewness <span class="LaTeX">$\gamma_1$</span>; their expected values can even have the opposite sign from the true skewness. (For instance a mixed distribution consisting of very thin Gaussians centred at −99, 0.5, and 2 with weights 0.01, 0.66, and 0.33 has a skewness of about −9.77, but in a sample of 3, <span class="LaTeX">$G_1$</span> has an expected value of about 0.32, since usually all three samples are in the positive-valued part of the distribution, which is skewed the other way.) Nevertheless, <span class="LaTeX">$b_1$</span> and <span class="LaTeX">$G_1$</span> each have obviously the correct expected value of zero for any symmetric distribution with a finite third moment, including a <a href="normal_distribution" title="wikilink">normal distribution</a>.</p>
<p>The variance of the skewness of a random sample of size <em>n</em> from a normal distribution is<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<p><span class="LaTeX">$$\operatorname{var}(G_1)= \frac{6n ( n - 1 )}{ ( n - 2 )( n + 1 )( n + 3 ) } .$$</span></p>
<p>An approximate alternative is 6/<em>n</em> but this is inaccurate for small samples.</p>
<p>In normal samples, <span class="LaTeX">$b_1$</span> has the smaller variance of the two estimators, with</p>
<p><span class="LaTeX">$$\operatorname{var}(b_1) < \operatorname{var} \left( \frac{m_3}{m_2^{3/2}} \right) < \operatorname{var}(G_1),$$</span></p>
<p>where <em>m</em><sub>2</sub> in the denominator is the (biased) sample second central moment.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>
<p>The adjusted Fisher–Pearson standardized moment coefficient <span class="LaTeX">$G_1$</span> is the version found in <a href="Microsoft_Excel" title="wikilink">Excel</a> and several statistical packages including <a class="uri" href="Minitab" title="wikilink">Minitab</a>, <a href="SAS_(software)" title="wikilink">SAS</a> and <a class="uri" href="SPSS" title="wikilink">SPSS</a>.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>
<h2 id="applications">Applications</h2>
<p>Skewness has benefits in many areas. Many models assume normal distribution; i.e., data are symmetric about the mean. The normal distribution has a skewness of zero. But in reality, data points may not be perfectly symmetric. So, an understanding of the skewness of the dataset indicates whether deviations from the mean are going to be positive or negative.</p>
<p><a href="D'Agostino's_K-squared_test" title="wikilink">D'Agostino's K-squared test</a> is a <a class="uri" href="goodness-of-fit" title="wikilink">goodness-of-fit</a> <a href="normality_test" title="wikilink">normality test</a> based on sample skewness and sample kurtosis.</p>
<h2 id="other-measures-of-skewness">Other measures of skewness</h2>
<figure><b>(Figure)</b>
<figcaption>Comparison of <a class="uri" href="mean" title="wikilink">mean</a>, <a class="uri" href="median" title="wikilink">median</a> and <a href="mode_(statistics)" title="wikilink">mode</a> of two <a href="log-normal_distribution" title="wikilink">log-normal distributions</a> with different skewness.</figcaption>
</figure>
<p>Other measures of skewness have been used, including simpler calculations suggested by <a href="Karl_Pearson" title="wikilink">Karl Pearson</a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> (not to be confused with Pearson's moment coefficient of skewness, see above). These other measures are:</p>
<h3 id="pearsons-first-skewness-coefficient-mode-skewness">Pearson's first skewness coefficient (mode skewness)</h3>
<p>The Pearson mode skewness,<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> or first skewness coefficient, is defined by</p>
<ul>
<li>(<a class="uri" href="mean" title="wikilink">mean</a> − <a href="Mode_(statistics)" title="wikilink">mode</a>) / <a href="standard_deviation" title="wikilink">standard deviation</a>,</li>
</ul>
<h3 id="pearsons-second-skewness-coefficient-median-skewness">Pearson's second skewness coefficient (median skewness)</h3>
<p>The Pearson median skewness, or second skewness coefficient,<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> is defined by</p>
<ul>
<li>3 (<a class="uri" href="mean" title="wikilink">mean</a> − <a class="uri" href="median" title="wikilink">median</a>) / <a href="standard_deviation" title="wikilink">standard deviation</a>.</li>
</ul>
<p>The latter is a simple multiple of the <a href="nonparametric_skew" title="wikilink">nonparametric skew</a>.</p>
<h3 id="other">Other</h3>
<p>Starting from a standard cumulant expansion around a Normal distribution, one can actually show that skewness = 6 (<a class="uri" href="mean" title="wikilink">mean</a> − <a class="uri" href="median" title="wikilink">median</a>) / <a href="standard_deviation" title="wikilink">standard deviation</a> ( 1 + kurtosis / 8) + O(skewness<sup>2</sup>). One should keep in mind that above given equalities often don't hold even approximately and these empirical formulas are abandoned nowadays. There is no guarantee that these will be the same sign as each other or as the ordinary definition of skewness.</p>
<h3 id="quantile-based-measures">Quantile-based measures</h3>
<p>A skewness function</p>
<p><span class="LaTeX">$$\gamma( u )= \frac{ F^{ -1 }( u ) +F^{ -1 }( 1 - u )-2F^{ -1 }( 1 / 2 ) }{F^{ -1 }( u ) -F^{ -1 }( 1 - u ) }$$</span></p>
<p>can be defined,<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a><a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> where <em>F</em> is the <a href="cumulative_distribution_function" title="wikilink">cumulative distribution function</a>. This leads to a corresponding overall measure of skewness<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> defined as the <a class="uri" href="supremum" title="wikilink">supremum</a> of this over the range 1/2 ≤ <em>u</em>  The function <em>γ</em>(<em>u</em>) satisfies −1 ≤ <em>γ</em>(<em>u</em>) ≤ 1 and is well defined without requiring the existence of any moments of the distribution.<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a></p>
<p>Galton's measure of skewness<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> is γ(<em>u</em>) evaluated at <em>u</em> = 3/4. Other names for this same quantity are the Bowley Skewness,<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> the Yule–Kendall index<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> and the quartile skewness.</p>
<p>Kelley's measure of skewness uses u = 0.1.</p>
<h3 id="l-moments">L-moments</h3>
<p>Use of <a href="L-moment" title="wikilink">L-moments</a> in place of moments provides a measure of skewness known as the L-skewness.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a></p>
<h3 id="distance-skewness">Distance skewness</h3>
<p>A value of skewness equal to zero does not imply that the probability distribution is symmetric. Thus there is a need for another measure of asymmetry which has this property: such a measure was introduced in 2000.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a> It is called <strong>distance skewness</strong> and denoted by dSkew. If <em>X</em> is a random variable which takes values in the d-dimensional Euclidean space, X has finite expectation, X' is an independent identically distributed copy of X and <span class="LaTeX">$\|\cdot\|$</span> denotes the norm in the Euclidean space then a simple <em>measure of asymmetry</em> is</p>
<p><span class="LaTeX">$$\operatorname{dSkew}(X) := 1 - \frac{\operatorname{E}\|X-X'\|}{\operatorname{E}\|X+X'\|} \text{ if } \Pr(X=0)\ne 1$$</span></p>
<p>and dSkew(<em>X</em>) := 0 for <em>X</em> =  (with probability 1). Distance skewness is always between 0 and 1, equals 0 if and only if <em>X</em> is diagonally symmetric (<em>X</em> and −<em>X</em> has the same probability distribution) and equals 1 if and only if X is a nonzero constant with probability one.<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> Thus there is a simple consistent <a href="statistical_test" title="wikilink">statistical test</a> of diagonal symmetry based on the <strong>sample distance skewness</strong>:</p>
<p><span class="LaTeX">$$\operatorname{dSkew}_n(X):= 1 - \frac{\sum_{i,j} \|x_i-x_j\| }{\sum_{i,j} \|x_i+x_j\|}.$$</span></p>
<h3 id="groeneveld-meedens-coefficient">Groeneveld & Meeden’s coefficient</h3>
<p>Groeneveld & Meeden have suggested, as an alternative measure of skewness,<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a></p>
<p><span class="LaTeX">$$\mathrm{skew}(X) = \frac{( \mu - \nu ) }{ E( | X - \nu | ) }$$</span></p>
<p>where <em>μ</em> is the mean, <em>ν</em> is the median, |…| is the <a href="absolute_value" title="wikilink">absolute value</a> and <em>E</em>() is the expectation operator.</p>
<h3 id="medcouple">Medcouple</h3>
<p>The <a class="uri" href="medcouple" title="wikilink">medcouple</a> is a scale-invariant robust measure of skewness, with a <a href="breakdown_point" title="wikilink">breakdown point</a> of 25%. It is the <a class="uri" href="median" title="wikilink">median</a> of the values of the kernel function</p>
<p><span class="LaTeX">$$h(x_i, x_j) = \frac{ (x_i - x_m) - (x_m - x_j)}{x_i - x_j}$$</span> taken over all couples <span class="LaTeX">$(x_i, x_j)$</span> such that <span class="LaTeX">$x_i \geq x_m \geq x_j$</span>, where <span class="LaTeX">$x_m$</span> is the median of the <a href="sample_(statistics)" title="wikilink">sample</a> <span class="LaTeX">$\{x_1, x_2, \ldots, x_n\}$</span>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bragg_peak" title="wikilink">Bragg peak</a></li>
<li><a class="uri" href="Coskewness" title="wikilink">Coskewness</a></li>
<li><a href="Shape_parameter" title="wikilink">Shape parameters</a></li>
<li><a href="Skew_normal_distribution" title="wikilink">Skew normal distribution</a></li>
<li><a href="Skewness_risk" title="wikilink">Skewness risk</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li>Johnson, NL, Kotz, S, Balakrishnan N (1994) <em>Continuous Univariate Distributions, Vol 1, 2nd Edition</em> Wiley ISBN 0-471-58495-9</li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li></li>
<li><a href="http://petitjeanmichel.free.fr/itoweb.petitjean.skewness.html">An Asymmetry Coefficient for Multivariate Distributions</a> by Michel Petitjean</li>
<li>[<a class="uri" href="http://repositories.cdlib.org/cgi/viewcontent.cgi?article=1017&context">http://repositories.cdlib.org/cgi/viewcontent.cgi?article=1017&context</a>;=ucsdecon On More Robust Estimation of Skewness and Kurtosis] Comparison of skew estimators by Kim and White.</li>
<li><a href="http://dahoiv.net/master/index.html">Closed-skew Distributions — Simulation, Inversion and Parameter Estimation</a></li>
</ul>
<p>"</p>
<p><a href="Category:Theory_of_probability_distributions" title="wikilink">Category:Theory of probability distributions</a> <a href="Category:Statistical_deviation_and_dispersion" title="wikilink">Category:Statistical deviation and dispersion</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Susan Dean, Barbara Illowsky <a href="http://cnx.org/content/m17104/latest/">"Descriptive Statistics: Skewness and the Mean, Median, and Mode"</a>, Connexions website<a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="http://www.tc3.edu/instruct/sbrown/stat/shape.htm">"Measures of Shape: Skewness and Kurtosis"</a>, 2008–2014 by Stan Brown, Oak Road Systems<a href="#fnref5">↩</a></li>
<li id="fn6"><a href="http://www.fxsolver.com/browse/formulas/Pearson's+moment+coefficient+of+skewness">Pearson's moment coefficient of skewness</a>, FXSolver.com<a href="#fnref6">↩</a></li>
<li id="fn7"></li>
<li id="fn8"></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10">Duncan Cramer (1997) Fundamental Statistics for Social Research. Routledge. ISBN 9780415172042 (p 85)<a href="#fnref10">↩</a></li>
<li id="fn11">Kendall, M.G.; Stuart, A. (1969) <em>The Advanced Theory of Statistics, Volume 1: Distribution Theory, 3rd Edition</em>, Griffin. ISBN 0-85264-141-9 (Ex 12.9)<a href="#fnref11">↩</a></li>
<li id="fn12"></li>
<li id="fn13">Doane DP, Seward LE (2011) J Stat Educ 19 (2)<a href="#fnref13">↩</a></li>
<li id="fn14"><a href="http://www.stat.upd.edu.ph/s114%20cnotes%20fcapistrano/Chapter%2010.pdf">http://www.stat.upd.edu.ph/s114%20cnotes%20fcapistrano/Chapter%2010.pdf</a><a href="#fnref14">↩</a></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
<li id="fn17">MacGillivray (1992)<a href="#fnref17">↩</a></li>
<li id="fn18">Hinkley DV (1975) "On power transformations to symmetry", ''<a class="uri" href="Biometrika" title="wikilink">Biometrika</a>, 62, 101–111<a href="#fnref18">↩</a></li>
<li id="fn19">MacGillivray (1992)<a href="#fnref19">↩</a></li>
<li id="fn20"></li>
<li id="fn21">Johnson <em>et al</em> (1994) p 3, p 40<a href="#fnref21">↩</a></li>
<li id="fn22">Kenney JF and Keeping ES (1962) <em>Mathematics of Statistics, Pt. 1, 3rd ed.</em>, Van Nostrand, (page 102)<a href="#fnref22">↩</a></li>
<li id="fn23">Wilks DS (1995) <em>Statistical Methods in the Atmospheric Sciences</em>, p 27. Academic Press. ISBN 0-12-751965-3<a href="#fnref23">↩</a></li>
<li id="fn24"><a href="#fnref24">↩</a></li>
<li id="fn25">Szekely, G.J. (2000). "Pre-limit and post-limit theorems for statistics", In: <em>Statistics for the 21st Century</em> (eds. C. R. Rao and G. J. Szekely), Dekker, New York, pp. 411–422.<a href="#fnref25">↩</a></li>
<li id="fn26">Szekely, G. J. and Mori, T. F. (2001) "A characteristic measure of asymmetry and its application for testing diagonal symmetry", <em>Communications in Statistics – Theory and Methods</em> 30/8&9, 1633–1639.<a href="#fnref26">↩</a></li>
<li id="fn27"><a href="#fnref27">↩</a></li>
</ol>
</section>
</body>
</html>
