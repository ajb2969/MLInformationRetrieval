---------------------------------------------
# NTCIR-12 MathIR Task Wikipedia Corpus

**Created: September-October 2015**

Richard Zanibbi  
Document and Pattern Recognition Lab   
Department of Computer Science  
Rochester Institute of Technology, Rochester, NY, USA  
*[ rlaz@cs.rit.edu ] [ https://www.cs.rit.edu/~dprl ]*

---------------------------------------------


## Overview

This is the 'Wikipedia' corpus generated for the NTCIR-12 Mathematical Information Retrieval (MathIR) task being held as part of the NTCIR-12 (2016) conference in Tokyo ( http://ntcir-math.nii.ac.jp/introduction/ ).

The collection is broken up into 'math' articles containing \<math\> tags, and 'text' articles that do not. Here is a summary of the contents of the corpus:

* *MathTagArticles/*  (~10% of collection, containing \<math\> tags)
	* &nbsp;31,742 articles (.html files, stored in 16 .tar.bz2 archive files)
	* 551,675 formulae  

* *TextArticles/*  (~90% of collection, without \<math\> tags) 
	* 285,925 articles (.html files, stored in 144 .tar.bz2 archive files)
	* &nbsp;12,271 formulae (many are very small, e.g. single variables indicated by {{mvar}} ).

* **Total**
	* 317,667 articles
	* 563,946 formulae


Details of how the corpus data is stored, how to decompress archive files, the formula representation, and information about how the corpus was created are provided below. 

---
## Wikipedia Data and License

The Wikipedia articles included in this collection are being provided under a Creative Commons BY-SA license (http://creativecommons.org/licenses/by-sa/3.0/), as required by the Wikimedia foundation (https://en.wikipedia.org/wiki/Wikipedia:Copyrights#Reusers.27_rights_and_obligations). 

The complete original content of all articles in this collection may be obtained from the Aug. 5, 2015 text-only Wikipedia snapshot provided online:

	https://dumps.wikimedia.org/enwiki/20150805/enwiki-20150805-pages-articles.xml.bz2   

Wikipedia articles included in this collection are unchanged aside from alterations resulting from the automated MediaWiki to HTML conversion process. 

**Our sincerest thanks go to Wikipedia authors and the people at the Wikimedia Foundation for making snapshots publicly available.** 


---

## Archive Files (.tar.bz2)


The corpus has been split into 160 archive files holding around 2000 articles each. Each archive has a main directory where articles are provided in .html format. 

Some additional book-keeping data about the archive contents and conversion process is also included: the number of formulae located and processed with errors, counts for correctly and incorrectly converted articles, files with Math processing errors (omitted from the collection), and a log generated by the MediaWiki to HTML conversion scripts.



## Decompressing Archive Files

Articles are stored in .tar.bz2 archives containing around 2000 articles each. Archives can be expanded using the 'tar' command, e.g. on our system:

	tar jxf <fileName>.tar.bz2

will decompress the file (in bzip2 format), followed by expanding the tar archive.

Using bash shell, if you have enough space (and file system nodes) available, it is easy to write a short program to decompress archive files using a loop:

	for file in *.tar.bz2
	do
		tar jxf $file
	done 

This process can be sped up using the 'parallel' program (http://www.gnu.org/software/parallel/) to distribute operations across the processors on your system:

	ls *.tar.bz2 | parallel -j 4 tar xjf 
	
This sets up a process pool with 4 jobs, that will decompress archive files in parallel until all files have been expanded.

---

##MathML Formula Representation

Formulae are translated to MathML, an XML encoding (http://www.w3.org/Math/). Each formula appears as a \<math\> tag, and is annotated with a unique identifier (the name of the file, followed by the relative offset of the formula in the file, e.g. *id="FileName:0"* for the first formula in *FileName.html*).

LaTeXML ( http://dlmf.nist.gov/LaTeXML/ ) is used to convert each formula from LaTeX to MathML, producing three representations for each formula:

1. **Presentation MathML** (symbol layout (appearance)). This is shown first, directly below a *\<semantics\>* tag.  
2. **Content MathML** (operator tree (mathematical semantics)). Where possible, LaTeXML provides an operator tree representing the mathematical semantics of an expression. This is demarcated by this tag: *\<annotation-xml encoding="MathML-Content"\>.*
3. **LaTeX string** (symbol layout (appearance)), demarcated by: *\<annotation-xml encoding="application/x-tex"\>.*

All articles contain a reference to MathJax ( https://www.mathjax.org/ ), which renders the Presentation MathML as SVG in modern web browsers.

---

##Corpus Creation 

1. **Raw Data.** A 'raw' mediawiki dump (i.e. the shorthand markup language used for Wikipedia) was obtained online:

	https://dumps.wikimedia.org/enwiki/20150805/enwiki-20150805-pages-articles.xml.bz2   
	  
	This is the Aug. 5, 2015 snapshot of English Wikipedia, omitting non-textual content (e.g. images). The raw dump file was just over 54GB in size.
	
2. **Article Extraction.** A python program using the 'iterparse' library for iterative parsing was used to:
	* Remove articles that were 'redirect' articles (i.e. entries that only refer to other articles)
	* Remove 'meta' articles about Wikipedia (with titles beginning with "Wikipedia:")
	* Identify articles containing LaTeX formulae, demarcated by \<math\> tags, as 'math' articles
	* Accept remaining articles as 'text' articles  
	  

   After splitting articles into 'math' and 'text' groups:
   
   * Article entries were converted to title and content fields.
   * Articles were stored in batches of 2000 articles, to avoid overwhelming file systems and make manipulation at the command line feasible.   
     
     
       
   The resulting text archive (.dat) files, containing roughly 2000 articles each were stored on disk. In total, there were 3897 archive files extracted from the Wikipedia dump: 16 'math' and 3881 'text.'
   
3. **Sampling 'Text' Articles.** To keep the size of the corpus relatively small while still insuring that many 'non-math' articles were present in the collection, it was decided to include 144 randomly selected 'text' archives, so that 'math' articles would comprise roughly 10% of the collection. Note that the percentage of math articles in the full English Wikipedia collection is actually *much* smaller (roughly 16 / 3897 = 0.41%). 
	
4. **MediaWiki Text to HTML Conversion.** *pandoc* (http://pandoc.org/) was used to make an initial translation from mediawiki to HTML text. The command used was the following:

	 	pandoc --latexmathml -f mediawiki -t html5 -s <mediwikifile.txt> -o <outputfile.html>

	--latexmathml converts \<math\> tags to \<span\> tags with a "LaTeX" class attribute. HTML5 was used as the target language for output.
	
5. **MediaWiki Math Templates to LaTeX Conversion.** A simple recursive-descent parser was implemented in python to convert MediaWiki math 'templates' (e.g. {{frac|1|2}} for '1/2') to LaTeX (these are not demarcated by \<math\> tags). While it is unlikely that all templates were located, we attempted to translate nearly all templates described online at: **https://en.wikipedia.org/wiki/Category:Mathematical_formatting_templates** (accessed late Sept. 2015; a .pdf snapshot of the list of template names was generated on Oct. 8, 2015, and is included as a .pdf file in this directory). 

	**Related articles:** https://en.wikipedia.org/wiki/Help:Displaying_a_formula, https://en.wikipedia.org/wiki/Template:Math . 

6. **MediaWiki Table Templates to HTML Conversion.** Conversion of tables in MediaWiki to HTML syntax was another important task, as these are very frequent in the collection. To avoid conversion failures that were occurring with pandoc when ill-formatted tables were provided, almost all formatting information (e.g. 'align', 'valign', 'width,' etc.) was removed and some other simple normalizations made, but with the goal to preserve all table headers, data and captions (i.e. table content). The MediaWii table format is described here: https://www.mediawiki.org/wiki/Help:Tables . 

7. **Figure Conversion.** To save space, all image tags were removed, but the captions from their associated figures were retained. Figure regions are indented and indicated by a bold **(Figure)** prefix in the final article set.


8. **Math and Table Translation.** LaTeXML ( http://dlmf.nist.gov/LaTeXML/ ) was used to translate LaTeX strings to MathML (see above). Each formula has a unique identifier indicated by an 'id' attribute. Formula identifiers are comprised of the article file name, ':' and the relative offset for the formula in the file. For example, the first formula in file 'Ex_Article.html' is "Ex_Article:0", the second formula "Ex_Article:1," and so on. 

9. **Conversion of Archive Files to HTML.**  Using a combination of bash scripts, perl one-liners and python programs, the conversion process involved:
	a. Extracting articles from a 'raw' .dat text archive file (see Step 2, above)
	b. Converting MediaWiki to HTML using pandoc, after pre-processing MediaWiki math/table data and modifying figures, followed by some simple post-processing,
	c. Using the Python *BeautifulSoup* library to extract all formula regions, call LaTeXML to generate MathML content, and then replace each LaTeX formula with its corresponding LaTeXML output (the format of this output is described above).
	d. Compressing the resulting articles and conversion process data as a .tar.bz2 archive file.
	
	**Note:** both 'math' and 'text' .dat archives were converted in the same way. A number of formulae represented by MediaWiki math templates were found (and converted) for the 'text' articles.

